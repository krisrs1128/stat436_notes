[
  {
    "path": "posts/2024-12-27-week01-01/",
    "title": "Introduction to ggplot2",
    "description": "A discussion of ggplot2 terminology, and an example of iteratively refining a\nsimple scatterplot.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-31",
    "categories": [],
    "contents": "\n\n\n\n\nReading,\nRecording,\nRmarkdown\n\n\nggplot2 is an R\nimplementation of the Grammar of Graphics. The idea is to\ndefine the basic “words” from which visualizations are built, and then\nlet users compose them in original ways. This is in contrast to systems\nwith prespecified chart types, where the user is forced to pick from a\nlimited dropdown menu of plots. Just like in ordinary language, the\ncreative combination of simple building blocks can support a very wide\nrange of expression.\n\n\nThese are libraries we’ll use in this lecture.\n\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(ggrepel)\nlibrary(scales)\n\n\n\nComponents of a Graph\n\n\nWe’re going to create this plot in these notes.\n\n\n\n\n\n\nEvery ggplot2 plot is made from three components,\n\n\nData: This is the data.frame that we want to visualize.\n\n\nGeometry: These are the types of visual marks that appear on the plot.\n\n\nAesthetic Mapping: This links the data with the visual marks.\n\n\nThe Data\n\n\nLet’s load up the data. Each row is an observation, and each column is\nan attribute that describes the observation. This is important because\neach mark that you see on a ggplot – a line, a point, a tile, … – had to\nstart out as a row within an R data.frame. The visual\nproperties of the mark (e.g., color) are determined by the values along\ncolumns. These type of data are often referred to as tidy data, and\nwe’ll have a full week discussing this topic.\n\n\nHere’s an example of the data above in tidy format,\n\n\n\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nThis is one example of how the same information might be stored in a\nnon-tidy way, making visualization much harder.\n\n\n\nnon_tidy <- data.frame(t(murders))\ncolnames(non_tidy) <- non_tidy[1, ]\nnon_tidy <- non_tidy[-1, ]\nnon_tidy[, 1:6]\n\n            Alabama   Alaska  Arizona Arkansas California Colorado\nabb              AL       AK       AZ       AR         CA       CO\nregion        South     West     West    South       West     West\npopulation  4779736   710231  6392017  2915918   37253956  5029196\ntotal           135       19      232       93       1257       65\n\n\nOften, one of the hardest parts in making a ggplot2 plot is not coming\nup with the right ggplot2 commands, but reshaping the data so that it’s\nin a tidy format.\n\n\nGeometry\n\n\nThe words in the grammar of graphics are the geometry layers. We can\nassociate each row of a data frame with points, lines, tiles, etc., just\nby referring to the appropriate geom in ggplot2. A typical plot will\ncompose a chain of layers on top of a dataset,\n\n\n\nggplot(data) + [layer 1] + [layer 2] + …\n\n\n\nFor example, by deconstructing the plot above, we would expect to have\npoint and text layers. For now, let’s just tell the plot to put all the\ngeom’s at the origin.\n\n\n\nggplot(murders) +\n  geom_point(x = 0, y = 0) +\n  geom_text(x = 0, y = 0, label = \"test\")\n\n\n\n\n\nYou can see all the types of geoms in the\ncheat\nsheet. We’ll be experimenting with a few of these in a\nlater lecture.\n\n\nAesthetic mappings\n\n\nAesthetic mappings make the connection between the data and the\ngeometry. It’s the piece that translates abstract data fields into\nvisual properties. Analyzing the original graph, we recognize these\nspecific mappings,\n\n\nState Population → \\(x\\)-axis coordinate\n\n\nNumber of murders → \\(y\\)-axis coordinate\n\n\nGeographical region → color\n\n\nTo establish these mappings, we need to use the aes\nfunction. Notice that column names don’t have to be quoted – ggplot2\nknows to refer back to the murders data frame in\nggplot(murders).\n\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region))\n\n\n\n\n\nThe original plot used a log-scale. To transform the x and y axes, we\ncan use\nscales.\n\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nOnce nuance is that scales aren’t limited to \\(x\\) and\n\\(y\\)\ntransformations. They can be applied to modify any relationship between\na data field and its appearance on the page. For example, this changes\nthe mapping between the region field and circle color.\n\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  scale_x_log10() +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#6a4078\", \"#aa1518\", \"#9ecaf8\", \"#50838c\")) # exercise: find better colors using https://imagecolorpicker.com/\n\n\n\n\n\nA problem with this graph is that it doesn’t tell us which state each\npoint corresponds to. For that, we’ll need text labels. We can encode\nthe coordinates for these marks again using aes, but this\ntime within a geom_text layer.\n\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  geom_text(\n    aes(x = population, y = total, label = abb),\n    nudge_x = 0.08 # what would happen if I remove this?\n  ) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nNote that each type of layer uses different visual properties to encode\nthe data – the argument label is only available for the\ngeom_text layer. You can see which aesthetic mappings are\nrequired for each type of geom by checking that\ngeom’s documentation page, under the Aesthetics heading.\n\n\nIt’s usually a good thing to make your code as concise as possible. For\nggplot2, we can achieve this by sharing elements across aes\ncalls (e.g., not having to type population and\ntotal twice). This can be done by defining a “global”\naesthetic, putting it inside the initial ggplot call.\n\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_point(aes(col = region)) +\n  geom_text(aes(label = abb), nudge_x = 0.08) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nFinishing touches\n\n\nHow can we improve the readability of this plot? You might already have\nideas,\n\n\nPrevent labels from overlapping. It’s impossible to read some of the\nstate names.\n\n\nAdd a line showing the national rate. This serves as a point of\nreference, allowing us to see whether an individual state is above or\nbelow the national murder rate.\n\n\nGive meaningful axis / legend labels and a title.\n\n\nMove the legend to the top of the figure. Right now, we’re wasting a lot\nof visual real estate in the right hand side, just to let people know\nwhat each color means.\n\n\nUse a better color theme.\n\n\nFor 1., the ggrepel package find better state name\npositions, drawing links when necessary.\n\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) + # I moved it up so that the geom_point's appear on top of the lines\n  geom_point(aes(col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nFor 2., let’s first compute the national murder rate,\n\n\n\nr <- murders %>% \n  summarize(rate = sum(total) /  sum(population)) %>%\n  pull(rate)\nr\n\n[1] 3.034555e-05\n\n\nNow, we can use this as the slope in a geom_abline layer,\nwhich encodes a slope and intercept as a line on a graph.\n\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nFor 3., we can add a labs layer to write labels and a\ntheme to reposition the legend. I used\nunit_format from the scales package to change\nthe scientific notation in the \\(x\\)-axis labels to something more\nreadable.\n\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10(labels = unit_format(unit = \"million\", scale = 1e-6)) + # used to convert scientific notation to readable labels\n  scale_y_log10() +\n  labs(\n    x = \"Population (log scale)\",\n    y = \"Total number of murders (log scale)\",\n    color = \"region\",\n    title = \"US Gun Murders in 2010\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nFor 5., I find the gray background with reference lines a bit\ndistracting. We can simplify the appearance using theme_bw.\nI also like the colorbrewer palette, which can be used by calling a\ndifferent color scale.\n\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10(labels = unit_format(unit = \"million\", scale = 1e-6)) +\n  scale_y_log10() +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"Population (log scale)\",\n    y = \"Total number of murders (log scale)\",\n    color = \"Region\",\n    title = \"US Gun Murders in 2010\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\nSome bonus exercises, which will train you to look at your graphics more\ncarefully, as well as build your familiarity with ggplot2.\n\n\nTry reducing the size of the text labels. Hint: use the\nsize argument in geom_text_repel.\n\n\nIncrease the size of the circles in the legend. Hint: Use\noverride.aes within a guide.\n\n\nRe-order the order of regions in the legend. Hint: Reset the factor\nlevels in the region field of the murders\ndata.frame.\n\n\nOnly show labels for a subset of states that are far from the national\nrate. Hint: Filter the murders data.frame, and use a\ndata field specific to the geom_text_repel\nlayer.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week01-02/",
    "title": "A Vocabulary of Marks",
    "description": "Examples of encodings and sequential refinement of a plot.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-30",
    "categories": [],
    "contents": "\n\n\n\n\nReading,\nRecording,\nRmarkdown\n\n\n\nThe choice of encodings can have a strong effect on (1) the types of\ncomparisons that a visualization suggests and (2) the chance that\nreaders leave with complete and accurate conclucions. With this in mind,\nit’s worthwhile to develop a rich vocabulary of potential visual\nencodings.\n\n\n\n\nSo, let’s look at a few different types of encodings available in\nggplot2. Before we get started, let’s load up the libraries\nthat will be used in these notes. ggplot2 is our plotting\nlibrary. readr is used to read data files from a web link,\nand dplyr is useful for some of the data manipulations\nbelow (we dive into it deeply in Week 2).\n\n\n\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\ntheme_set(theme_bw()) # create a simpler default theme\n\n\n\nPoint Marks\n\n\nLet’s read in the gapminder dataset, which describes the changes in\nstandard of living around the world over the last few decades. The\n%>% “pipe” operator takes the output of the previous\ncommand as input to the next one – it is useful for chains of commands\nwhere the intermediate results are not needed. The mutate\ncommand makes sure that the country group variable is treated as a\ncategorical, and not numeric, variable.\n\n\n\ngapminder <- read_csv(\"https://uwmadison.box.com/shared/static/dyz0qohqvgake2ghm4ngupbltkzpqb7t.csv\", col_types = cols()) %>%\n  mutate(cluster = as.factor(cluster))  # specify that cluster is nominal\ngap2000 <- filter(gapminder, year == 2000) # keep only year 2000\n\n\n\nPoint marks can encode data fields using their \\(x\\) and\n\\(y\\)\npositions, color, size, and shape. Below, each mark is a country, and\nwe’re using shape and the \\(y\\) position to distinguish between\ncountry clusters.\n\n\n\nggplot(gap2000) +\n  geom_point(aes(x = fertility, y = cluster, shape = cluster))\n\n\n\n\n\nSince the first two arguments in aes are always the\nx and y positions, we can omit it from our\ncommand. The code below produces the exact same plot (try it!).\n\n\n\nggplot(gap2000) +\n  geom_point(aes(fertility, cluster, shape = cluster))\n\n\n\nWe can specify different types of shapes using the shape\nparameter outside of the aes encoding.\n\n\n\nggplot(gap2000) +\n  geom_point(aes(fertility, cluster), shape = 15)\n\n\n\n\n\nBar Marks\n\n\nBar marks let us associate a continuous field with a nominal one.\n\n\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\")\n\n\n\n\n\nThe plot above is messy – it would not be appropriate for a publication\nor presentation. The grid lines associated with each bar are\ndistracting. Further, the axis labels are all running over one another.\nFor the first issue, we can customize the theme of the\nplot. Note that we don’t have to memorize the names of these arguments,\nsince they should autocomplete when pressing tab (we just need to\nmemorize the first few letters).\n\n\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\") +\n  theme(panel.grid.major.x = element_blank())\n\n\n\n\n\nFor the second issue, one approach is to turn the labels on their side,\nagain by customizing the theme.\n\n\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90),\n    panel.grid.major.x = element_blank()\n  )\n\n\n\n\n\nAn approach I like better is to turn the bars on their side. This way,\nreaders don’t have to tilt their heads to read the country names.\n\n\n\nggplot(gap2000) +\n  geom_bar(aes(pop, country), stat = \"identity\") +\n  theme(panel.grid.major.y = element_blank()) # note change from x to y\n\n\n\n\n\nI’m also going to remove the small tick marks associated with every\nname, again because it seems distracting.\n\n\n\nggplot(gap2000) +\n  geom_bar(aes(pop, country), stat = \"identity\") +\n  theme(\n    panel.grid.major.y = element_blank(),\n    axis.ticks = element_blank() # remove tick marks\n  )\n\n\n\n\n\n\nTo make comparisons between countries with similar populations easier,\nwe can order them by population (alphabetical ordering is not that\nmeaningful). To compare clusters, we can color in the bars.\n\n\n\nggplot(gap2000) +\n   geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\n\n\n\n\nWe’ve been spending a lot of time on this plot. This is because I want\nto emphasize that a visualization is not just something we can get just\nby memorizing some magic (programming) incantation. Instead, it is\nsomething worth critically engaging with and refining, in a similar way\nthat we would refine an essay or speech.\n\n\nPhilosophy aside, there are still a few points that need to be improved\nin this figure,\n\n\nThe axis titles are not meaningful.\n\n\nThere is a strange gap between the left hand edge of the plot and the\nstart of the bars.\n\n\nI would also prefer if the bars were exactly touching one another,\nwithout the small vertical gap.\n\n\nThe scientific notation for population size is unnecessarily technical.\n\n\nThe color scheme is a bit boring.\n\n\nI’ll address each of these in a separate code block, with comments on\nthe parts that are different. First, improving the axis titles,\n\n\n\nggplot(gap2000) +\n   geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\") + # add better titles\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\n\n\nNow we remove the gap. I learned this trick by\ngoogling\nit – there is no shame in doing this! A wise friend of mine once\nshared, “I am not a programming expert, just an expert at\nStackOverflow.”\n\n\n\nggplot(gap2000) +\n   geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n   scale_x_continuous(expand = c(0, 0, 0.1, 0.1)) + # remove space to the axis\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\") + \n   theme(\n     axis.text.y = element_text(size = 6),\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\n\n\nNow, removing the gaps between bars.\n\n\n\nggplot(gap2000) +\n   geom_bar(\n     aes(pop, reorder(country, pop), fill = cluster),\n     width = 1, stat = \"identity\" # increase width of bars\n   ) +\n   scale_x_continuous(expand = c(0, 0, 0.1, 0.1)) +\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\n\n\nNow, we remove scientific notation,\n\n\n\nggplot(gap2000) +\n   geom_bar(\n     aes(pop, reorder(country, pop), fill = cluster),\n     width = 1, stat = \"identity\"\n   ) +\n   scale_x_continuous(label = label_number(scale_cut = cut_short_scale()), expand = c(0, 0, 0.1, 0.1)) + # remove scientific notation. ::omma() is also useful.\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\n\n\nFinally, we customize the colors. I often like to look up neat colors on\ncolor.adobe.com,\niwanthue or\ncolorhexa, but there are dozens\nof similar colorpicker sites out there.\n\n\n\nggplot(gap2000) +\n   geom_bar(\n     aes(pop, reorder(country, pop), fill = cluster),\n     width = 1, stat = \"identity\"\n   ) +\n   scale_x_continuous(label = label_number(scale_cut = cut_short_scale()), expand = c(0, 0, 0.1, 0.1)) + # remove scientific notation. comma() is also useful.\n   scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\")) +\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\n\n\n\n\nThis seems like a lot of work for just a lowly bar plot! But I think\nit’s amazing customizable the figure is – we can give it our own sense\nof style. With a bit of practice, these sorts of modifications will\nbecome second nature, and it won’t be necessary to keep track of all the\nintermediate code. And really, even though we spent some time on this\nplot, there are still many things that could be interesting to\nexperiment with, like font styles, background appearance, maybe even\nsplitting the countries into two panels.\n\n\n\n\nIn the plot above, each bar is anchored at 0. Instead, we could have\neach bar encode two continuous values, a left and right. To\nillustrate, let’s compare the minimum and maximimum life expectancies\nwithin each country cluster. We’ll need to create a new\ndata.frame with just the summary information. For this, we\ngroup_by each cluster, so that a summarise\ncall finds the minimum and maximum life expectancies restricted to each\ncluster. We’ll discuss the group_by +\nsummarise pattern in detail next week.\n\n\n\n# find summary statistics\nlife_ranges <- gap2000 %>%\n  group_by(cluster) %>%\n  summarise(\n    min_life = min(life_expect),\n    max_life = max(life_expect)\n  )\n\n# look at a few rows\nhead(life_ranges)\n\n# A tibble: 6 × 3\n  cluster min_life max_life\n  <fct>      <dbl>    <dbl>\n1 0           42.1     63.6\n2 1           70.5     80.6\n3 2           43.4     53.4\n4 3           58.1     79.8\n5 4           66.7     82  \n6 5           57.0     79.7\n\nggplot(life_ranges) +\n  geom_segment(\n    aes(min_life, reorder(cluster, max_life), xend = max_life, yend = cluster, col = cluster),\n    size = 5,\n  ) +\n  scale_color_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\")) +\n  labs(x = \"Minimum and Maximum Expected Span\", col = \"Country Group\", y = \"Country Group\") +\n  xlim(0, 85) # otherwise would only range from 42 to 82\n\n\n\n\n\n\nLine Marks\n\n\n\nLine marks are useful for comparing changes. Our eyes naturally focus on\nrates of change when we see lines. Below, we’ll plot the fertility over\ntime, colored in by country cluster. The group argument is\nuseful for ensuring each country gets its own line; if we removed it,\nggplot2 would become confused by the fact that the same\nx (year) values are associated with multiple\ny’s (fertility rates).\n\n\n\nggplot(gapminder) +\n  geom_line(\n    aes(year, fertility, col = cluster, group = country),\n      alpha = 0.7, size = 0.9\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +  # same trick of removing gap\n  scale_color_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))\n\n\n\n\n\n\nArea Marks\n\n\n\nArea marks have a flavor of both bar and line marks. The filled area\nsupports absolute comparisons, while the changes in shape suggest\nderivatives.\n\n\n\npopulation_sums <- gapminder %>%\n  group_by(year, cluster) %>%\n  summarise(total_pop = sum(pop))\nhead(population_sums)\n\n# A tibble: 6 × 3\n# Groups:   year [1]\n   year cluster total_pop\n  <dbl> <fct>       <dbl>\n1  1955 0       495927174\n2  1955 1       360609771\n3  1955 2        60559800\n4  1955 3       355392405\n5  1955 4       854125031\n6  1955 5        56064015\n\nggplot(population_sums) +\n  geom_area(aes(year, total_pop, fill = cluster)) +\n  scale_y_continuous(expand = c(0, 0, 0.1, 0.1), label = label_number(scale_cut = cut_short_scale()))  + # remove scientific notation. scales::comma() is also useful.\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))\n\n\n\n\n\nJust like in bar marks, we don’t necessarily need to anchor the \\(y\\)-axis\nat 0. For example, here the bottom and top of each area mark is given by\nthe 30% and 70% quantiles of population within each country cluster.\n\n\n\npopulation_ranges <- gapminder %>%\n  group_by(year, cluster) %>%\n  summarise(min_pop = quantile(pop, 0.3), max_pop = quantile(pop, 0.7))\nhead(population_ranges)\n\n# A tibble: 6 × 4\n# Groups:   year [1]\n   year cluster   min_pop   max_pop\n  <dbl> <fct>       <dbl>     <dbl>\n1  1955 0       40880121. 83941368.\n2  1955 1        4532940  25990229.\n3  1955 2        6600426. 17377594.\n4  1955 3        2221139   8671500 \n5  1955 4        9014491  61905422 \n6  1955 5        3007625  12316126.\n\nggplot(population_ranges) +\n  geom_ribbon(\n    aes(x = year, ymin = min_pop, ymax = max_pop, fill = cluster),\n    alpha = 0.8\n  ) +\n  scale_y_continuous(expand = c(0, 0, 0.1, 0.1), label = label_number(scale_cut = cut_short_scale())) + # remove scientific notation. scales::comma() is also useful.\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week02-01/",
    "title": "Tidy Data",
    "description": "The definition of tidy data, and why it's often helpful for visualization._",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-29",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\n\nA dataset is called tidy if rows correspond to distinct observations and\ncolumns correspond to distinct variables.\n\n\n\n\nFor visualization, it is important that data be in tidy format. This is\nbecause (a) each visual mark will be associated with a row of the\ndataset and (b) properties of the visual marks will determined by values\nwithin the columns. A plot that is easy to create when the data are in\ntidy format might be very hard to create otherwise.\n\n\nThe tidy data might seem like an idea so natural that it’s not worth\nteaching (let alone formalizing). However, exceptions are encountered\nfrequently, and it’s important that you be able to spot them. Further,\nthere are now many utilities for “tidying” data, and they are worth\nbecoming familiar with.\n\n\nHere is an example of a tidy dataset.\n\n\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nIt is easy to visualize the tidy dataset.\n\n\n\nggplot(table1, aes(x = year, y = cases, col = country)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\nBelow are three non-tidy versions of the same dataset. They are\nrepresentative of more general classes of problems that may arise,\n\n\nA variable might be implicitly stored within column names, rather than\nexplicitly stored in its own column. Here, the years are stored as\ncolumn names. It’s not really possible to create the plot above using\nthe data in this format.\n\n\n\n\ntable4a # cases\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b # population\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  <chr>            <dbl>      <dbl>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\nThe same observation may appear in multiple rows, where each instance of\nthe row is associated with a different variable. Here, the observations\nare the country by year combinations.\n\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\nA single column actually stores multiple variables. Here,\nrate is being used to store both the population and case\ncount variables.\n\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nThe trouble is that this variable has to be stored as a character;\notherwise, we lose access to the original population and case variable.\nBut, this makes the plot useless.\n\n\n\nggplot(table3, aes(x = year, y = rate)) +\n  geom_point() +\n  geom_line(aes(group = country))\n\n\n\n\n\nThe next few lectures provide tools for addressing these three problems.\n\n\nA few caveats are in order. It’s easy to become a tidy-data purist, and\nlose sight of the bigger data-analytic picture. To prevent that, first,\nremember that what is or is not tidy may be context dependent. Maybe you\nwant to treat each week as an observation, rather than each day. Second,\nknow that there are sometimes computational reasons to prefer non-tidy\ndata. For example, “long” data often require more memory, since column\nnames that were originally stored once now have to be copied onto each\nrow. Certain statistical models are also sometimes best framed as matrix\noperations on non-tidy datasets.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week02-02/",
    "title": "Pivoting",
    "description": "Tools for reshaping data into tidy format.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-28",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\n\n\nPivoting refers to the process of changing the interpretation of each\nrow in a data frame. It is useful for addressing problems 1 - 2 in the\nprevious lecture, which we repeat here for completeness.\n\n\nA variable might be implicitly stored within column names, rather than\nexplicitly stored in its own column.\n\n\nThe same observation may appear in multiple rows, where each instance of\nthe row is associated with a different variable.\n\n\n\n\nTo address (a), we can use the pivot_longer function in\ntidyr. It takes an implicitly stored variable and\nexplicitly stores it in a column defined by the names_to\nargument. In\n\n\n\n\nThe example below shows pivot_longer being used to tidy one\nof the non-tidy tuberculosis datasets. Note that the data has doubled in\nlength, because there are now two rows per country (one per year).\n\n\n\nFor reference, these are the original data.\n\n\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\nThis step lengthens the data,\n\n\n\ntable4a_longer <- table4a %>% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntable4a_longer\n\n# A tibble: 6 × 3\n  country     year   cases\n  <chr>       <chr>  <dbl>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\ndim(table4a)\n\n[1] 3 3\n\ndim(table4a_longer)\n\n[1] 6 3\n\n\nWe can pivot both the population and the cases table, then combine them\nusing a join operation. A join operation matches rows across two tables\naccording to their shared columns.\n\n\n\n# helper function, to avoid copying and pasting code\npivot_fun <- function(x, value_column = \"cases\") {\n  x %>%\n    pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = value_column)\n}\n\ntable4 <- left_join(\n  pivot_fun(table4a), # look for all country x year combinations in left table\n  pivot_fun(table4b, \"population\") # and find matching rows in right table\n)\ntable4\n\n# A tibble: 6 × 4\n  country     year   cases population\n  <chr>       <chr>  <dbl>      <dbl>\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\nThis lets us make the year vs. rate plot that we had tried to put\ntogether in the last lecture. It’s much easier to recognize trends when\ncomparing the rates, than when looking at the raw case counts.\n\n\n\nggplot(table4, aes(x = year, y = cases / population, col = country)) +\n  geom_point() +\n  geom_line(aes(group = country))\n\n\n\n\n\n\nTo address (b), we can use the pivot_wider function. It\nspreads the column in the values_from argument across new\ncolumns specified by the names_from argument.\n\n\n\n\nThe example below shows pivot_wider being used to tidy one\nof the other non-tidy datasets. Note when there are more than two levels\nin the names_from column, this will always be wider than\nthe starting data frame, which is why this operation is called\npivot_wider.\n\n\n\nFor reference, here is table2 before pivoting.\n\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\nNow, we spread the cases and population\nvariables into their own columns.\n\n\n\ntable2 %>%\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week02-03/",
    "title": "Deriving Variables",
    "description": "Using `separate`, `mutate`, and `summarise` to derive new variables for\ndownstream visualization.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-27",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\n\n\n\n\nIt’s easiest to define visual encodings when the variables we want to\nencode are contained in their own columns. After sketching out a\nvisualization of interest, we may find that these variables are not\nexplicitly represented among the columns of the raw dataset. In this\ncase, we may need to derive them based on what is available. The\ndplyr and tidyr packages provide functions for\nderiving new variables, which we review in these notes.\n\n\n\n\nSometimes a single column is used to implicitly store several variables.\nTo make the data tidy, separate can be used to split that\nsingle column into several columns, each of which corresponds to exactly\none variable.\n\n\n\n\nThe block below separates our earlier table3, which stored\nrate as a fraction in a character column. The original table was,\n\n\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nand the separated version is,\n\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) # try without convert, and compare the data types of the columns\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nNote that this function has an inverse, called unite, which\ncan merge several columns into one. This is sometimes useful, but not as\noften as separate, since it isn’t needed to tidy a dataset.\n\n\n\n\nSeparating a single column into several is a special case of a more\ngeneral operation, mutate, which defines new columns as\nfunctions of existing ones. We have used this is in previous lectures,\nbut now we can philosophically justify it: the variables we want to\nencode need to be defined in advance.\n\n\n\n\nFor example, we may want to create a column rate that\nincludes cases over population,\n\n\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) %>%\n  mutate(rate = cases / year)\n\n# A tibble: 6 × 5\n  country      year  cases population    rate\n  <chr>       <dbl>  <int>      <int>   <dbl>\n1 Afghanistan  1999    745   19987071   0.373\n2 Afghanistan  2000   2666   20595360   1.33 \n3 Brazil       1999  37737  172006362  18.9  \n4 Brazil       2000  80488  174504898  40.2  \n5 China        1999 212258 1272915272 106.   \n6 China        2000 213766 1280428583 107.   \n\n\n\nSometimes, the variables of interest are functions of several rows. For\nexample, perhaps we want to visualize averages of a variable across age\ngroups. In this case, we can derive a summary across groups of rows\nusing the group_by-followed-by-summarise\npattern.\n\n\n\n\nFor example, perhaps we want the average rate over both years.\n\n\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) %>%\n  mutate(rate = cases / year) %>%\n  group_by(country) %>%\n  summarise(avg_rate = mean(rate))\n\n# A tibble: 3 × 2\n  country     avg_rate\n  <chr>          <dbl>\n1 Afghanistan    0.853\n2 Brazil        29.6  \n3 China        107.   \n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week02-04/",
    "title": "Tidy Data Example",
    "description": "An extended example of tidying a real-world dataset.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-26",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(skimr)\n\n\n\n\nLet’s work through the example of tidying a WHO dataset. This was\ndiscussed in the reading and is good practice in pivoting and deriving\nnew variables.\n\n\n\n\nThe raw data, along with a summary from the skimr package,\nis shown below (notice the small multiples!).\n\n\n\n\nwho\n\n# A tibble: 7,240 × 60\n   country     iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534\n   <chr>       <chr> <chr> <dbl>       <dbl>        <dbl>        <dbl>\n 1 Afghanistan AF    AFG    1980          NA           NA           NA\n 2 Afghanistan AF    AFG    1981          NA           NA           NA\n 3 Afghanistan AF    AFG    1982          NA           NA           NA\n 4 Afghanistan AF    AFG    1983          NA           NA           NA\n 5 Afghanistan AF    AFG    1984          NA           NA           NA\n 6 Afghanistan AF    AFG    1985          NA           NA           NA\n 7 Afghanistan AF    AFG    1986          NA           NA           NA\n 8 Afghanistan AF    AFG    1987          NA           NA           NA\n 9 Afghanistan AF    AFG    1988          NA           NA           NA\n10 Afghanistan AF    AFG    1989          NA           NA           NA\n# ℹ 7,230 more rows\n# ℹ 53 more variables: new_sp_m3544 <dbl>, new_sp_m4554 <dbl>,\n#   new_sp_m5564 <dbl>, new_sp_m65 <dbl>, new_sp_f014 <dbl>,\n#   new_sp_f1524 <dbl>, new_sp_f2534 <dbl>, new_sp_f3544 <dbl>,\n#   new_sp_f4554 <dbl>, new_sp_f5564 <dbl>, new_sp_f65 <dbl>,\n#   new_sn_m014 <dbl>, new_sn_m1524 <dbl>, new_sn_m2534 <dbl>,\n#   new_sn_m3544 <dbl>, new_sn_m4554 <dbl>, new_sn_m5564 <dbl>, …\n\n\n\nskim(who)\n\n\nTable 1: Data summary\n\n\nName\n\n\nwho\n\n\nNumber of rows\n\n\n7240\n\n\nNumber of columns\n\n\n60\n\n\n_______________________\n\n\n\n\nColumn type frequency:\n\n\n\n\ncharacter\n\n\n3\n\n\nnumeric\n\n\n57\n\n\n________________________\n\n\n\n\nGroup variables\n\n\nNone\n\n\nVariable type: character\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nempty\n\n\nn_unique\n\n\nwhitespace\n\n\ncountry\n\n\n0\n\n\n1\n\n\n4\n\n\n52\n\n\n0\n\n\n219\n\n\n0\n\n\niso2\n\n\n34\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n218\n\n\n0\n\n\niso3\n\n\n0\n\n\n1\n\n\n3\n\n\n3\n\n\n0\n\n\n219\n\n\n0\n\n\nVariable type: numeric\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\nyear\n\n\n0\n\n\n1.00\n\n\n1996.56\n\n\n9.83\n\n\n1980\n\n\n1988.00\n\n\n1997.0\n\n\n2005.00\n\n\n2013\n\n\n▇▇▇▇▇\n\n\nnew_sp_m014\n\n\n4067\n\n\n0.44\n\n\n83.71\n\n\n316.14\n\n\n0\n\n\n0.00\n\n\n5.0\n\n\n37.00\n\n\n5001\n\n\n▇▁▁▁▁\n\n\nnew_sp_m1524\n\n\n4031\n\n\n0.44\n\n\n1015.66\n\n\n4885.38\n\n\n0\n\n\n9.00\n\n\n90.0\n\n\n502.00\n\n\n78278\n\n\n▇▁▁▁▁\n\n\nnew_sp_m2534\n\n\n4034\n\n\n0.44\n\n\n1403.80\n\n\n5718.39\n\n\n0\n\n\n14.00\n\n\n150.0\n\n\n715.50\n\n\n84003\n\n\n▇▁▁▁▁\n\n\nnew_sp_m3544\n\n\n4021\n\n\n0.44\n\n\n1315.88\n\n\n6003.26\n\n\n0\n\n\n13.00\n\n\n130.0\n\n\n583.50\n\n\n90830\n\n\n▇▁▁▁▁\n\n\nnew_sp_m4554\n\n\n4017\n\n\n0.45\n\n\n1103.86\n\n\n5441.06\n\n\n0\n\n\n12.00\n\n\n102.0\n\n\n440.00\n\n\n82921\n\n\n▇▁▁▁▁\n\n\nnew_sp_m5564\n\n\n4022\n\n\n0.44\n\n\n800.70\n\n\n4418.31\n\n\n0\n\n\n8.00\n\n\n63.0\n\n\n279.00\n\n\n63814\n\n\n▇▁▁▁▁\n\n\nnew_sp_m65\n\n\n4031\n\n\n0.44\n\n\n682.82\n\n\n4089.14\n\n\n0\n\n\n8.00\n\n\n53.0\n\n\n232.00\n\n\n70376\n\n\n▇▁▁▁▁\n\n\nnew_sp_f014\n\n\n4066\n\n\n0.44\n\n\n114.33\n\n\n504.63\n\n\n0\n\n\n1.00\n\n\n7.0\n\n\n50.75\n\n\n8576\n\n\n▇▁▁▁▁\n\n\nnew_sp_f1524\n\n\n4046\n\n\n0.44\n\n\n826.11\n\n\n3552.02\n\n\n0\n\n\n7.00\n\n\n66.0\n\n\n421.00\n\n\n53975\n\n\n▇▁▁▁▁\n\n\nnew_sp_f2534\n\n\n4040\n\n\n0.44\n\n\n917.30\n\n\n3580.15\n\n\n0\n\n\n9.00\n\n\n84.0\n\n\n476.25\n\n\n49887\n\n\n▇▁▁▁▁\n\n\nnew_sp_f3544\n\n\n4041\n\n\n0.44\n\n\n640.43\n\n\n2542.51\n\n\n0\n\n\n6.00\n\n\n57.0\n\n\n308.00\n\n\n34698\n\n\n▇▁▁▁▁\n\n\nnew_sp_f4554\n\n\n4036\n\n\n0.44\n\n\n445.78\n\n\n1799.23\n\n\n0\n\n\n4.00\n\n\n38.0\n\n\n211.00\n\n\n23977\n\n\n▇▁▁▁▁\n\n\nnew_sp_f5564\n\n\n4045\n\n\n0.44\n\n\n313.87\n\n\n1381.25\n\n\n0\n\n\n3.00\n\n\n25.0\n\n\n146.50\n\n\n18203\n\n\n▇▁▁▁▁\n\n\nnew_sp_f65\n\n\n4043\n\n\n0.44\n\n\n283.93\n\n\n1267.94\n\n\n0\n\n\n4.00\n\n\n30.0\n\n\n129.00\n\n\n21339\n\n\n▇▁▁▁▁\n\n\nnew_sn_m014\n\n\n6195\n\n\n0.14\n\n\n308.75\n\n\n1727.25\n\n\n0\n\n\n1.00\n\n\n9.0\n\n\n61.00\n\n\n22355\n\n\n▇▁▁▁▁\n\n\nnew_sn_m1524\n\n\n6210\n\n\n0.14\n\n\n513.02\n\n\n3643.27\n\n\n0\n\n\n2.00\n\n\n15.5\n\n\n102.00\n\n\n60246\n\n\n▇▁▁▁▁\n\n\nnew_sn_m2534\n\n\n6218\n\n\n0.14\n\n\n653.69\n\n\n3430.03\n\n\n0\n\n\n2.00\n\n\n23.0\n\n\n135.50\n\n\n50282\n\n\n▇▁▁▁▁\n\n\nnew_sn_m3544\n\n\n6215\n\n\n0.14\n\n\n837.87\n\n\n8524.53\n\n\n0\n\n\n2.00\n\n\n19.0\n\n\n132.00\n\n\n250051\n\n\n▇▁▁▁▁\n\n\nnew_sn_m4554\n\n\n6213\n\n\n0.14\n\n\n520.79\n\n\n3301.70\n\n\n0\n\n\n2.00\n\n\n19.0\n\n\n127.50\n\n\n57181\n\n\n▇▁▁▁▁\n\n\nnew_sn_m5564\n\n\n6219\n\n\n0.14\n\n\n448.62\n\n\n3488.68\n\n\n0\n\n\n2.00\n\n\n16.0\n\n\n101.00\n\n\n64972\n\n\n▇▁▁▁▁\n\n\nnew_sn_m65\n\n\n6220\n\n\n0.14\n\n\n460.36\n\n\n3991.90\n\n\n0\n\n\n2.00\n\n\n20.5\n\n\n111.75\n\n\n74282\n\n\n▇▁▁▁▁\n\n\nnew_sn_f014\n\n\n6200\n\n\n0.14\n\n\n291.95\n\n\n1647.30\n\n\n0\n\n\n1.00\n\n\n8.0\n\n\n58.00\n\n\n21406\n\n\n▇▁▁▁▁\n\n\nnew_sn_f1524\n\n\n6218\n\n\n0.14\n\n\n407.90\n\n\n2379.13\n\n\n0\n\n\n1.00\n\n\n12.0\n\n\n89.00\n\n\n35518\n\n\n▇▁▁▁▁\n\n\nnew_sn_f2534\n\n\n6224\n\n\n0.14\n\n\n466.26\n\n\n2272.86\n\n\n0\n\n\n2.00\n\n\n18.0\n\n\n103.25\n\n\n28753\n\n\n▇▁▁▁▁\n\n\nnew_sn_f3544\n\n\n6220\n\n\n0.14\n\n\n506.59\n\n\n5013.53\n\n\n0\n\n\n1.00\n\n\n11.0\n\n\n82.25\n\n\n148811\n\n\n▇▁▁▁▁\n\n\nnew_sn_f4554\n\n\n6222\n\n\n0.14\n\n\n271.16\n\n\n1511.72\n\n\n0\n\n\n1.00\n\n\n10.0\n\n\n76.75\n\n\n23869\n\n\n▇▁▁▁▁\n\n\nnew_sn_f5564\n\n\n6223\n\n\n0.14\n\n\n213.39\n\n\n1468.62\n\n\n0\n\n\n1.00\n\n\n8.0\n\n\n56.00\n\n\n26085\n\n\n▇▁▁▁▁\n\n\nnew_sn_f65\n\n\n6221\n\n\n0.14\n\n\n230.75\n\n\n1597.70\n\n\n0\n\n\n1.00\n\n\n13.0\n\n\n74.00\n\n\n29630\n\n\n▇▁▁▁▁\n\n\nnew_ep_m014\n\n\n6202\n\n\n0.14\n\n\n128.61\n\n\n460.14\n\n\n0\n\n\n0.00\n\n\n6.0\n\n\n55.00\n\n\n7869\n\n\n▇▁▁▁▁\n\n\nnew_ep_m1524\n\n\n6214\n\n\n0.14\n\n\n158.30\n\n\n537.74\n\n\n0\n\n\n1.00\n\n\n11.0\n\n\n88.00\n\n\n8558\n\n\n▇▁▁▁▁\n\n\nnew_ep_m2534\n\n\n6220\n\n\n0.14\n\n\n201.23\n\n\n764.05\n\n\n0\n\n\n1.00\n\n\n13.0\n\n\n124.00\n\n\n11843\n\n\n▇▁▁▁▁\n\n\nnew_ep_m3544\n\n\n6216\n\n\n0.14\n\n\n272.72\n\n\n3381.41\n\n\n0\n\n\n1.00\n\n\n10.5\n\n\n91.25\n\n\n105825\n\n\n▇▁▁▁▁\n\n\nnew_ep_m4554\n\n\n6220\n\n\n0.14\n\n\n108.11\n\n\n380.61\n\n\n0\n\n\n1.00\n\n\n8.5\n\n\n63.25\n\n\n5875\n\n\n▇▁▁▁▁\n\n\nnew_ep_m5564\n\n\n6225\n\n\n0.14\n\n\n72.17\n\n\n234.55\n\n\n0\n\n\n1.00\n\n\n7.0\n\n\n46.00\n\n\n3957\n\n\n▇▁▁▁▁\n\n\nnew_ep_m65\n\n\n6222\n\n\n0.14\n\n\n78.94\n\n\n227.34\n\n\n0\n\n\n1.00\n\n\n10.0\n\n\n55.00\n\n\n3061\n\n\n▇▁▁▁▁\n\n\nnew_ep_f014\n\n\n6208\n\n\n0.14\n\n\n112.89\n\n\n446.55\n\n\n0\n\n\n0.00\n\n\n5.0\n\n\n50.00\n\n\n6960\n\n\n▇▁▁▁▁\n\n\nnew_ep_f1524\n\n\n6219\n\n\n0.14\n\n\n149.17\n\n\n543.89\n\n\n0\n\n\n1.00\n\n\n9.0\n\n\n78.00\n\n\n7866\n\n\n▇▁▁▁▁\n\n\nnew_ep_f2534\n\n\n6219\n\n\n0.14\n\n\n189.52\n\n\n761.79\n\n\n0\n\n\n1.00\n\n\n12.0\n\n\n95.00\n\n\n10759\n\n\n▇▁▁▁▁\n\n\nnew_ep_f3544\n\n\n6219\n\n\n0.14\n\n\n241.70\n\n\n3218.50\n\n\n0\n\n\n1.00\n\n\n9.0\n\n\n77.00\n\n\n101015\n\n\n▇▁▁▁▁\n\n\nnew_ep_f4554\n\n\n6223\n\n\n0.14\n\n\n93.77\n\n\n339.33\n\n\n0\n\n\n1.00\n\n\n8.0\n\n\n56.00\n\n\n6759\n\n\n▇▁▁▁▁\n\n\nnew_ep_f5564\n\n\n6223\n\n\n0.14\n\n\n63.04\n\n\n212.95\n\n\n0\n\n\n1.00\n\n\n6.0\n\n\n42.00\n\n\n4684\n\n\n▇▁▁▁▁\n\n\nnew_ep_f65\n\n\n6226\n\n\n0.14\n\n\n72.31\n\n\n202.72\n\n\n0\n\n\n0.00\n\n\n10.0\n\n\n51.00\n\n\n2548\n\n\n▇▁▁▁▁\n\n\nnewrel_m014\n\n\n7050\n\n\n0.03\n\n\n538.18\n\n\n2082.18\n\n\n0\n\n\n5.00\n\n\n32.5\n\n\n210.00\n\n\n18617\n\n\n▇▁▁▁▁\n\n\nnewrel_m1524\n\n\n7058\n\n\n0.03\n\n\n1489.51\n\n\n6848.18\n\n\n0\n\n\n17.50\n\n\n171.0\n\n\n684.25\n\n\n84785\n\n\n▇▁▁▁▁\n\n\nnewrel_m2534\n\n\n7057\n\n\n0.03\n\n\n2139.72\n\n\n7539.87\n\n\n0\n\n\n25.00\n\n\n217.0\n\n\n1091.00\n\n\n76917\n\n\n▇▁▁▁▁\n\n\nnewrel_m3544\n\n\n7056\n\n\n0.03\n\n\n2036.40\n\n\n7847.94\n\n\n0\n\n\n24.75\n\n\n208.0\n\n\n851.25\n\n\n84565\n\n\n▇▁▁▁▁\n\n\nnewrel_m4554\n\n\n7056\n\n\n0.03\n\n\n1835.07\n\n\n8324.28\n\n\n0\n\n\n19.00\n\n\n175.0\n\n\n688.50\n\n\n100297\n\n\n▇▁▁▁▁\n\n\nnewrel_m5564\n\n\n7055\n\n\n0.03\n\n\n1525.30\n\n\n8760.27\n\n\n0\n\n\n13.00\n\n\n136.0\n\n\n536.00\n\n\n112558\n\n\n▇▁▁▁▁\n\n\nnewrel_m65\n\n\n7058\n\n\n0.03\n\n\n1426.00\n\n\n9431.99\n\n\n0\n\n\n17.00\n\n\n117.0\n\n\n453.50\n\n\n124476\n\n\n▇▁▁▁▁\n\n\nnewrel_f014\n\n\n7050\n\n\n0.03\n\n\n532.84\n\n\n2117.78\n\n\n0\n\n\n5.00\n\n\n32.5\n\n\n226.00\n\n\n18054\n\n\n▇▁▁▁▁\n\n\nnewrel_f1524\n\n\n7056\n\n\n0.03\n\n\n1161.85\n\n\n4606.76\n\n\n0\n\n\n10.75\n\n\n123.0\n\n\n587.75\n\n\n49491\n\n\n▇▁▁▁▁\n\n\nnewrel_f2534\n\n\n7058\n\n\n0.03\n\n\n1472.80\n\n\n5259.59\n\n\n0\n\n\n18.00\n\n\n161.0\n\n\n762.50\n\n\n44985\n\n\n▇▁▁▁▁\n\n\nnewrel_f3544\n\n\n7057\n\n\n0.03\n\n\n1125.01\n\n\n4210.58\n\n\n0\n\n\n12.50\n\n\n125.0\n\n\n544.50\n\n\n38804\n\n\n▇▁▁▁▁\n\n\nnewrel_f4554\n\n\n7057\n\n\n0.03\n\n\n877.27\n\n\n3556.18\n\n\n0\n\n\n10.00\n\n\n92.0\n\n\n400.50\n\n\n37138\n\n\n▇▁▁▁▁\n\n\nnewrel_f5564\n\n\n7057\n\n\n0.03\n\n\n686.41\n\n\n3379.33\n\n\n0\n\n\n8.00\n\n\n69.0\n\n\n269.00\n\n\n40892\n\n\n▇▁▁▁▁\n\n\nnewrel_f65\n\n\n7055\n\n\n0.03\n\n\n683.76\n\n\n3618.47\n\n\n0\n\n\n9.00\n\n\n69.0\n\n\n339.00\n\n\n47438\n\n\n▇▁▁▁▁\n\n\n\nAccording to the data dictionary, the columns have the following\nmeanings,\n\n\nThe first three letters -> are we counting new or old cases of TB?\n\n\nNext two letters -> Type of tab.\n\n\nSixth letter -> Sex of patients\n\n\nRemaining numbers -> Age group. E.g., 3544 should be\ninterpreted as 35 - 44 years old.\n\n\nOur first step is to pivot_longer. There is quite a bit of\ninformation implicitly stored in the column names, and we want to make\nthose variables explicitly available for visual encoding.\n\n\n\nwho_longer <- who %>% \n  pivot_longer(\n    cols = new_sp_m014:newrel_f65,  # notice we can refer to groups of columns without naming each one\n    names_to = \"key\", \n    values_to = \"cases\", \n    values_drop_na = TRUE # if a cell is empty, we do not keep it in the tidy version\n  )\n\nwho_longer\n\n# A tibble: 76,046 × 6\n   country     iso2  iso3   year key          cases\n   <chr>       <chr> <chr> <dbl> <chr>        <dbl>\n 1 Afghanistan AF    AFG    1997 new_sp_m014      0\n 2 Afghanistan AF    AFG    1997 new_sp_m1524    10\n 3 Afghanistan AF    AFG    1997 new_sp_m2534     6\n 4 Afghanistan AF    AFG    1997 new_sp_m3544     3\n 5 Afghanistan AF    AFG    1997 new_sp_m4554     5\n 6 Afghanistan AF    AFG    1997 new_sp_m5564     2\n 7 Afghanistan AF    AFG    1997 new_sp_m65       0\n 8 Afghanistan AF    AFG    1997 new_sp_f014      5\n 9 Afghanistan AF    AFG    1997 new_sp_f1524    38\n10 Afghanistan AF    AFG    1997 new_sp_f2534    36\n# ℹ 76,036 more rows\n\n\nThe new column key contains several variables at once. We\ncan separate it into gender and age group.\n\n\n\nwho_separate <- who_longer %>% \n  mutate(key = str_replace(key, \"newrel\", \"new_rel\")) %>%\n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %>%\n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\nwho_separate\n\n# A tibble: 76,046 × 9\n   country     iso2  iso3   year new   type  sex   age   cases\n   <chr>       <chr> <chr> <dbl> <chr> <chr> <chr> <chr> <dbl>\n 1 Afghanistan AF    AFG    1997 new   sp    m     014       0\n 2 Afghanistan AF    AFG    1997 new   sp    m     1524     10\n 3 Afghanistan AF    AFG    1997 new   sp    m     2534      6\n 4 Afghanistan AF    AFG    1997 new   sp    m     3544      3\n 5 Afghanistan AF    AFG    1997 new   sp    m     4554      5\n 6 Afghanistan AF    AFG    1997 new   sp    m     5564      2\n 7 Afghanistan AF    AFG    1997 new   sp    m     65        0\n 8 Afghanistan AF    AFG    1997 new   sp    f     014       5\n 9 Afghanistan AF    AFG    1997 new   sp    f     1524     38\n10 Afghanistan AF    AFG    1997 new   sp    f     2534     36\n# ℹ 76,036 more rows\n\n\nWhile we have performed each step one at a time, it’s possible to chain\nthem into a single block of code. This is good practice, because it\navoids having to define intermediate variables that are only ever used\nonce. This is also typically more concise.\n\n\n\nwho %>%\n  pivot_longer(\n    cols = new_sp_m014:newrel_f65, \n    names_to = \"key\", \n    values_to = \"cases\", \n    values_drop_na = TRUE # if a cell is empty, we do not keep it in the tidy version\n  ) %>%\n  mutate(key = str_replace(key, \"newrel\", \"new_rel\")) %>%\n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %>%\n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\n# A tibble: 76,046 × 9\n   country     iso2  iso3   year new   type  sex   age   cases\n   <chr>       <chr> <chr> <dbl> <chr> <chr> <chr> <chr> <dbl>\n 1 Afghanistan AF    AFG    1997 new   sp    m     014       0\n 2 Afghanistan AF    AFG    1997 new   sp    m     1524     10\n 3 Afghanistan AF    AFG    1997 new   sp    m     2534      6\n 4 Afghanistan AF    AFG    1997 new   sp    m     3544      3\n 5 Afghanistan AF    AFG    1997 new   sp    m     4554      5\n 6 Afghanistan AF    AFG    1997 new   sp    m     5564      2\n 7 Afghanistan AF    AFG    1997 new   sp    m     65        0\n 8 Afghanistan AF    AFG    1997 new   sp    f     014       5\n 9 Afghanistan AF    AFG    1997 new   sp    f     1524     38\n10 Afghanistan AF    AFG    1997 new   sp    f     2534     36\n# ℹ 76,036 more rows\n\n\nA recommendation for visualization in javascript. We have only discussed\ntidying in R. While there is work to implement tidy-style\ntransformations in javascript, the R tidyverse provides a more mature\nsuite of tools. If you are making an interactive visualization in\njavascript, I recommend first tidying data in R so that each row\ncorresponds to a visual mark and each column to a visual property. You\ncan always save the result as either a json or csv, which can serve as\nthe source data for your javascript visualization.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week03-01/",
    "title": "Faceting",
    "description": "Using small multiples to create information dense plots.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2024-12-25",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\nIt might seem like we’re limited with the total number of variables we\ncan display at a time. While there are many types of encodings we could\nin theory use, only a few them are very effective, and they can\ninterfere with one another.\n\n\nNot all is lost, though! A very useful idea for visualizing\nhigh-dimensional data is the idea of small multiples. It turns out that\nour eyes are pretty good at making sense of many small plots, as long as\nthere is some shared structure across the plots.\n\n\n\n\nLet’s see these ideas in action. These are libraries we need.\n\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\ntheme_set(theme_bw())\n\n\n\nIn ggplot2, we can implement this idea using the facet_wrap\nand facet_grid commands. We specify the column in the\ndata.frame along which we want to generate comparable small multiples.\n\n\n\nyears <- c(1962, 1980, 1990, 2000, 2012)\ncontinents <- c(\"Europe\", \"Asia\")\ngapminder_subset <- gapminder %>%\n  filter(year %in% years, continent %in% continents)\n\nggplot(\n    gapminder_subset, \n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(. ~ year) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nIn facet grid, you specify whether you want the plot to be repeated\nacross rows or columns, depending on whether you put the variable before\nor after the tilde.\n\n\n\nggplot(\n    gapminder_subset, \n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(year ~ .) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nYou can also facet by more than one variable at a time, specifying which\nvariables should go in rows and which should go in columns again using\nthe tilde.\n\n\n\nggplot(\n    gapminder %>% filter(year %in% years),\n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(year ~ continent) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nSometimes, you just want to see the display repeated over groups, but\nyou don’t really need them to all appear in the same row or column. In\nthis case, you can use facet_wrap.\n\n\n\nggplot(\n    gapminder %>% filter(year %in% years),\n    aes(x = fertility, y = life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_wrap(~ year)\n\n\n\n\n\nJust to illustrate, faceting makes sense for datasets other than\nscatterplots. This example also shows that faceting will apply to\nmultiple geom layers at once.\n\n\nThe dataset shows the abundances of five different bacteria across three\ndifferent subjects over time, as they were subjected to antibiotics. The\ndata were the basis for\nthis study.\n\n\n\nantibiotic <- read_csv(\"https://uwmadison.box.com/shared/static/5jmd9pku62291ek20lioevsw1c588ahx.csv\")\nhead(antibiotic)\n\n# A tibble: 6 × 7\n  species  sample value ind    time svalue antibiotic     \n  <chr>    <chr>  <dbl> <chr> <dbl>  <dbl> <chr>          \n1 Unc05qi6 D1         0 D         1   NA   Antibiotic-free\n2 Unc05qi6 D2         0 D         2   NA   Antibiotic-free\n3 Unc05qi6 D3         0 D         3    0   Antibiotic-free\n4 Unc05qi6 D4         0 D         4    0   Antibiotic-free\n5 Unc05qi6 D5         0 D         5    0   Antibiotic-free\n6 Unc05qi6 D6         0 D         6    0.2 Antibiotic-free\n\n\nI have also separately computed running averages for each of the\nvariables – this is in the svalue column. We’ll discuss\nways to do this during the week on time series visualization.\n\n\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\n\nIt seems like some of the species are much more abundant than others. In\nthis situation, it might make sense to rescale the \\(y\\)-axis.\nThough, this is always a risky decision – people might easily\nmisinterpret the plot and conclude that the different species all have\nthe same abundances. Nonetheless, it can’t hurt to try, using the\nscale argument to facet_grid.\n\n\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind, scale = \"free_y\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\n\nUnlike the years example, the facets don’t automatically come with their\nown natural order. We can define an order based on the average value of\nthe responses over the course of the survey, and then change the factor\nlevels of the Species column to reorder the panels.\n\n\n\nspecies_order <- antibiotic %>%\n  group_by(species) %>%\n  summarise(avg_value = mean(value)) %>%\n  arrange(desc(avg_value)) %>%\n  pull(species)\n\nantibiotic <- antibiotic %>%\n  mutate(species = factor(species, levels = species_order))\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind, scale = \"free_y\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week03-02/",
    "title": "Ridge Plots",
    "description": "An extended example of faceting with data summaries.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-24",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\nRidge plots are a special case of small multiples that are particularly\nsuited to displaying multiple parallel time series or densities.\n\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(ggridges)\ntheme_set(theme_bw())\n\n\n\nAn example ridge plot for the gapminder dataset is shown below. The\neffectiveness of this plot comes from the fact that multiple densities\ncan be displayed in close proximity to one another, making it possible\nto facet across many variables in a space-efficient way.\n\n\n\ndata(gapminder)\ngapminder <- gapminder %>%\n  mutate(\n    dollars_per_day = (gdp / population) / 365,\n    group = case_when(\n      region %in% c(\"Western Europe\", \"Northern Europe\",\"Southern Europe\",  \"Northern America\",  \"Australia and New Zealand\") ~ \"West\",\n      region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n      region %in% c(\"Caribbean\", \"Central America\",  \"South America\") ~ \"Latin America\",\n      continent == \"Africa\" &  region != \"Northern Africa\" ~ \"Sub-Saharan\", \n      TRUE ~ \"Others\"\n    ),\n    group = factor(group, levels = c(\"Others\", \"Latin America\",  \"East Asia\", \"Sub-Saharan\", \"West\")),\n    west = ifelse(group == \"West\", \"West\", \"Developing\")\n  )\n\n\n\n\npast_year <- 1970\ngapminder_subset <- gapminder %>%\n  filter(year == past_year, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +  \n  geom_density_ridges() +\n  scale_x_log10()\n\n\n\n\n\nYou might wonder, why not plot the raw data? We do this using a\ngeom_point below. The result isn’t as satisfying, because,\nwhile the shape of the ridges popped out immediately in the original\nplot, we have to invest a bit more effort to understand the regions of\nhigher density in the raw data plot. Also, when there are many samples,\nthe entire range might appeared covered in marks when in fact there are\nsome intervals with higher density than others.\n\n\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_point(shape = \"|\", size = 5) +\n  scale_x_log10()\n\n\n\n\n\nA nice compromise is to include both the raw positions and the smoothed\ndensities.\n\n\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_density_ridges(\n    jittered_points = TRUE, \n    position = position_points_jitter(height = 0),\n    point_shape = '|', point_size = 3\n  ) +\n  scale_x_log10()\n\n\n\n\n\nTo exercise our knowledge of both faceting and ridge plots, let’s study\na particular question in depth: How did the gap between rich and poor\ncountries change between 1970 and 2010?\n\n\nA first reasonable plot is to facet year and development status. While\nthe rich countries have become slightly richer, there is a larger shift\nin the incomes for the poorer countries.\n\n\n\npast_year <- 1970\npresent_year <- 2010\nyears <- c(past_year, present_year)\n\ngapminder_subset <- gapminder %>%\n  filter(year %in% years, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day)) +\n  geom_histogram(binwidth = 0.25) +\n  scale_x_log10() +\n  facet_grid(year ~ west)\n\n\n\n\n\nWe can express this message more compactly by overlaying densities, but\nthe attempt below is misleading, because it gives the same total area to\nboth groups of countries. This doesn’t make sense, considering there are\nmore than 4 times as many developing vs. western countries (87 vs. 21).\n\n\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\n\n\nWe can adjust the areas of the curves by directly referencing the\n..count variable, which is implicitly computed by ggplot2\nwhile it’s computing these densities.\n\n\n\nggplot(gapminder_subset, aes(dollars_per_day, y = ..count.., fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\n\n\nCan we attribute the changes to specific regions? Let’s apply our\nknowledge of ridge plots.\n\n\n\nggplot(gapminder_subset, aes(dollars_per_day, y = group, fill = as.factor(year))) +\n  geom_density_ridges(alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_x_log10()\n\n\n\n\n\nAlternatively, we can overlay densities from the different regions. Both\nplots suggest that much of the increase in incomes can be attributed to\ncountries in Latin America and East Asia.\n\n\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = group, y = ..count..)) +\n  geom_density(alpha = 0.7, bw = .12, position = \"stack\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_x_log10(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0, 0.15, 0)) +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week03-03/",
    "title": "Compound Figures",
    "description": "Showing different variables across subpanels.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-23",
    "categories": [],
    "contents": "\n\n\n\n\nReading,\nRecording,\nRmarkdown\n\n\n\nFaceting is useful whenever we want different rows of the data to appear\nin different panels. What if we want to compare different columns, or\nwork with several datasets? A more general alternative is to use\ncompound plots. The idea is to construct plots separately and then\ncombine them only at the very end.\n\n\n\n\nThe main advantage of compound plots is that individual panels can be\ntailored to specific visual comparisons, but relationships across panels\ncan also be studied. For example, the plot below shows change in the\ntotal number and composition of undergraduate majors over the last few\ndecades. In principle, the same information could be communicated using\na stacked area plot (geom_area). However, comparing the\npercentages for 1970 and 2015 is much more straightforward using a line\nplot, and we can still see changes in the overall number of degrees\nusing the area plot.\n\n\n\n\n\n\nFor reference, here is a non-compound display of the same information.\n\n\n\n\n\n\n\n\nThere are a few considerations that can substantially improve the\nquality of a compound plot,\n\n\nConsistent visual encodings for shared variables\n\n\nClear, but unobtrusive annotation\n\n\nProper alignment in figure baselines\n\n\nWe will discuss each point separately.\n\n\n\n\nThe figures below are compound plots of a dataset of athlete physiology.\nThey are very similar, but the second is better because it enforces a\nmore strict consistency in encodings across panels. Specifically, the\nmale / female variable is (1) encoded using the same color scheme across\nall panels and (2) ordered so that female repeatedly appears on the\nright of male.\n\n\n\n\n\n\nThe improved, visually consistent approach is given below.\n\n\n\n\n\n\n\n\nEffective annotation can be used to refer to different subpanels of the\ndata without drawing too much attention to itself. Labels should be\nvisible but subtle – not too large, similar fonts as the figures, and\nlogically ordered ((a) on top left). A nice heuristic is to think of\nthese annotations like page numbers. They are useful for making\nreferences, but aren’t something that is actively read.\n\n\n\n\n\n\n\n\nFor alignment, we will want figure baselines / borders to be consistent.\nMisalignment can be distracting. This is primarily a problem when\ncompound plots are made from manually. If we follow the programmatic\napproaches discussed in the next lecture, we won’t have this issue.\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week03-04/",
    "title": "Patchwork",
    "description": "Implementing compound figures in R",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-22",
    "categories": [],
    "contents": "\n\n\n\n\nReading,\nRecording,\nRmarkdown\n\n\n\nIn the last set of notes we discussed principles for designing effective\ncompound figures. In these notes, we’ll review the\npatchwork R package, which can be used to implement\ncompound figures.\n\n\n\n\nThis package creates a simple syntax for combining figures,\n\n\np1 + p2 concatenates two figures horizontally\n\n\np1 / p2 concatenates two figures vertically\n\n\nThis idea is simple, but becomes very powerful once we realize that we\ncan define a whole algebra on plot layouts,\n\n\np1 + p2 + p3 concatenates three figures horizontally\n\n\np1 / p2 / p3 concatenates three figures vertically\n\n\n(p1 + p2) / p3 Concatenates the first two figures\nhorizontally, and places the third below both.\n\n\n…\n\n\n\n\nBefore we illustrate the use of this package, let’s read in the athletes\ndata from the previous notes. The code below constructs the three\ncomponent plots that we want to combine. Though it looks like a lot of\ncode, it’s just because we are making several plots and styling each one\nof them. Conceptually, this is the same type of ggplot2\ncode that we have been using all semester – the only difference is that\nwe save all the figure objects into one list, instead of printing them\nright away.\n\n\n\nlibrary(tidyverse)\n library(patchwork)\n \n athletes <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat436_s23/main/data/athletes.csv\") %>%\n   filter(sport %in% c(\"basketball\", \"field\", \"rowing\", \"swimming\", \"tennis\", \"track (400m)\")) %>%\n   mutate(sex = recode(sex, \"m\" = \"male\", \"f\" = \"female\"))\n \n p <- list()\n p[[\"bar\"]] <- ggplot(count(athletes, sex)) +\n   geom_bar(aes(sex, n, fill = sex), stat = \"identity\") +\n   scale_y_continuous(expand = c(0, 0)) +\n   scale_fill_brewer(palette = \"Set1\") +\n   labs(y = \"number\")\n \n p[[\"scatter\"]] <- ggplot(athletes) +\n   geom_point(aes(rcc, wcc, col = sex)) +\n   scale_color_brewer(palette = \"Set1\") +\n   theme(legend.position = \"none\") +\n   labs(x = \"RBC count\", y = \"WBC Count\")\n \n p[[\"box\"]] <- ggplot(athletes) +\n   geom_boxplot(aes(sport, pcBfat, col = sex, fill = sex), alpha = 0.5) +\n   scale_color_brewer(palette = \"Set1\") +\n   scale_fill_brewer(palette = \"Set1\") +\n   theme(legend.position = \"none\") +\n   labs(y = \"% body fat\", x = NULL)\n\n\n\n\n\nNow, we use patchwork to combine the subplots using the\ndifferent combinations discussed above.\n\n\n\np[[\"bar\"]] + p[[\"scatter\"]] + p[[\"box\"]]\n\n\np[[\"bar\"]] / p[[\"scatter\"]] / p[[\"box\"]]\n\n\n(p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]]\n\n\n\n\n\n\n\nA corollary of using the same encodings across panels is that it should\nbe possible to share legends across the entire compound figure. This is\nmost concisely done by setting plot_layout(legend =\n“collect”). For example, compare the athlete physiology dataset\nwith and without the collected legends,\n\n\n\n(p[[\"bar\"]] + p[[\"scatter\"]] + theme(legend.position = \"left\")) / p[[\"box\"]] # turns legends back on\n\n\n\n\n\nThe version with the legends collected is given below.\n\n\n\n(p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n       plot_layout(guides = \"collect\") &\n       plot_annotation(theme = theme(legend.position = \"bottom\"))\n\n\n\n\n\n\n\nFor annotation, we can add a title to each figure individually using\nggtitle(), before they are combined into the compound\nfigure. The size and font of the titles can be adjusted by using the\ntheme(title = element_text(…)) option. For example, the\ncode below adds the a - c titles for each subpanel.\n\n\n\np[[\"bar\"]] <- p[[\"bar\"]] + ggtitle(\"a\")\n p[[\"scatter\"]] <- p[[\"scatter\"]] + ggtitle(\"b\")\n p[[\"box\"]] <- p[[\"box\"]] + ggtitle(\"c\")\n \n (p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n   plot_layout(guides = \"collect\") &\n   plot_annotation(theme = theme(legend.position = \"bottom\", title = element_text(size = 10)))\n\n\n\n\n\n\n\nPatchwork handles alignment in the background, but sometimes we might\nwant to have control over the relative sizes of different panels. For\nthis, we can again use the plot_layout function, this time\nusing the height and width arguments. For example, the two examples\nchange the height and widths of the first component in the layout.\n\n\n\n\n    (p[[\"bar\"]] + p[[\"scatter\"]] + plot_layout(widths = c(1, 3))) / p[[\"box\"]] +\n      plot_layout(guides = \"collect\")\n\n\n    (p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n      plot_layout(guides = \"collect\", heights = c(1, 3))\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week04-01/",
    "title": "Elements of a Shiny App",
    "description": "Vocabulary used by the R Shiny Library, and a few example apps.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-21",
    "categories": [],
    "contents": "\n\nRecording,\nCode\n\n\n\n\n\n\nAll Shiny apps are made up from the same few building blocks. These\nnotes review the main types of blocks. When reading code from more\ncomplex apps, it can be helpful to try to classify pieces of the code\ninto these types of blocks.\n\n\n\n\nThe highest level breakdown of Shiny app code is between ui\nand server components. The ui controls what\nthe app looks like. It stands for “User Interface.” The\nserver controls what the app does. For example,\nthe app below defines a title and textbox where users can type. But it\ndoes not do anything, since the server is empty.\n\n\n\nlibrary(shiny)\n \n ui <- fluidPage(\n   titlePanel(\"Hello!\"),\n   textInput(\"name\", \"Enter your name\")  # first arg is ID, second is label\n )\n \n server <- function(input, output) {}\n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\nThe UI elements can be further broken down into Inputs, Outputs, and\nDescriptors1,\nall grouped together by an organizing layout function. Inputs are UI\nelements that users can manipulate to prompt certain types of\ncomputation. Outputs are parts of the interface that reflects the result\nof a server computation. Descriptors are parts of the page\nthat aren’t involved in computation, but which provide narrative\nstructure and guide the user.\n\n\nFor example, in the toy app above, titlePage is a\ndescriptor providing some title text. textInput is an input\nelement allowing users to enter text. fluidPage is a layout\nfunction that arranges these elements on a continuous page (some other\nlayout functions are sidebarLayout,\nnavbarPage, flowLayout, …)\n\n\n\n\nAn important point is that all input and output elements must be given a\nunique ID. This is always the first argument of a Input\nor Output function defined in Shiny. The ID tags are\nhow different parts of the application are able to refer to one another.\nFor example, if we wanted to refer to the text the user entered in the\napplication above, we could refer to the name ID.\n\n\n\n\nLet’s see how to (1) make user inputs cause some sort of computation and\n(2) have the result of that computation appear to the user. For (1), we\nwill add a renderText element to the server.\nAll render* functions do two things,\n\n\nThey make inputs from the ui available for computation.\n\n\nThey generate HTML code that allows the results of the computation to\nappear in a UI output.\n\n\nFor (2), we will add a textOutput element to the\nui layout defined above. Let’s look at the code,\n\n\n\nlibrary(shiny)\n \n ui <- fluidPage(\n   titlePanel(\"Hello!\"),\n   textInput(\"name\", \"Enter your name\"),\n   textOutput(\"printed_name\")\n )\n \n server <- function(input, output) {\n   output$printed_name <- renderText({\n     paste0(\"Welcome to shiny, \", input$name, \"!\")\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\nThere are a few points worth noting. First, the renderText\ncomponent was able to refer to the value entered in the textbox using\ninput\\(name<\/code>. This was\npossible because <code>name<\/code> was the ID that we gave\nto the <code>textInput<\/code> component. It\nwould not have worked if we had used\n<code>input\\)text outside of a\nrender function: this is what we mean by the\nrender functions making the UI inputs available for\ncomputation. Finally, we were able to refer to the rendered output in\nthe UI by adding a textOutput component. By giving this\ncomponent the id printed_name, we were able to tell it to\nlook into the server for a rendered output named\nprinted_name and fill it in.\n\n\n\n\nAn even deeper idea is that the code did not simply run linearly, from\ntop of the script to the bottom. If that were all the code did, then it\nwould have run once at the beginning, and it would never have updated\nwhen you entered your name. Instead, it ran every time you typed\ninto the textbox. This is the “reactive programming” paradigm, and\nit is what makes interactive visualization possible.\nrenderText knows to rerun every time something is entered\ninto the name text input, because we told it to depend on\ninput\\(name<\/code>. We will\nexplore\nthe idea of reactivity in more depth in the next lecture, but for now,\njust\nremember that the order in which code is executed is not simply\ndetermined by\nthe order of lines in a file.<\/p><\/li>\n<li><p>Let’s look at a few more examples, just to get a feel\nfor things. The app\nbelow updates a plot of random normal variables given a mean specified\nby the\nuser. We’ve introduced a new type of input, a\n<code>numericInput<\/code>, which captures\nnumbers. We’ve also added a new output,\n<code>plotOutput<\/code>, allowing with its\naccompanying renderer, <code>renderPlot<\/code> (remember, UI\noutputs are always paired with\nserver renderers).<\/p>\n<div class=\"layout-chunk\"\ndata-layout=\"l-body\">\n<div class=\"sourceCode\">\n<pre class=\"sourceCode r\"><code\nclass=\"sourceCode r\"><span><span\nclass='kw'><a\nhref='https://rdrr.io/r/base/library.html'>library<\/a><\/span><span\nclass='op'>(<\/span><span class='va'><a\nhref='https://shiny.posit.co/'>shiny<\/a><\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='kw'><a\nhref='https://rdrr.io/r/base/library.html'>library<\/a><\/span><span\nclass='op'>(<\/span><span class='va'><a\nhref='https://tidyverse.tidyverse.org'>tidyverse<\/a><\/span><span\nclass='op'>)<\/span><\/span>\n<span><\/span>\n<span><span class='va'>ui<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/fluidPage.html'>fluidPage<\/a><\/span><span\nclass='op'>(<\/span><\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/titlePanel.html'>titlePanel<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"Random Normals\"<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/numericInput.html'>numericInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"mean\"<\/span>, <span\nclass='st'>\"Enter the mean\"<\/span>, <span\nclass='fl'>0<\/span><span\nclass='op'>)<\/span>, <span class='co'># 0\nis the default<\/span><\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/plotOutput.html'>plotOutput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"histogram\"<\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='op'>)<\/span><\/span>\n<span><\/span>\n<span><span class='va'>server<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='kw'>function<\/span><span\nclass='op'>(<\/span><span\nclass='va'>input<\/span>, <span\nclass='va'>output<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>{<\/span><\/span>\n<span>  <span class='va'>output<\/span><span\nclass='op'>\\)histogram\n<- renderPlot({  data.frame(values = rnorm(100, input\\(<\/span><span\nclass='va'>mean<\/span><span\nclass='op'>)<\/span><span\nclass='op'>)<\/span> <span\nclass='op'><a\nhref='https://magrittr.tidyverse.org/reference/pipe.html'>%&gt;%<\/a><\/span><\/span>\n<span>      <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/ggplot.html'>ggplot<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>+<\/span><\/span>\n<span>        <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/geom_histogram.html'>geom_histogram<\/a><\/span><span\nclass='op'>(<\/span><span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/aes.html'>aes<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='va'>values<\/span><span\nclass='op'>)<\/span><span\nclass='op'>)<\/span><\/span>\n<span>  <span class='op'>}<\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='op'>}<\/span><\/span>\n<span><\/span>\n<span><span class='va'>app<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/shinyApp.html'>shinyApp<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='va'>ui<\/span>, <span\nclass='va'>server<\/span><span\nclass='op'>)<\/span><\/span><\/code><\/pre>\n<\/div>\n<\/div>\n<p><iframe\nsrc=\"https://connect.doit.wisc.edu/content/758dd969-32e0-4722-8e9a-2c7e9364d644/\"\nallowfullscreen=\"\" data-external=\"1\" height=535\nwidth=600><\/iframe><\/p><\/li>\n<li><p>We can make the plot depend on several inputs. The\ncode below allows the user\nto change the total number of data points and the variance, this time\nusing\nslider inputs. I recommend taking a look at different inputs on the\nshiny\n<a\nhref=\"https://shiny.rstudio.com/images/shiny-cheatsheet.pdf\">cheatsheet<\/a>,\nthough be\naware that there are many\n<a\nhref=\"https://github.com/nanxstats/awesome-shiny-extensions\">extensions<\/a>\nbuilt by the\ncommunity.<\/p>\n<div class=\"layout-chunk\"\ndata-layout=\"l-body\">\n<div class=\"sourceCode\">\n<pre class=\"sourceCode r\"><code\nclass=\"sourceCode r\"><span><span\nclass='kw'><a\nhref='https://rdrr.io/r/base/library.html'>library<\/a><\/span><span\nclass='op'>(<\/span><span class='va'><a\nhref='https://shiny.posit.co/'>shiny<\/a><\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='kw'><a\nhref='https://rdrr.io/r/base/library.html'>library<\/a><\/span><span\nclass='op'>(<\/span><span class='va'><a\nhref='https://tidyverse.tidyverse.org'>tidyverse<\/a><\/span><span\nclass='op'>)<\/span><\/span>\n<span><\/span>\n<span><span class='va'>ui<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/fluidPage.html'>fluidPage<\/a><\/span><span\nclass='op'>(<\/span><\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/titlePanel.html'>titlePanel<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"Random Normals\"<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/numericInput.html'>numericInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"mean\"<\/span>, <span\nclass='st'>\"Enter the mean\"<\/span>, <span\nclass='fl'>0<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/sliderInput.html'>sliderInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"n\"<\/span>, <span\nclass='st'>\"Enter the number of\nsamples\"<\/span>, <span\nclass='fl'>500<\/span>, min<span\nclass='op'>=<\/span><span\nclass='fl'>1<\/span>, max<span\nclass='op'>=<\/span><span\nclass='fl'>2000<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/sliderInput.html'>sliderInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"sigma\"<\/span>, <span\nclass='st'>\"Enter the standard\ndeviation\"<\/span>, <span\nclass='fl'>1<\/span>, min<span\nclass='op'>=<\/span><span\nclass='fl'>.1<\/span>, max<span\nclass='op'>=<\/span><span\nclass='fl'>5<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/plotOutput.html'>plotOutput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"histogram\"<\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='op'>)<\/span><\/span>\n<span><\/span>\n<span><span class='va'>server<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='kw'>function<\/span><span\nclass='op'>(<\/span><span\nclass='va'>input<\/span>, <span\nclass='va'>output<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>{<\/span><\/span>\n<span>  <span class='va'>output<\/span><span\nclass='op'>\\)histogram\n<- renderPlot({  data.frame(values = rnorm(input\\(<\/span><span\nclass='va'>n<\/span>, <span\nclass='va'>input<\/span><span\nclass='op'>\\)mean,\ninput\\(<\/span><span\nclass='va'>sigma<\/span><span\nclass='op'>)<\/span><span\nclass='op'>)<\/span> <span\nclass='op'><a\nhref='https://magrittr.tidyverse.org/reference/pipe.html'>%&gt;%<\/a><\/span><\/span>\n<span>      <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/ggplot.html'>ggplot<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>+<\/span><\/span>\n<span>        <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/geom_histogram.html'>geom_histogram<\/a><\/span><span\nclass='op'>(<\/span><span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/aes.html'>aes<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='va'>values<\/span><span\nclass='op'>)<\/span>, bins <span\nclass='op'>=<\/span> <span\nclass='fl'>100<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>+<\/span><\/span>\n<span>        <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/lims.html'>xlim<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='op'>-<\/span><span\nclass='fl'>10<\/span>, <span\nclass='fl'>10<\/span><span\nclass='op'>)<\/span><\/span>\n<span>  <span class='op'>}<\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='op'>}<\/span><\/span>\n<span><\/span>\n<span><span class='va'>app<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/shinyApp.html'>shinyApp<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='va'>ui<\/span>, <span\nclass='va'>server<\/span><span\nclass='op'>)<\/span><\/span><\/code><\/pre>\n<\/div>\n<\/div>\n<p><iframe\nsrc=\"https://connect.doit.wisc.edu/content/6a50080d-d900-450b-97e4-4604e3ce2c5f/\"\nallowfullscreen=\"\" data-external=\"1\" height=735\nwidth=600><\/iframe><\/p><\/li>\n<li><p>We can also make the app return several outputs, not\njust a plot. The code\nbelow attempts to print the data along in addition to the histogram, but\nit\nmakes a crucial mistake (can you spot it?).<\/p>\n<div class=\"layout-chunk\"\ndata-layout=\"l-body\">\n<div class=\"sourceCode\">\n<pre class=\"sourceCode r\"><code\nclass=\"sourceCode r\"><span><span\nclass='kw'><a\nhref='https://rdrr.io/r/base/library.html'>library<\/a><\/span><span\nclass='op'>(<\/span><span class='va'><a\nhref='https://shiny.posit.co/'>shiny<\/a><\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='kw'><a\nhref='https://rdrr.io/r/base/library.html'>library<\/a><\/span><span\nclass='op'>(<\/span><span class='va'><a\nhref='https://tidyverse.tidyverse.org'>tidyverse<\/a><\/span><span\nclass='op'>)<\/span><\/span>\n<span><\/span>\n<span><span class='va'>ui<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/fluidPage.html'>fluidPage<\/a><\/span><span\nclass='op'>(<\/span><\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/titlePanel.html'>titlePanel<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"Random Normals\"<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/numericInput.html'>numericInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"mean\"<\/span>, <span\nclass='st'>\"Enter the mean\"<\/span>, <span\nclass='fl'>0<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/sliderInput.html'>sliderInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"n\"<\/span>, <span\nclass='st'>\"Enter the number of\nsamples\"<\/span>, <span\nclass='fl'>500<\/span>, min<span\nclass='op'>=<\/span><span\nclass='fl'>1<\/span>, max<span\nclass='op'>=<\/span><span\nclass='fl'>2000<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/sliderInput.html'>sliderInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"sigma\"<\/span>, <span\nclass='st'>\"Enter the standard\ndeviation\"<\/span>, <span\nclass='fl'>1<\/span>, min<span\nclass='op'>=<\/span><span\nclass='fl'>.1<\/span>, max<span\nclass='op'>=<\/span><span\nclass='fl'>5<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/plotOutput.html'>plotOutput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"histogram\"<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/renderDataTable.html'>dataTableOutput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"dt\"<\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='op'>)<\/span><\/span>\n<span><\/span>\n<span><span class='va'>server<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='kw'>function<\/span><span\nclass='op'>(<\/span><span\nclass='va'>input<\/span>, <span\nclass='va'>output<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>{<\/span><\/span>\n<span>  <span class='va'>output<\/span><span\nclass='op'>\\)histogram\n<- renderPlot({  data.frame(values = rnorm(input\\(<\/span><span\nclass='va'>n<\/span>, <span\nclass='va'>input<\/span><span\nclass='op'>\\)mean,\ninput\\(<\/span><span\nclass='va'>sigma<\/span><span\nclass='op'>)<\/span><span\nclass='op'>)<\/span> <span\nclass='op'><a\nhref='https://magrittr.tidyverse.org/reference/pipe.html'>%&gt;%<\/a><\/span><\/span>\n<span>      <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/ggplot.html'>ggplot<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>+<\/span><\/span>\n<span>        <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/geom_histogram.html'>geom_histogram<\/a><\/span><span\nclass='op'>(<\/span><span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/aes.html'>aes<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='va'>values<\/span><span\nclass='op'>)<\/span>, bins <span\nclass='op'>=<\/span> <span\nclass='fl'>100<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>+<\/span><\/span>\n<span>        <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/lims.html'>xlim<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='op'>-<\/span><span\nclass='fl'>10<\/span>, <span\nclass='fl'>10<\/span><span\nclass='op'>)<\/span><\/span>\n<span>  <span class='op'>}<\/span><span\nclass='op'>)<\/span><\/span>\n<span>  <\/span>\n<span>  <span class='va'>output<\/span><span\nclass='op'>\\)dt <- renderDataTable({  data.frame(values = rnorm(input\\(<\/span><span\nclass='va'>n<\/span>, <span\nclass='va'>input<\/span><span\nclass='op'>\\)mean,\ninput\\(<\/span><span\nclass='va'>sigma<\/span><span\nclass='op'>)<\/span><span\nclass='op'>)<\/span><\/span>\n<span>  <span class='op'>}<\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='op'>}<\/span><\/span>\n<span><\/span>\n<span><span class='va'>app<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/shinyApp.html'>shinyApp<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='va'>ui<\/span>, <span\nclass='va'>server<\/span><span\nclass='op'>)<\/span><\/span><\/code><\/pre>\n<\/div>\n<\/div>\n<p><iframe\nsrc=\"https://connect.doit.wisc.edu/content/20911ef0-057c-4934-92cd-d5a7846783d1/\"\nallowfullscreen=\"\" data-external=\"1\" height=935\nwidth=600><\/iframe><\/p><\/li>\n<li><p>The issue is that this code reruns\n<code>rnorm<\/code> for each output. So, even though\nthe interfaces suggests that the printed samples are the same as the\nones in the\nhistogram, they are actually different. To resolve this, we need a way\nof\nstoring an intermediate computation which (1) depends on the inputs but\n(2)\nfeeds into several outputs. Whenever we encounter this need, we can use\na\nreactive expression. It is a type of server element that depends on the\ninput\nand can be referred to directly by outputs, which call the reactive\nexpression\nlike a function. For example, the code below generates the random normal\nsamples\na single time, using the <code>samples()<\/code> reactive\nexpression.<\/p>\n<div class=\"layout-chunk\"\ndata-layout=\"l-body\">\n<div class=\"sourceCode\">\n<pre class=\"sourceCode r\"><code\nclass=\"sourceCode r\"><span><span\nclass='kw'><a\nhref='https://rdrr.io/r/base/library.html'>library<\/a><\/span><span\nclass='op'>(<\/span><span class='va'><a\nhref='https://shiny.posit.co/'>shiny<\/a><\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='kw'><a\nhref='https://rdrr.io/r/base/library.html'>library<\/a><\/span><span\nclass='op'>(<\/span><span class='va'><a\nhref='https://tidyverse.tidyverse.org'>tidyverse<\/a><\/span><span\nclass='op'>)<\/span><\/span>\n<span><\/span>\n<span><span class='va'>ui<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/fluidPage.html'>fluidPage<\/a><\/span><span\nclass='op'>(<\/span><\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/titlePanel.html'>titlePanel<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"Random Normals\"<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/numericInput.html'>numericInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"mean\"<\/span>, <span\nclass='st'>\"Enter the mean\"<\/span>, <span\nclass='fl'>0<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/sliderInput.html'>sliderInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"n\"<\/span>, <span\nclass='st'>\"Enter the number of\nsamples\"<\/span>, <span\nclass='fl'>500<\/span>, min<span\nclass='op'>=<\/span><span\nclass='fl'>1<\/span>, max<span\nclass='op'>=<\/span><span\nclass='fl'>2000<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/sliderInput.html'>sliderInput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"sigma\"<\/span>, <span\nclass='st'>\"Enter the standard\ndeviation\"<\/span>, <span\nclass='fl'>1<\/span>, min<span\nclass='op'>=<\/span><span\nclass='fl'>.1<\/span>, max<span\nclass='op'>=<\/span><span\nclass='fl'>5<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/plotOutput.html'>plotOutput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"histogram\"<\/span><span\nclass='op'>)<\/span>,<\/span>\n<span>  <span class='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/renderDataTable.html'>dataTableOutput<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='st'>\"dt\"<\/span><span\nclass='op'>)<\/span><\/span>\n<span><span class='op'>)<\/span><\/span>\n<span><\/span>\n<span><span class='va'>server<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='kw'>function<\/span><span\nclass='op'>(<\/span><span\nclass='va'>input<\/span>, <span\nclass='va'>output<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>{<\/span><\/span>\n<span>  <span class='va'>samples<\/span>\n<span class='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/reactive.html'>reactive<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='op'>{<\/span><\/span>\n<span>    <span class='fu'><a\nhref='https://rdrr.io/r/base/data.frame.html'>data.frame<\/a><\/span><span\nclass='op'>(<\/span>values <span\nclass='op'>=<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/r/stats/Normal.html'>rnorm<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='va'>input<\/span><span\nclass='op'>\\)n, input\\(<\/span><span\nclass='va'>mean<\/span>, <span\nclass='va'>input<\/span><span\nclass='op'>\\)sigma))  })  \n output\\(<\/span><span\nclass='va'>histogram<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='fu'><a\nhref='https://rdrr.io/pkg/shiny/man/renderPlot.html'>renderPlot<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='op'>{<\/span><\/span>\n<span>      <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/ggplot.html'>ggplot<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='fu'>samples<\/span><span\nclass='op'>(<\/span><span\nclass='op'>)<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>+<\/span><\/span>\n<span>        <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/geom_histogram.html'>geom_histogram<\/a><\/span><span\nclass='op'>(<\/span><span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/aes.html'>aes<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='va'>values<\/span><span\nclass='op'>)<\/span>, bins <span\nclass='op'>=<\/span> <span\nclass='fl'>100<\/span><span\nclass='op'>)<\/span> <span\nclass='op'>+<\/span><\/span>\n<span>        <span class='fu'><a\nhref='https://ggplot2.tidyverse.org/reference/lims.html'>xlim<\/a><\/span><span\nclass='op'>(<\/span><span\nclass='op'>-<\/span><span\nclass='fl'>10<\/span>, <span\nclass='fl'>10<\/span><span\nclass='op'>)<\/span><\/span>\n<span>  <span class='op'>}<\/span><span\nclass='op'>)<\/span><\/span>\n<span>  <\/span>\n<span>  <span class='va'>output<\/span><span\nclass='op'>\\)dt <- renderDataTable(samples()) }\n app <- shinyApp(ui, server)\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week04-02/",
    "title": "Introduction to Reactivity",
    "description": "Viewing shiny code execution as a graph.",
    "author": [],
    "date": "2024-12-20",
    "categories": [],
    "contents": "\n\nRecording,\nCode\n\n\n\n\n\n\nThese notes will explore the idea of reactivity in more depth. Recall\nthat reactivity refers to the fact that Shiny app code is not run from\ntop to bottom, like an ordinary R script. Instead, it runs reactively,\ndepending on inputs that the user has provided. This can make writing\nShiny code a bit unintuitive at first, but there are a few higher-level\nconcepts that can help when writing reactive code.\n\n\n\n\nThe most important of these concepts is that reactive code can be viewed\nas a graph. The ui and server define an\nexplicit dependency structure for how components depend on one another.\nThe input\\(<\/code>’s within\n<code>render*<\/code> functions in\nthe server specify how UI inputs affect server computations. The IDs\nwithin the\n<code>*Output<\/code> elements in the\n<code>ui<\/code> specify which of the rendered\n<code>output\\)’s in the server should be used to\npopulate the visible interface.\n\n\n\n\nFor example, our first “Hello” app has the following (simple) reactivity\ngraph. Note that I’ve drawn input and output nodes differently, to\nemphasize the flow of computation. I’ve also copied the code from the\noriginal app for reference.\n\n\n\n\n\n\n\nlibrary(shiny)\n \n ui <- fluidPage(\n   titlePanel(\"Hello!\"),\n   textInput(\"name\", \"Enter your name\"),\n   textOutput(\"printed_name\")\n )\n \n server <- function(input, output) {\n   output$printed_name <- renderText({\n     paste0(\"Welcome to shiny, \", input$name, \"!\")\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\nEven though the graph is simple, note that the outputs will be\nrecomputed each time that the input is changed. For more general graphs,\nall downstream nodes will be re-executed whenever an upstream source is\nchanged (typically by a user input, though it’s possible to trigger\nchanges automatically).\n\n\n\n\nReactive expressions provide a special kind of node that live between\ninputs and outputs. They depend on inputs, and they feed into outputs,\nbut they are never made directly visible to the user. This is why we’ve\ndrawn them as a kind of special intermediate node. Below, I’ve drawn the\ngraph for our random normal plotter, with the reactive\nsamples() expression.\n\n\n\n\n\n\n\nlibrary(shiny)\n library(tidyverse)\n \n ### Functions within app components\n generate_data <- function(n, mean, sigma) {\n   data.frame(values = rnorm(n, mean, sigma))\n }\n \n histogram_fun <- function(df) {\n   ggplot(df) +\n     geom_histogram(aes(values), bins = 100) +\n     xlim(-10, 10)\n }\n \n ### Defines the app\n ui <- fluidPage(\n   titlePanel(\"Random Normals\"),\n   numericInput(\"mean\", \"Enter the mean\", 0),\n   sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n   sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n   plotOutput(\"histogram\"),\n   dataTableOutput(\"dt\")\n )\n \n server <- function(input, output) {\n   samples <- reactive({\n     generate_data(input$n, input$mean, input$sigma)\n   })\n   output$histogram <- renderPlot(histogram_fun(samples()))\n   output$dt <- renderDataTable(samples())\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\nA useful perspective is to think of reactive expressions as simplifying\nthe overall reactivity graph. Specifically, by adding a reactive node,\nit’s possible to trim away many edges. For example, our initial\nimplementation of the random normal plotter (which didn’t use the\nreactive expression) has a much more complicated graph, since many\ninputs feed directly into outputs.\n\n\n\n\n\n\n\n\nLet’s see these principles in action for a similar, but more complex\napp. The app below can be used for power analysis. It simulates two\ngroups of samples, both from normal distributions, but with different\n(user specified) means. We’ve used a reactive expression to generate the\nsamples, so that both the histogram and hypothesis test result outputs\ncan refer to the same intermediate simulated data.\n\n\n\nlibrary(shiny)\n library(tidyverse)\n library(broom)\n \n ### Functions within app components\n generate_data <- function(n, mean1, mean2, sigma) {\n   data.frame(\n     values = c(rnorm(n, mean1, sigma), rnorm(n, mean2, sigma)),\n     group = rep(c(\"A\", \"B\"), each = n)\n   )\n }\n \n histogram_fun <- function(df) {\n   ggplot(df) +\n     geom_histogram(\n       aes(values, fill = group), \n       bins = 100, position = \"identity\",\n       alpha = 0.8\n     ) +\n     xlim(-10, 10)\n }\n \n test_fun <- function(df) {\n   t.test(values ~ group, data = df) %>%\n     tidy() %>%\n     select(p.value, conf.low, conf.high)\n }\n \n ### Defines the app\n ui <- fluidPage(\n   sidebarLayout(\n     sidebarPanel(\n       sliderInput(\"mean1\", \"Mean (Group 1)\", 0, min = -10.0, max = 10.0, step = 0.1),\n       sliderInput(\"mean2\", \"Mean (Group 2)\", 0, min = -10, max = 10, step = 0.1),\n       sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n       sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n     ),\n     mainPanel(\n       plotOutput(\"histogram\"),\n       dataTableOutput(\"test_result\")\n     )\n   )\n )\n \n server <- function(input, output) {\n   samples <- reactive({\n     generate_data(input$n, input$mean1, input$mean2, input$sigma)\n   })\n   output$histogram <- renderPlot(histogram_fun(generate_data(input$n, input$mean1, input$mean2, input$sigma)))\n   output$test_result <- renderDataTable(test_fun(generate_data(input$n, input$mean1, input$mean2, input$sigma)))\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\nOther than that, the only difference is that I’ve saved output from the\nt.test using test_result. Notice the use of\nthe broom package, which helps format the test output into\na data.frame.\n\n\n\n\nSo far, all of our reactive code has lived within the\nrender or reactive() sets of functions.\nHowever, there is a another kind that is often useful, especially in\nmore advanced applications: observers. An observer is a\ncomputation that is done every time certain inputs are changed, but\nwhich don’t affect downstream UI outputs through a\nrender function. For example, below, we’ve added a\nblock (under observeEvent) that prints to the console every\ntime either of the means are changed. I realize it is a bit of a mystery\nwhy these functions would ever be useful, but we will see them in more\nrealistic contexts next week.\n\n\n\nlibrary(shiny)\n library(tidyverse)\n library(broom)\n \n ### Functions within app components\n generate_data <- function(n, mean1, mean2, sigma) {\n   data.frame(\n     values = c(rnorm(n, mean1, sigma), rnorm(n, mean2, sigma)),\n     group = rep(c(\"A\", \"B\"), each = n)\n   )\n }\n \n histogram_fun <- function(df) {\n   ggplot(df) +\n     geom_histogram(\n       aes(values, fill = group), \n       bins = 100, position = \"identity\",\n       alpha = 0.8\n     ) +\n     xlim(-10, 10)\n }\n \n test_fun <- function(df) {\n   t.test(values ~ group, data = df) %>%\n     tidy() %>%\n     select(p.value, conf.low, conf.high)\n }\n \n ### Defines the app\n ui <- fluidPage(\n   sidebarLayout(\n     sidebarPanel(\n       sliderInput(\"mean1\", \"Mean (Group 1)\", 0, min = -10.0, max = 10.0, step = 0.1),\n       sliderInput(\"mean2\", \"Mean (Group 2)\", 0, min = -10, max = 10, step = 0.1),\n       sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n       sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n     ),\n     mainPanel(\n       plotOutput(\"histogram\"),\n       dataTableOutput(\"test_result\")\n     )\n   )\n )\n \n server <- function(input, output) {\n   samples <- reactive({\n     generate_data(input$n, input$mean1, input$mean2, input$sigma)\n   })\n   output$histogram <- renderPlot(histogram_fun(samples()))\n   output$test_result <- renderDataTable(test_fun(samples()))\n   observeEvent(input$mean1 | input$mean2, {\n     message(\"group 1 mean is now: \", input$mean1)\n     message(\"group 2 mean is now: \", input$mean2)\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week04-03/",
    "title": "IMDB Shiny Application",
    "description": "Using Shiny to explore a movies dataset",
    "author": [],
    "date": "2024-12-19",
    "categories": [],
    "contents": "\n\nRecording,\nCode\n\n\n\n\n\n\nSo far, all of our Shiny applications have been based on toy simulated\ndata. In this set of notes, we’ll use Shiny to explore a real dataset,\nillustrating the general development workflow in the process. Before\ndiving into code, let’s consider the role of interactivity in data\nanalysis.\n\n\n\n\nA major difference between doing visualization on paper and on computers\nis that visualization on computers can make use of interactivity. An\ninteractive visualization is one that changes in response to user cues.\nThis allows a display to update in a way that provides a visual\ncomparison that was not available in a previous view. In this way,\ninteractive visualization allows users to answer a sequence of\nquestions.\n\n\n\n\nSelection, both of observations and of attributes, is fundamental to\ninteractive visualization. This is because it precedes other interactive\noperations: you can select a subset of observations to filter down to or\nattributes to coordinate across multiple displays (we consider both\ntypes of interactivity in later lectures).\n\n\n\n\nThe code below selects movies to highlight based on Genre. We use a\nselectInput to create the dropdown menu. A reactive\nexpression creates a new column (selected) in the\nmovies dataset specifiying whether the current movie is\nselected. The reactive graph structure means that the ggplot2 figure is\nrecreated each time the selection is changed, and the\nselected column is used to shade in the points. This\nprocess of changing the visual encoding of graphical marks depending on\nuser selections is called “conditional encoding.”\n\n\n\nlibrary(shiny)\n library(tidyverse)\n library(lubridate)\n \n movies <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %>%\n   mutate(\n     date = as_date(Release_Date, format = \"%b %d %Y\"),\n     year = year(date),\n     Major_Genre = fct_explicit_na(Major_Genre),\n     MPAA_Rating = fct_explicit_na(MPAA_Rating),\n   )\n \n genres <- pull(movies, Major_Genre) %>%\n   unique() %>%\n   na.omit()\n \n ### functions used in app\n scatterplot <- function(df) {\n   ggplot(df) +\n     geom_point(\n       aes(Rotten_Tomatoes_Rating, IMDB_Rating, size = selected, alpha = selected)\n     ) +\n     scale_size(limits = c(0, 1), range = c(.5, 2), guide = \"none\") +\n     scale_alpha(limits = c(0, 1), range = c(.1, 1), guide = \"none\")\n }\n \n ### definition of app\n ui <- fluidPage(\n   titlePanel(\"IMDB Analysis\"),\n   selectInput(\"genres\", \"Genre\", genres),\n   plotOutput(\"ratings_scatter\")\n )\n \n server <- function(input, output) {\n   movies_subset <- reactive({\n     movies %>%\n       mutate(selected = 1 * (Major_Genre %in% input$genres))\n   })\n   \n   output$ratings_scatter <- renderPlot({\n     scatterplot(movies_subset())\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can extend this further. Let’s allow the user to filter by year and\nMPAA rating. Notice that there are some years in the future! We also\nfind that there are systematic differences in IMDB and Rotten Tomatoes\nratings as a function of these variables.\n\n\n\nlibrary(shiny)\n library(tidyverse)\n library(lubridate)\n \n movies <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %>%\n   mutate(\n     date = as_date(Release_Date, format = \"%b %d %Y\"),\n     year = year(date),\n     Major_Genre = fct_explicit_na(Major_Genre),\n     MPAA_Rating = fct_explicit_na(MPAA_Rating),\n   )\n \n genres <- pull(movies, Major_Genre) %>%\n   unique() %>%\n   na.omit()\n ratings <- pull(movies, MPAA_Rating) %>%\n   unique() %>%\n   na.omit()\n \n ### functions used in app\n scatterplot <- function(df) {\n   ggplot(df) +\n     geom_point(\n       aes(Rotten_Tomatoes_Rating, IMDB_Rating, size = selected, alpha = selected)\n     ) +\n     scale_size(limits = c(0, 1), range = c(.5, 2), guide = \"none\") +\n     scale_alpha(limits = c(0, 1), range = c(.1, 1), guide = \"none\")\n }\n \n ### definition of app\n ui <- fluidPage(\n   titlePanel(\"IMDB Analysis\"),\n   selectInput(\"genres\", \"Genre\", genres, multiple = TRUE),\n   checkboxGroupInput(\"mpaa\", \"MPAA Rating\", ratings, ratings),\n   sliderInput(\"year\", \"Year\", min = min(movies$year), max = max(movies$year), c(1928, 2020), sep = \"\"),\n   plotOutput(\"ratings_scatter\")\n )\n \n server <- function(input, output) {\n   movies_subset <- reactive({\n     movies %>%\n       mutate(selected = 1 * (\n         (Major_Genre %in% input$genres) &\n         (MPAA_Rating %in% input$mpaa) &\n         (year >= input$year[1]) &\n         (year <= input$year[2])\n       ))\n   })\n   \n   output$ratings_scatter <- renderPlot({\n     scatterplot(movies_subset())\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ll include a final version of this plot which additionally shows the\nmovie name when points are hovered. To accomplish this, we can no longer\nuse ggplot2 on its own – it has to be linked with a\nplotting library that renders web-based visualizations (not just static\nimage files). This is what the ggplotly() call does in the\nupdated version of the app. The mouseover text is added through the\ntooltip argument.\n\n\n\nlibrary(shiny)\n library(tidyverse)\n library(lubridate)\n library(plotly)\n \n movies <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %>%\n   mutate(\n     date = as_date(Release_Date, format = \"%b %d %Y\"),\n     year = year(date),\n     Major_Genre = fct_explicit_na(Major_Genre),\n     MPAA_Rating = fct_explicit_na(MPAA_Rating),\n   )\n \n genres <- pull(movies, Major_Genre) %>%\n   unique() %>%\n   na.omit()\n ratings <- pull(movies, MPAA_Rating) %>%\n   unique() %>%\n   na.omit()\n \n ### functions used in app\n scatterplot <- function(df) {\n   p <- ggplot(mapping = aes(Rotten_Tomatoes_Rating, IMDB_Rating)) +\n     geom_point(data = df %>% filter(selected),  aes(text = Title), size = 2, alpha = 1) +\n     geom_point(data = df %>% filter(!selected),  size = .5, alpha = .1)\n   ggplotly(p, tooltip = \"Title\") %>%\n     style(hoveron = \"fill\")\n }\n \n ### definition of app\n ui <- fluidPage(\n   titlePanel(\"IMDB Analysis\"),\n   selectInput(\"genres\", \"Genre\", genres),\n   checkboxGroupInput(\"mpaa\", \"MPAA Rating\", ratings, ratings),\n   sliderInput(\"year\", \"Year\", min = min(movies$year), max = max(movies$year), c(1928, 2020), sep = \"\"),\n   plotlyOutput(\"ratings_scatter\")\n )\n \n server <- function(input, output) {\n   movies_subset <- reactive({\n     movies %>%\n       mutate(selected = (\n         (Major_Genre %in% input$genres) &\n         (MPAA_Rating %in% input$mpaa) &\n         (year >= input$year[1]) &\n         (year <= input$year[2])\n       ))\n   })\n   \n   output$ratings_scatter <- renderPlotly({\n     scatterplot(movies_subset())\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\nThese visualizations are an instance of the more general idea of using\nfiltering to reduce complexity in data. Filtering is an especially\npowerful technique in the interactive paradigm, where it is possible to\neasily reverse (or compare) filtering choices.\n\n\n\n\nConceptually, what we are doing falls under the name of “Dynamic\nQuerying,” which refers more generally to updating a visualization based\non user queries. There are several ways to think about these dynamic\nqueries,\n\n\nInterpretation 1: Dynamic queries create the visual analog of a database\ninteraction. Rather than using a programming-based interface to filter\nelements or select attributes, we can design interactive visual\nequivalents.\n\n\nInterpretation 2: Dynamic queries allow rapid evaluation of conditional\nprobabilities. The visualization above was designed to answer: What is\nthe joint distribution of movie ratings, conditional on being a drama?\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week05-01/",
    "title": "Graphical Queries - Click Events",
    "description": "An introduction to click events in Shiny",
    "author": [],
    "date": "2024-12-18",
    "categories": [],
    "contents": "\n\nCode,\nRecording\n\n\n\n\n\n\nSome of the most sophisticated interactive data visualizations are based\non the idea that user queries can themselves be defined visually. For\nexample, to select a date range, we could directly interact with a time\nseries plot, rather than relying on a slider input. Or, instead of a\nlong dropdown menu of items, a user could select items by clicking on\nbars in a bar plot. There are many variations of this idea, but they all\nleverage graphical (rather than textual) displays to define queries. The\nadvantage of this approach is that it increases information density –\nthe selection inputs themselves encode data.\n\n\n\n\nTo implement this in Shiny, we first need a way of registering user\ninteractions on plots themselves. We will consider two types of plot\ninteraction mechanisms: clicks and brushes. These can be specified by\nadding click or brush events to\nplotOutput objects.\n\n\n\n\nThis creates a UI with a single plot on which we will be able to track\nuser clicks,\n\n\n\nui <- fluidPage(\n   plotOutput(\"plot\", click = \"plot_click\")\n )\n\n\n\nHere, plot_click is an ID that can be used as\ninput\\(plot_click<\/code> in the\nserver. We could name it however we want, but we need to be consistent\nacross the UI and server (just like ordinary, non-graphical\ninputs).<\/p><\/li>\n<li><p>Before, we just needed to place the\n<code>input\\)id items within render and\nreactive server components, and the associated outputs\nwould automatically know to redraw each time the value of any input was\nchanged. Clicks are treated slightly differently. We have to both (a)\nrecognize when a click event has occurred and (b) extract relevant\ninformation about what the click was referring to.\n\n\n\n\nFor (a), we generally use observeEvent,\n\n\n\nobserveEvent(\n  input$plot_click,\n  ... things to do when the plot is clicked ...\n)\n\n\n\nThis piece of code will be run anytime the plot is clicked.\n\n\n\n\nFor (b), we can use the nearPoints helper function. Suppose\nthe plot was made using the data.frame x. Then\n\n\n\nnearPoints(x, input$click)\n\n\n\nwill return the samples in x that are close to the clicked\nlocation. We will often use a variant of this code that doesn’t just\nreturn the closeby samples – it returns all samples, along with their\ndistance from the clicked location,\n\n\n\nnearPoints(x, input$click, allRows = TRUE, addDist = TRUE)\n\n\n\n\n\nWe are almost ready to build a visualization whose outputs respond to\ngraphical queries. Suppose we want a scatterplot where point sizes\nupdate according to their distance from the user’s click. Everytime the\nplot is clicked, we need to update the set of distances between samples\nand the clicked point. We then need to rerender the plot to reflect the\nnew distances. This logic is captured by the block below,\n\n\n\nserver <- function(input, output) {\n   dist <- reactiveVal(rep(1, nrow(x)))\n   observeEvent(\n     input$plot_click,\n     dist(reset_dist(x, input$plot_click))\n   )\n   \n   output$plot <- renderPlot({\n     scatter(x, dist())\n   })\n }\n\n\n\nThe code above uses one new concept, the reactiveVal on the\nfirst line of the function. It is a variable that doesn’t directly\ndepend on any inputs, which can become a source node for downstream\nreactive and render nodes in the reactive\ngraph. Anytime the variable’s value is changed, all downstream nodes\nwill be recomputed. A very common pattern is use an\nobserveEvent to update a reactiveVal every\ntime a graphical query is performed. Any plots that depend on this value\nwill then be updated. For example,\n\n\n\nval <- reactiveVal(initial_val) # initialize the reactive value\n\nobserveEvent(\n  ...some input event...\n  ...do some computation...\n  val(new_value) # update val to new_val\n)\n\n# runs each time the reactiveVal changes\nrenderPlot({\n  val() # get the current value of the reactive value\n})\n\n\n\n\n\nSo, revisiting the dist in the earlier code block, we see\nthat it is initialized as a vector of 1’s whose length is\nequal to the number of rows of x. Everytime the plot is\nclicked, we update the value of dist according to the\nfunction reset_dist. Finally, the changed value of\ndist triggers a rerun of renderPlot. Let’s\nlook at the full application in action. It makes a scatterplot using the\ncars dataset and resizes points every time the plot is clicked.\n\n\n\nlibrary(tidyverse)\n library(shiny)\n \n # wrapper to get the distances from points to clicks\n reset_dist <- function(x, click) {\n   nearPoints(x, click, allRows = TRUE, addDist = TRUE)$dist_\n }\n \n # scatterplot plot with point size dependent on click location\n scatter <- function(x, dists) {\n   x %>%\n     mutate(dist = dists) %>%\n     ggplot() +\n     geom_point(aes(mpg, hp, size = dist)) +\n     scale_size(range = c(6, 1))\n }\n \n ui <- fluidPage(\n   plotOutput(\"plot\", click = \"plot_click\")\n )\n \n server <- function(input, output) {\n   dist <- reactiveVal(rep(1, nrow(mtcars)))\n   observeEvent(\n     input$plot_click,\n     dist(reset_dist(mtcars, input$plot_click))\n   )\n   \n   output$plot <- renderPlot(scatter(mtcars, dist()))\n }\n \n shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\nThe reset_dist function uses nearPoints to\ncompute the distance between each sample and the plot, each time the\nplot is clicked. The associated reactive value dist gets\nchanged, which triggers scatterplot to run, and it is\nencoded using size in the downstream ggplot2 figure.\n\n\n\n\nWe can make the plot more interesting by outputting a table showing the\noriginal dataset. Using the same dist() call, we can sort\nthe table by distance each time the plot is clicked.\n\n\n\nlibrary(tidyverse)\n library(shiny)\n mtcars <- add_rownames(mtcars)\n \n reset_dist <- function(x, click) {\n   nearPoints(x, click, allRows = TRUE, addDist = TRUE)$dist_\n }\n \n scatter <- function(x, dists) {\n   x %>%\n     mutate(dist = dists) %>%\n     ggplot() +\n     geom_point(aes(mpg, hp, size = dist)) +\n     scale_size(range = c(6, 1))\n }\n \n ui <- fluidPage(\n   plotOutput(\"plot\", click = \"plot_click\"),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   dist <- reactiveVal(rep(1, nrow(mtcars)))\n   observeEvent(\n     input$plot_click,\n     dist(reset_dist(mtcars, input$plot_click))\n   )\n   \n   output$plot <- renderPlot(scatter(mtcars, dist()))\n   output$table <- renderDataTable({\n     mtcars %>%\n       mutate(dist = dist()) %>%\n       arrange(dist)\n   })\n }\n \n shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week05-02/",
    "title": "Graphical Queries - Brush Events",
    "description": "An introduction to brush events in Shiny.",
    "author": [],
    "date": "2024-12-17",
    "categories": [],
    "contents": "\n\nCode,\nRecording\n\n\n\n\n\n\nClick events are useful for referring to individual samples. However,\nthey are not ideal for referring to groups of samples. In this case, a\nuseful type of plot input is a brush. This is a selection\nthat can be defined by clicking and dragging over a region.\n\n\n\n\nIn shiny, brush events are treated similarly to click events. For\nexample, to define a new brush input, we can set the brush\nargument to plotOutput.\n\n\n\nui <- fluidPage(\n   plotOutput(\"plot\", brush = \"plot_brush\")\n )\n\n\n\nJust like the click argument, the value\n“plot_brush” is an ID that can be used in the server. Also\nlike in click events, we can setup an observer to change a reactive\nvalue every time a brush is\ndrawn1.\nThe general pattern is similar to what we had before,\n\n\n\nserver <- function(input, output) {\n  selected <- reactiveVal(initial value)\n  observeEvent(\n    input$plot_brush,\n    ... computation using get new_value ...\n    selected(new_value)\n  )\n\n  output$plot <- renderPlot(... use scatter() reactive val...)\n}\n\n\n\n\n\nThe example below is similar to the plot_click example from\nthe previous notes. Instead of sorting points by proximity to the click,\nthough, prints the subset of rows that have been currently brushed.\n\n\n\nlibrary(tidyverse)\n library(shiny)\n mtcars <- add_rownames(mtcars)\n \n reset_selection <- function(x, brush) {\n   brushedPoints(x, brush, allRows = TRUE)$selected_\n }\n \n scatter <- function(x, selected_) {\n   x %>%\n     mutate(selected_ = selected_) %>%\n     ggplot() +\n     geom_point(aes(mpg, hp, alpha = as.numeric(selected_))) +\n     scale_alpha(range = c(0.1, 1))\n }\n \n ui <- fluidPage(\n   plotOutput(\"plot\", brush = \"plot_brush\"),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   selected <- reactiveVal(rep(TRUE, nrow(mtcars)))\n   observeEvent(\n     input$plot_brush,\n     selected(reset_selection(mtcars, input$plot_brush))\n   )\n   \n   output$plot <- renderPlot(scatter(mtcars, selected()))\n   output$table <- renderDataTable(filter(mtcars, selected()))\n }\n \n shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\nIt is often useful to combine multi-view composition (i.e.,\nfaceting or compound figures) with dynamic queries. The basic idea is to\n(a) show different aspects of a dataset using different views, and then\n(b) link the views using dynamic queries. This strategy is sometimes\ncalled dynamic linking.\n\n\n\n\nThe example below implements dynamic linking with the penguins dataset.\nBrushing over either scatterplot highlights the corresponding points in\nthe adjacent plot (it also updates the data table). This is a way of\nunderstanding structure beyond two dimensions. The implementation is\nsimilar to the brushing above, except that the reactive value\nselected() is called in two renderPlot\ncontexts, leading to changes in both plots every time the brush is\nmoved.\n\n\n\nlibrary(tidyverse)\n library(shiny)\n penguins <- read_csv(\"https://uwmadison.box.com/shared/static/ijh7iipc9ect1jf0z8qa2n3j7dgem1gh.csv\")\n \n reset_selection <- function(x, brush) {\n   brushedPoints(x, brush, allRows = TRUE)$selected_\n }\n \n scatter <- function(x, selected_, var1, var2) {\n   x %>%\n     mutate(selected_ = selected_) %>%\n     ggplot(aes_string(var1, var2)) +\n     geom_point(aes(alpha = as.numeric(selected_), col = species)) +\n     scale_alpha(range = c(0.1, 1))\n }\n \n ui <- fluidPage(\n   fluidRow(\n     column(6, plotOutput(\"scatter1\", brush = \"plot_brush\")),\n     column(6, plotOutput(\"scatter2\", brush = \"plot_brush\"))\n   ),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   selected <- reactiveVal(rep(TRUE, nrow(penguins)))\n   observeEvent(\n     input$plot_brush,\n     selected(reset_selection(penguins, input$plot_brush))\n   )\n   \n   output$scatter1 <- renderPlot({\n     scatter(penguins, selected(), \"bill_length_mm\", \"bill_depth_mm\")\n   })\n   output$scatter2 <- renderPlot({\n     scatter(penguins, selected(), \"flipper_length_mm\", \"body_mass_g\")\n   })\n   \n   output$table <- renderDataTable(filter(penguins, selected()))\n }\n \n shinyApp(ui, server)\n\n\n\n\n\n\n\n\n\n\n\n\nTechnically, the code only executes when the mouse lifts off the brush\nselection. Some visualizations will be able to call the updating code\nevery time the mouse is moved with the brush selected. This creates a\nsmoother\nexperience.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week05-03/",
    "title": "Linked Brushing",
    "description": "More examples defining brush queries using Shiny and `ggplot2`.",
    "author": [],
    "date": "2024-12-16",
    "categories": [],
    "contents": "\n\nCode,\nRecording\n\n\n\n\n\n\nThese notes provide more realistic examples of linked brushing. Though\nthe visual design problems they address are more complex, they follow\nthe same recipe described earlier,\n\n\nA reactiveVal is defined to track the currently selected\nsamples.\n\n\nAn observeEvent is used to update the\nreactiveVal every time a plot is brushed.\n\n\nDownstream render contexts update plot and data table\noutputs whenever the reactiveVal is changed.\n\n\n\n\nThe first example implements linked brushing on the movie ratings\ndataset presented earlier. Before we used a slider to select movies\nwithin a user-specified time range. Our graphical alternative is to\nallow selections over a histogram of movie release dates within the\ndataset. Specifically, we will create an interactive version of the\nhistogram below,\n\n\n\nlibrary(tidyverse)\n library(lubridate)\n \n movies <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %>%\n   mutate(\n     date = as_date(Release_Date, format = \"%b %d %Y\"),\n     year = year(date),\n     Major_Genre = fct_explicit_na(Major_Genre)\n   )\n \n movies %>% \n   count(year) %>%\n   ggplot(aes(year, n)) +\n   geom_bar(stat = \"identity\", width = 1) +\n   scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\nand when a subset of years of has been brushed, we will highlight the\ncorresponding movies in the same kind of scatterplot used in the\nearlier, slider-based implementation.\n\n\n\nggplot(movies) +\n   geom_point(aes(Rotten_Tomatoes_Rating, IMDB_Rating))\n\n\n\n\n\n\n\nViewed more abstractly, we are going to use a brush to link the\nhistogram and scatterplot views. We will be able to evaluate the change\nin a visualization (the scatterplot) after “conditioning” on a subset\ndefined by a complementary view (the histogram). This is analogous to\nthe penguins dataset example – only the form of the base plots has\nchanged.\n\n\n\n\nThe main logic needed to link these views is given in the block below.\nThe histogram plotOutput in the UI is given a brush which\nwill be used to select\nyears1.\nWe use the selected reactive value to store a list of\nTRUE/FALSE’s indicating which movie falls into the\ncurrently brushed time range. Each time the brushed range is changed,\nthe output\\(scatterplot<\/code> and\n<code>output\\)table outputs are regenerated,\nhighlighting those movies that appear in the selected()\nlist.\n\n\n\nui <- fluidPage(\n   fluidRow(\n     column(6, plotOutput(\"histogram\", brush = brushOpts(\"plot_brush\", direction = \"x\"))),\n     column(6, plotOutput(\"scatterplot\"))\n   ),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   selected <- reactiveVal(rep(TRUE, nrow(movies)))\n   \n   observeEvent(\n     input$plot_brush,\n     selected(reset_selection(movies, input$plot_brush))\n   )\n   \n   output$histogram <- renderPlot(histogram(movies))\n   output$scatterplot <- renderPlot(scatterplot(movies, selected()))\n   output$table <- renderDataTable(data_table(movies, selected()))\n }\n\n\n\n\n\nWe haven’t included the full code for histogram,\nscatterplot, and data_table, since they in and\nof themselves don’t require any logic for interactivity. You can try out\nthe full code\nhere\nand tinker with the interface below.\n\n\n\n\n\n\n\n\nA natural extension of the previous app is to allow brushing on both the\nhistogram and the scatterplot. Brushing over the scatterplot would show\nthe years during which the selected movies were released – this can be\nused to find out if very poorly or highly rated movies are associated\nwith specific time ranges, for example.\n\n\n\n\nThe updated application is below. The main differences are that,\n\n\nThe scatterplot plotOutput now includes a brush.\n\n\nWe are passing in the reactive value of the selected()\nmovies into the histogram as well.\n\n\n\nui <- fluidPage(\n   fluidRow(\n     column(6, plotOutput(\"histogram\", brush = brushOpts(\"plot_brush\", direction = \"x\"))),\n     column(6, plotOutput(\"scatterplot\", brush = \"plot_brush\"))\n   ),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   selected <- reactiveVal(rep(TRUE, nrow(movies)))\n   \n   observeEvent(\n     input$plot_brush,\n     selected(reset_selection(movies, input$plot_brush))\n   )\n   \n   output$histogram <- renderPlot(histogram(movies, selected()))\n   output$scatterplot <- renderPlot(scatterplot(movies, selected()))\n   output$table <- renderDataTable(data_table(movies, selected()))\n }\n \n shinyApp(ui, server)\n\n\n\n\n\nFor the scatterplot, we simply reduced the transparency for the movies\nthat weren’t selected. We cannot do this for the histogram, though,\nbecause the movies are not directly represented in this plot, only their\ncounts over time. Instead, our idea will be to draw two overlapping\nhistograms. A static one in the background will represent the year\ndistribution before any selection. A changing one in the foreground will\nbe redrawn whenever the selected movies are changed. For example, the\ncode below overlays two geom_bar layers, with one\ncorresponding only to the first 500 movies in the dataset.\n\n\n\n  sub_counts <- movies[1:500, ] %>%\n     count(year)\n   \n   movies %>%\n     count(year) %>%\n     ggplot(aes(year, n)) +\n     geom_bar(stat = \"identity\", fill = \"#d3d3d3\", width = 1) +\n     geom_bar(data = sub_counts, stat = \"identity\", width = 1) +\n     scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\nCombining these ideas leads to the app\nhere\nand included below. Try brushing on both the scatterplot and the\nhistogram. The especially interesting thing about this approach is that,\nwithout introducing any new screen elements, we’ve widened the class of\nquestions of that can be answered. In a sense, we’ve increased the\ninformation density of the display – we can present more information\nwithout having to introduce any peripheral UI components or graphical\nmarks.\n\n\n\n\n\n\nIn our last problem, we would like to use a dataset of flight delays to\nunderstand what characteristics of the flights make some more / less\nlikely to be delayed. The basic difficulty is that there are many\npotentially relevant variables, and they might interact in ways that are\nnot obvious in advance.\n\n\n\nlibrary(nycflights13)\nhead(flights)\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# ℹ 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\n\n\n\nOur solution strategy will be to dynamically link complementary\nhistograms. By brushing the histogram of delays time, we’ll be able to\nsee the conditional distributions for other variables of interest. In\nprinciple, we could do this for every variable in the dataset, but for\nthe example, we’ll focus on just the scheduled departure time and flight\ndistance.\n\n\n\n\nThe UI in this case creates three separate histograms, each of which\nintroduces a brush. We will plan on brushing one histogram at a time,\nwhich is then used to update overlays on each.\n\n\n\n  ui <- fluidPage(\n  fluidRow(\n    column(\n      6, \n      plotOutput(\"h1\", brush = brushOpts(\"plot_brush\", direction = \"x\"), height = 200),\n      plotOutput(\"h2\", brush = brushOpts(\"plot_brush\", direction = \"x\"), height = 200),\n      plotOutput(\"h3\", brush = brushOpts(\"plot_brush\", direction = \"x\"), height = 200)\n    ),\n    column(6, dataTableOutput(\"table\"))\n  ),\n)\n\n\n\n\n\nThe logic for drawing the overlays is encapsulated by the functions\nbelow. The bar_plot function draws two bar plots over one\nanother, one referring to a global counts object of\nunchanging histogram bar heights. The second refers to the bar heights\nfor the continually updated overlays. Notice that we use\n.data[[v]] to use variable names encoded in strings. The\nplot_overlay function provides the histogram bar heights\nfor variable v after brushing over the flights in\nselected_.\n\n\n\nbar_plot <- function(sub_flights, v) {\n   ggplot(counts[[v]], aes(.data[[v]], n)) +\n     geom_bar(fill = \"#d3d3d3\", stat = \"identity\") +\n     geom_bar(data = sub_flights, stat = \"identity\")\n }\n \n plot_overlay <- function(selected_, v) {\n   flights %>%\n     filter(selected_) %>%\n     count(.data[[v]]) %>%\n     bar_plot(v)\n }\n\n\n\n\n\nCode for the full application is linked\nhere.\nThanks to shiny’s reactiveVal and\nbrushedPoints definitions, implementing interactivity only\nrequires about 20 lines (starting from ui <- … to the\nend). The rest of the code is used to draw new static plots depending on\nthe current selection.\n\n\n\n\n\n\n\n\n\n\n\nNote that we restrict brush motion to the \\(x\\)-direction. This is because the\n\\(x\\)\ndirection alone encodes year information, which we want to\nselect.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week05-04/",
    "title": "Linking using Crosstalk",
    "description": "Linking in web-based visualizations.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-15",
    "categories": [],
    "contents": "\n\n\n\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(leaflet)\nlibrary(DT)\nlibrary(crosstalk)\n\n\n\n\nFor most graphical queries, the click and brush inputs implemented in\nShiny will be sufficient. However, a basic limitation of shiny’s\nplotOutput is that it has to create views by generating\nstatic image files – it only creates the illusion of interactivity by\nrapidly changing the underlying files. In some cases, there will be so\nmany points on the display each update will be slow and the fluidity of\ninteraction will suffer.\n\n\n\n\nOne approach around this problem is to use a library that directly\nsupports web-based plots. These plots can modify elements in place,\nwithout having to redraw and save the entire figure on each interaction.\nThe crosstalk package gives one approach to linked views in\nthis setting. We’ll only give an overview of it here, but the purpose of\nsharing it is so we have at least one example that we can refer to in\ncase the Shiny approach becomes untenable.\n\n\n\n\nWe’ll study a problem about the dropoff in Chicago subway ridership\nafter the start of the COVID-19 lockdowns. We have data on the weekday\nand weekend transit ridership at each subway station, along with the\nlocations of the stations. We are curious about the extent of the change\nin ridership, along with features that might be responsible for some\nstations being differentially affected. The block below reads in this\nraw data,\n\n\n\ndownload.file(\"https://github.com/emilyriederer/demo-crosstalk/blob/master/data/stations.rds?raw=true\", \"stations.rds\")\n download.file(\"https://github.com/emilyriederer/demo-crosstalk/blob/master/data/trips_apr.rds?raw=true\", \"trips_apr.rds\")\n stations <- readRDS(\"stations.rds\")\n trips <- readRDS(\"trips_apr.rds\")\n\n\n\n\n\nThe crosstalk package implements a SharedData object. This\nis used to track selections across all the plots that refer to it. We\ncan think of it as crosstalk’s analog of our earlier\nbrushedPoints function. These objects are defined by\ncalling SharedData\\(new()<\/code>\non the data.frame which will be used\nacross views. The <code>key<\/code> argument provides a\nunique identifier that is used to\nmatch corresponding samples across all displays (notice that it uses\n<code>~<\/code> formula\nnotation).<\/p>\n<div class=\"layout-chunk\"\ndata-layout=\"l-body\">\n<div class=\"sourceCode\">\n<pre class=\"sourceCode r\"><code\nclass=\"sourceCode r\"><span><span\nclass='va'>trips_ct<\/span> <span\nclass='op'>&lt;-<\/span> <span\nclass='va'><a\nhref='https://rdrr.io/pkg/crosstalk/man/SharedData.html'>SharedData<\/a><\/span><span\nclass='op'>\\)new(trips, key = ~station_id, group = “stations”)\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week06-01/",
    "title": "tsibble Objects",
    "description": "A data structure for managing time series data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-14",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(tsibbledata)\n\n\n\n\nTsibbles are data structures that are designed specifically for storing\ntime series data. They are useful because they create a unified\ninterface to various time series visualization and modeling tasks. This\nremoves the friction of having to transform back and forth between\ndata.frames, lists, and matrices, depending on the particular task of\ninterest.\n\n\n\n\nThe key difference between a tsibble and an ordinary data.frame is that\nit requires a temporal key variable, specifying the frequency with which\nobservations are collected. For example, the code below generates a\ntsibble with yearly observations.\n\n\n\n\ntsibble(\n  Year = 2015:2019,\n  Observation = c(123, 39, 78, 52, 110),\n  index = Year\n)\n\n# A tsibble: 5 x 2 [1Y]\n   Year Observation\n  <int>       <dbl>\n1  2015         123\n2  2016          39\n3  2017          78\n4  2018          52\n5  2019         110\n\n\nWe can also create a tsibble from an ordinary data.frame by calling the\nas_tsibble function. The only subtlety is that we have to\nspecify an index.\n\n\n\nx <- data.frame(\n  Year = 2015:2019,\n  Observation = c(123, 39, 78, 52, 110)\n)\n\nas_tsibble(x, index = Year)\n\n# A tsibble: 5 x 2 [1Y]\n   Year Observation\n  <int>       <dbl>\n1  2015         123\n2  2016          39\n3  2017          78\n4  2018          52\n5  2019         110\n\n\nThe index is useful because it creates a data consistency check. If a\nfew days are missing from a daily dataset, the index makes it easy to\ndetect and fill in these gaps. Notice that when we print a tsibble\nobject, it prints the index and guessed sampling frequency on the top\nright corner.\n\n\n\ndays <- seq(as_date(\"2021-01-01\"), as_date(\"2021-01-31\"), by = \"day\")\ndays <- days[-5] # Skip January 5\n\nx <- tsibble(day = days, value = rnorm(30), index = day)\nfill_gaps(x)\n\n# A tsibble: 31 x 2 [1D]\n   day          value\n   <date>       <dbl>\n 1 2021-01-01 -1.57  \n 2 2021-01-02 -0.419 \n 3 2021-01-03  0.0740\n 4 2021-01-04 -1.94  \n 5 2021-01-05 NA     \n 6 2021-01-06  0.188 \n 7 2021-01-07 -1.14  \n 8 2021-01-08 -0.0866\n 9 2021-01-09  3.05  \n10 2021-01-10 -0.927 \n# ℹ 21 more rows\n\n\nTsibbles can store more than one time series at a time. In this case, we\nhave to specify key columns that distinguish between the separate time\nseries. For example, in the olympics running times dataset,\n\n\n\nolympic_running\n\n# A tsibble: 312 x 4 [4Y]\n# Key:       Length, Sex [14]\n    Year Length Sex    Time\n   <int>  <int> <chr> <dbl>\n 1  1896    100 men    12  \n 2  1900    100 men    11  \n 3  1904    100 men    11  \n 4  1908    100 men    10.8\n 5  1912    100 men    10.8\n 6  1916    100 men    NA  \n 7  1920    100 men    10.8\n 8  1924    100 men    10.6\n 9  1928    100 men    10.8\n10  1932    100 men    10.3\n# ℹ 302 more rows\n\n\nthe keys are running distance and sex. If we were creating a tsibble\nfrom a data.frame containing these multiple time series, we would need\nto specify the keys. This protects against accidentally having duplicate\nobservations at given times.\n\n\n\nolympic_df <- as.data.frame(olympic_running)\nas_tsibble(olympic_df, index = Year, key = c(\"Sex\", \"Length\")) # what happens if we remove key?\n\n# A tsibble: 312 x 4 [4Y]\n# Key:       Sex, Length [14]\n    Year Length Sex    Time\n   <int>  <int> <chr> <dbl>\n 1  1896    100 men    12  \n 2  1900    100 men    11  \n 3  1904    100 men    11  \n 4  1908    100 men    10.8\n 5  1912    100 men    10.8\n 6  1916    100 men    NA  \n 7  1920    100 men    10.8\n 8  1924    100 men    10.6\n 9  1928    100 men    10.8\n10  1932    100 men    10.3\n# ℹ 302 more rows\n\n\n\nThe usual data tidying functions from dplyr are implemented\nfor tsibbles. Filtering rows, selecting columns, deriving variables\nusing mutate, and summarizing groups using\ngroup_by and summarise all work as expected.\nOne distinction to be careful about is that the results will be grouped\nby their index.\n\n\n\n\nFor example, this computes the total cost of Australian pharmaceuticals\nper month for a particular type of script. We simply filter to the\nscript type and take the sum of costs.\n\n\n\n\nPBS %>%\n  filter(ATC2 == \"A10\") %>%\n  summarise(TotalC = sum(Cost))\n\n# A tsibble: 204 x 2 [1M]\n      Month  TotalC\n      <mth>   <dbl>\n 1 1991 Jul 3526591\n 2 1991 Aug 3180891\n 3 1991 Sep 3252221\n 4 1991 Oct 3611003\n 5 1991 Nov 3565869\n 6 1991 Dec 4306371\n 7 1992 Jan 5088335\n 8 1992 Feb 2814520\n 9 1992 Mar 2985811\n10 1992 Apr 3204780\n# ℹ 194 more rows\n\n\nIf we had wanted the total cost by year, we would have to convert to an\nordinary data.frame with a year variable. We cannot use a tsibble here\nbecause we would have multiple measurements per year, and this would\nviolate tsibble’s policy of having no duplicates.\n\n\n\nPBS %>%\n  filter(ATC2 == \"A10\") %>%\n  mutate(Year = year(Month)) %>%\n  as_tibble() %>%\n  group_by(Year) %>%\n  summarise(TotalC = sum(Cost))\n\n# A tibble: 18 × 2\n    Year     TotalC\n   <dbl>      <dbl>\n 1  1991  21442946 \n 2  1992  45686946.\n 3  1993  55532688.\n 4  1994  60816080.\n 5  1995  67326599.\n 6  1996  77397927.\n 7  1997  85131672.\n 8  1998  93310626.\n 9  1999 105959043.\n10  2000 122496586.\n11  2001 136467442.\n12  2002 149066136.\n13  2003 156464261.\n14  2004 183798935.\n15  2005 199655595 \n16  2006 220354676 \n17  2007 265718966.\n18  2008 135036513 \n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week06-02/",
    "title": "Time Series Patterns",
    "description": "Vocabulary for describing visual structure in time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-13",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(tsibbledata)\nlibrary(fpp2)\ntheme_set(theme_minimal())\n\n\n\n\nThere are a few structures that are worth keeping an eye out for\nwhenever you plot a single time series. We’ll review the main vocabulary\nin these notes.\n\n\n\n\nVocabulary\n\n\nTrend: A long-run increase or decrease in a time series.\n\n\nSeasonal: A pattern that recurs over a fixed and known period.\n\n\nCyclic: A rising and falling pattern that does not occur over a fixed or\nknown period.\n\n\n\nA few series with different combinations of these patterns are shown\nbelow.\n\n\n\nggplot(as_tsibble(qauselec)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Australian Quarterly Electricity Production\")\n\n\nggplot(as_tsibble(hsales)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Housing Sales\")\n\n\nggplot(as_tsibble(ustreas)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"US Treasury Bill Contracts\")\n\n\nggplot(as_tsibble(diff(goog))) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Google Stock Prices\")\n\n\n\n\n\n\nA series can display combinations of these patterns at once. Further,\nthe same data can exhibit different patterns depending on the scale at\nwhich it is viewed. For example, though a dataset might seem seasonal at\nshort time scales, a long-term trend might appear after zooming out.\nThis is visible in the electricity production plot above.\n\n\n\n\nFinally, it’s worth keeping in mind that real-world structure can be\nmuch more complicated than any of these patterns. For example, the plot\nbelow shows the number of passengers on flights from Melbourne to Sydney\nbetween 1987 and 1992. You can see a period when no flights were made\nand a trial in 1992 where economy seats were switched to business seats.\n\n\n\n\nmelbourne_sydney <- ansett %>%\n  filter(Airports == \"MEL-SYD\") # challenge: facet by Airports, instead of filtering\n\nggplot(melbourne_sydney) +\n  geom_line(aes(x = Week, y = Passengers, col = Class))\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week06-03/",
    "title": "Seasonal Plots",
    "description": "Approaches for visualizing seasonality.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-12",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(feasts)\nlibrary(fpp2)\nlibrary(tsibbledata)\ntheme_set(theme_minimal())\n\n\n\n\nIf our data have seasonal structure, it’s natural to compare individual\nperiods against one another. In contrast to plotting the data in one\nlong time series, seasonal plots reduce the amount of distance our eyes\nhave to travel in order to compare two periods in a seasonal pattern.\nThis also reduces the burden on our memory.\n\n\n\n\nIn R, we can use the gg_season function to overlay all the\nseasons onto one another. The plot below shows antidiabetes drug sales\nover time. This view makes it clear that there is a spike in sales every\nJanuary.\n\n\n\n\ncols <- scales::viridis_pal()(10)\ngg_season(as_tsibble(a10), pal = cols)\n\n\n\n\n\nIf the time series exhibit seasonal structure at multiple scales, then\nwe can view them all using the period argument.\n\n\n\ngg_season(vic_elec, Demand, period = \"day\", pal = cols)\n\n\ngg_season(vic_elec, Demand, period = \"week\", pal = cols)\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week06-04/",
    "title": "Cross and Auto-Correlation",
    "description": "Summaries of relationships between and within time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-11",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(tsibbledata)\nlibrary(feasts)\ntheme_set(theme_minimal())\n\n\n\nThere is often interest in seeing how two time series relate to one\nanother. A scatterplot can be useful in gauging this relationship. For\nexample, the plot below suggests that there is a relationship between\nelectricity demand and temperature, but it’s hard to make out exactly\nthe nature of the relationship.\n\n\n\nvic_2014 <- vic_elec %>%\n  filter(year(Time) == 2014)\n\nvic_2014 %>%\n  pivot_longer(Demand:Temperature) %>%\n  ggplot(aes(x = Time, y = value)) +\n  geom_line() +\n  facet_wrap(~ name, scales = \"free_y\")\n\n\n\n\n\nA scatterplot clarifies that, while electricity demand generally goes up\nin the cooler months, the very highest demand happens during high heat\ndays.\n\n\n\nggplot(vic_2014, aes(x = Temperature, y = Demand)) +\n  geom_point(alpha = 0.6, size = 0.7)\n\n\n\n\n\nNote that the timepoints are far from independent. Points tend to drift\ngradually across the scatterplot, rather than jumping to completely\ndifferent regions in short time intervals. This is just the 2D\nconsequence of the time series varying smoothly.\n\n\n\nlagged <- vic_2014[c(2:nrow(vic_2014), 2), ] %>%\n  setNames(str_c(\"lagged_\", colnames(vic_2014)))\n\nggplot(bind_cols(vic_2014, lagged), aes(x = Temperature, y = Demand)) +\n  geom_point(alpha = 0.6, size = 0.7) +\n  geom_segment(\n    aes(xend = lagged_Temperature, yend = lagged_Demand),\n    size = .4, alpha = 0.5\n  )\n\n\n\n\n\nTo formally measure the linear relationship between two time series, we\ncan use the cross-correlation, \\[\\begin{align}\n\\frac{\\sum_{t}\\left(x_{t} - \\hat{\\mu}_{X}\\right)\\left(y_{t} -\n\\hat{\\mu}_{Y}\\right)}{\\hat{\\sigma}_{X}\\hat{\\sigma}_{Y}}\n\\end{align}\\] which for the data above is,\n\n\n\ncor(vic_2014$Temperature, vic_2014$Demand)\n\n[1] 0.2797854\n\n\nCross-correlation can be extended to autocorrelation — the correlation\nbetween a time series and a lagged version of itself. This measure is\nuseful for quantifying the strength of seasonality within a time series.\nA daily time series with strong weekly seasonality will have high\nautocorrelation at lag 7, for example. The example below shows the\nlag-plots for Australian beer production after 2000. The plot makes\nclear that there is high autocorrelation at lags 4 and 8, suggesting\nhigh quarterly seasonality.\n\n\n\nrecent_production <- aus_production %>%\n  filter(year(Quarter) > 2000)\n\ngg_lag(recent_production, Beer, geom = \"point\")\n\n\n\n\n\nIndeed, we can confirm this by looking at the original data.\n\n\n\nggplot(recent_production) +\n  geom_line(aes(x = time(Quarter), y = Beer))\n\n\n\n\n\nThese lag plots take up a bit of space. A more compact summary is to\ncompute the autocorrelation function (ACF). Peaks and valleys in an ACF\nsuggest seasonality at the frequency indicated by the lag value.\n\n\n\nacf_data <- ACF(recent_production, Beer)\nautoplot(acf_data)\n\n\n\n\n\nGradually decreasing slopes in the ACF suggest trends. This is because\nif there is a trend, the current value tends to be very correlated with\nthe recent past. It’s possible to have both seasonality within a trend,\nin which case the ACF function has bumps where the seasonal peaks align.\n\n\n\nggplot(as_tsibble(a10), aes(x = time(index), y = value)) +\n  geom_line()\n\n\nacf_data <- ACF(as_tsibble(a10), lag_max = 100)\nautoplot(acf_data)\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week06-05/",
    "title": "Collections of Time Series",
    "description": "Navigating across related time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-10",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(feasts)\nlibrary(fpp2)\nlibrary(ggrepel)\nlibrary(tsibble)\nlibrary(tsibbledata)\ntheme_set(theme_minimal())\n\n\n\n\nWe have seen ways of visualizing a single time series (seasonal plots,\nACF) and small numbers of time series (Cross Correlation). In practice,\nit’s also common to encounter large collections of time series. These\ndatasets tend to require more sophisticated analysis techniques, but we\nwill review one useful approach, based on extracted features.\n\n\n\n\nThe high-level idea is to represent each time series by a vector of\nsummary statistics, like the maximum value, the slope, and so on. These\nvector summaries can then be used to create an overview of variation\nseen across all time series. For example, just looking at the first few\nregions in the Australian tourism dataset, we can see that there might\nbe useful features related to the overall level (Coral Coast is larger\nthan Barkly), recent trends (increased business in North West), and\nseasonality (South West is especially seasonal).\n\n\n\n\ntourism <- as_tsibble(tourism, index = Quarter) %>%\n  mutate(key = str_c(Region, Purpose, sep=\"-\")) %>%\n  update_tsibble(key = c(\"Region\", \"State\", \"Purpose\", \"key\"))\n\nregions <- tourism %>%\n  distinct(Region) %>%\n  pull(Region)\n\nggplot(tourism %>% filter(Region %in% regions[1:9])) +\n  geom_line(aes(x = date(Quarter), y = Trips, col = Purpose)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~Region, scale = \"free\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nComputing these kinds of summary statistics by hand would be tedious.\nFortunately, the feasts package makes it easy to extract a variety of\nstatistics for tsibble objects.\n\n\n\ntourism_features <- tourism %>%\n  features(Trips, feature_set(pkgs = \"feasts\"))\n\ntourism_features\n\n# A tibble: 304 × 52\n   Region    State Purpose key   trend_strength seasonal_strength_year\n   <chr>     <chr> <chr>   <chr>          <dbl>                  <dbl>\n 1 Adelaide  Sout… Busine… Adel…          0.464                  0.407\n 2 Adelaide  Sout… Holiday Adel…          0.554                  0.619\n 3 Adelaide  Sout… Other   Adel…          0.746                  0.202\n 4 Adelaide  Sout… Visiti… Adel…          0.435                  0.452\n 5 Adelaide… Sout… Busine… Adel…          0.464                  0.179\n 6 Adelaide… Sout… Holiday Adel…          0.528                  0.296\n 7 Adelaide… Sout… Other   Adel…          0.593                  0.404\n 8 Adelaide… Sout… Visiti… Adel…          0.488                  0.254\n 9 Alice Sp… Nort… Busine… Alic…          0.534                  0.251\n10 Alice Sp… Nort… Holiday Alic…          0.381                  0.832\n# ℹ 294 more rows\n# ℹ 46 more variables: seasonal_peak_year <dbl>,\n#   seasonal_trough_year <dbl>, spikiness <dbl>, linearity <dbl>,\n#   curvature <dbl>, stl_e_acf1 <dbl>, stl_e_acf10 <dbl>, acf1 <dbl>,\n#   acf10 <dbl>, diff1_acf1 <dbl>, diff1_acf10 <dbl>,\n#   diff2_acf1 <dbl>, diff2_acf10 <dbl>, season_acf1 <dbl>,\n#   pacf5 <dbl>, diff1_pacf5 <dbl>, diff2_pacf5 <dbl>, …\n\n\nOnce you have a data.frame summarizing these time series, you can run\nany clustering or dimensionality reduction procedure on the summary. For\nexample, this is 2D representation from PCA. We will get into much more\ndepth about dimensionality reduction later in this course — for now,\njust think of this as an abstract map relating all the time series.\n\n\n\npcs <- tourism_features %>%\n  select(-State, -Region, -Purpose, -key) %>%\n  prcomp(scale = TRUE) %>%\n  augment(tourism_features)\n\noutliers <- pcs %>% \n  filter(.fittedPC1 ^ 2 + .fittedPC2 ^ 2 > 120)\n\n\n\nThis PCA makes it very clear that the different travel purposes have\ndifferent time series, likely due to the heavy seasonality of holiday\ntravel (Melbourne seems to be an interesting exception).\n\n\n\nggplot(pcs, aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point(aes(col = Purpose)) +\n  geom_text_repel(\n    data = outliers,\n    aes(label = Region),\n    size = 2.5 \n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"PC1\", y = \"PC2\") +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nWe can look at the series that are outlying in the PCA. The reading has\nsome stories for why these should be considered outliers. They seem to\nbe series with substantial increasing trends or which have exceptionally\nhigh overall counts.\n\n\n\noutlier_series <- tourism %>%\n  filter(key %in% outliers$key)\n\nggplot(outlier_series) +\n  geom_line(aes(x = date(Quarter), y = Trips, col = Purpose)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~Region, scale = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nThis featurization approach is especially powerful when combined with\ncoordinated views. It is possible to link the points in the PCA plot\nwith the time series display, so that selecting points in the PCA shows\nthe corresponding time series.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:02:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week07-01/",
    "title": "Spatial Data Formats",
    "description": "An overview of common formats, with illustrative examples.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-09",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nknitr::opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE, eval = TRUE)\n\n\n\n\nlibrary(ceramic)\nlibrary(raster)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\ntheme_set(theme_minimal())\n\n\n\nSpatial data come in two main formats: vector and raster. We’ll examine\nthem in detail in the next few lectures, but this lecture motivates the\nhigh-level distinction and gives a few examples. It also shows how to\nread and write data to and from these formats.\n\n\nVector Data\n\n\n\nVector data formats are used to store geometric information, like the\nlocations of hospitals (points), trajectories of bus routes (lines), or\nboundaries of counties (polygons). It’s useful to think of the\nassociated data as being spatially enriched data frames, with each row\ncorresponding to one of these geometric features.\n\n\n\n\nVector data are usually stored in .geojson,\n.wkt, .shp, or .topojson formats.\nStandard data.frames cannot be used because then important spatial\nmetadata would be lost, like the Coordinate Reference System (to be\nexplained in the fourth lecture this week).\n\n\n\n\nIn R, these formats can be read using read_sf in the\nsf package. They can be written using the\nwrite_sf function. Here, we’ll read in a vector dataset\ncontaining the boundaries of lakes in Madison.\n\n\n\nlakes <- read_sf(\"https://uwmadison.box.com/shared/static/duqpj0dl3miltku1676es64d5zmygy92.geojson\")\n \n lakes %>%\n   dplyr::select(id, name, geometry)\n\nSimple feature collection with 10 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -89.54084 ymin: 42.94762 xmax: -89.17699 ymax: 43.2051\nGeodetic CRS:  WGS 84\n# A tibble: 10 × 3\n   id               name                                      geometry\n   <chr>            <chr>                                <POLYGON [°]>\n 1 relation/1997948 Lake Monona    ((-89.37974 43.0714, -89.37984 43.…\n 2 relation/3361036 Lake Mendota   ((-89.46885 43.08266, -89.46864 43…\n 3 relation/3447583 Upper Mud Lake ((-89.31364 43.04483, -89.31361 43…\n 4 relation/4090134 Hook Lake      ((-89.33198 42.94909, -89.33161 42…\n 5 relation/4231143 Lake Wingra    ((-89.4265 43.05514, -89.4266 43.0…\n 6 relation/6183370 Lake Waubesa   ((-89.32949 42.99166, -89.32908 42…\n 7 relation/7083727 Lake Kegonsa   ((-89.2648 42.9818, -89.26399 42.9…\n 8 relation/9668475 Lower Mud Lake ((-89.28011 42.98486, -89.2794 42.…\n 9 way/21415784     Goose Lake     ((-89.53957 42.97967, -89.53947 42…\n10 way/28721778     Brazee Lake    ((-89.18562 43.19529, -89.18533 43…\n\n#write_sf(lakes, \"output.geojson\", driver = \"GeoJSON\")\n\n\n\nWe’ll discuss plotting in the next lecture, but for a preview, this is\nhow you can visualize the lakes using ggplot2.\n\n\n\nlakes <- lakes %>%\n   group_by(id) %>%\n   mutate(\n     longitude = st_coordinates(geometry)[1, 1],\n     latitude = st_coordinates(geometry)[1, 2]\n   )\n \n tm_shape(lakes) +\n   tm_polygons(col = \"#00ced1\")\n\n\n\n\n\nWith a little extra effort, we can overlay the features onto public map\nbackgrounds (these are often called “basemaps”).\n\n\n\n# you can get your own at https://account.mapbox.com/access-tokens/\n Sys.setenv(MAPBOX_API_KEY=\"pk.eyJ1Ijoia3Jpc3JzMTEyOCIsImEiOiJjbDYzdjJzczQya3JzM2Jtb2E0NWU1a3B3In0.Mk4-pmKi_klg3EKfTw-JbQ\")\n basemap <- cc_location(loc= c(-89.401230, 43.073051), buffer = 15e3)\n \n tm_shape(basemap) +\n   tm_rgb() +\n   tm_shape(lakes) +\n   tm_polygons(col = \"#00ced1\")\n\n\n\n\n\n\n\nThere is a surprising amount of public vector data available online.\nUsing this\nquery1,\nI’ve downloaded locations of all hospital clinics in Madison.\n\n\n\nclinics <- read_sf(\"https://uwmadison.box.com/shared/static/896jdml9mfnmza3vf8bh221h9hlvh70v.geojson\")\n \n # how would you overlay the names of the clinics, using geom_text?\n tm_shape(basemap) +\n   tm_rgb() +\n   tm_shape(clinics) +\n   tm_dots(col = \"red\", size = 1)\n\n\n\n\n\nUsing this query, I’ve\ndownloaded all the bus routes.\n\n\n\nbus <- read_sf(\"https://uwmadison.box.com/shared/static/5neu1mpuh8esmb1q3j9celu73jy1rj2i.geojson\")\n \n tm_shape(basemap) +\n   tm_rgb() +\n   tm_shape(bus) +\n   tm_lines(col = \"#bc7ab3\", size = 1)\n\n\n\n\n\nFor the boundaries of the lakes above, I used this\nquery.\n\n\nMany organizations prepare geojson data themselves and make it publicly\navailable; e.g., the boundaries of\nrivers\nor\nglaciers.\nDon’t worry about how to visualize these data at this point — I just\nwant to give some motivating examples.\n\n\n\nRaster Data\n\n\n\nRaster data give a measurement along a spatial grid. You can think of\nthem as spatially enriched matrices, where the metadata says where on\nthe earth each entry of the matrix is associated with.\n\n\n\n\nRaster data are often stored in tiff format. They can be\nread in using the rast function in the terra\nlibrary, and can be written using writeRaster.\n\n\n\nshanghai <- rast(\"https://uwmadison.box.com/shared/static/u4na56w3r4eqg232k2ma3eqbvehfiaoq.tif\")\n #writeRaster(shanghai, \"output.tiff\", driver = \"GeoTIFF\")\n\n\n\n\n\nSome of the most common types of public raster data are satellite images\nor derived measurements, like elevation maps. For example, the code\nbelow shows an image of a neighborhood outside Shanghai.\n\n\n\n\ntm_shape(shanghai / 1636 * 255) + # rescale the max value to 255\n  tm_rgb()\n\n|---------|---------|---------|---------|=========================================                                          |---------|---------|---------|---------|=========================================                                          |---------|---------|---------|---------|=========================================                                          |---------|---------|---------|---------|=========================================                                          \n\n\n\n\nThere’s actually quite a bit of information in this image. We can zoom\nin…\n\n\n\nbbox <- ext(121.66, 121.665, 30.963, 30.968)\nshanghai_ <- crop(shanghai, bbox)\n\ntm_shape(shanghai_ / 1636 * 255) +\n  tm_rgb()\n\n\n\n\n\nHere are is data on elevation in Zion national park.\n\n\n\nf <- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nzion <- rast(f)\ntm_shape(zion) +\n  tm_raster(palette = \"PuBuGn\") +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n\n\n\n\n\nInstallation\n\n\nA note about R packages: for historical reasons, spatial data libraries\nin R reference a few command line programs, like gdal and\nproj. Since these command line programs are not themselves\na part of R, they need to be installed before the corresponding R\npackages. The process will differ from operating system to operating\nsystem, and the experience can be frustrating, especially when the R\npackages don’t recognize the underlying system installation. I recommend\nfollowing the instructions on\nthis page and reaching out\nearly if you have any issues.\n\n\n\n\n\n\nIt can be constructed easily using the\nwizard↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:09:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week07-02/",
    "title": "Vector Data",
    "description": "Manipulating and visualizing spatial vector data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-08",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(ceramic)\nlibrary(knitr)\nlibrary(sf)\nlibrary(spData)\nlibrary(tidyverse)\nlibrary(tmap)\ntheme_set(theme_minimal())\n\n\nAs mentioned previously, vector data are used to store geometric spatial\ndata. Specifically, there are 7 types of geometric information that are commonly\nused, as given in the figure below.\n\n\ninclude_graphics(\"https://krisrs1128.github.io/stat479/posts/2021-03-02-week7-2/sf-classes.png\")\n\n\n\nWe can construct these geometric objects from scratch. For example, starting\nfrom the defining coordinates, we can use st_point to create a point object,\n\n\n# make a point\np <- st_point(c(5, 2))\nplot(p)\n\n\n\nst_linestring to create a linestring,\n\n\n# make a line\nlinestring_matrix <- rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))\np <- st_linestring(linestring_matrix)\nplot(p)\n\n\n\nand st_polygon to create a polygon.\n\n\n# make a polygon\npolygon_list <- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\np <- st_polygon(polygon_list)\nplot(p)\n\n\n\nDifferent geometries can be combined into a geometry collection, using sfc.\n\n\npoint1 <- st_point(c(5, 2))\npoint2 <- st_point(c(1, 3))\npoints_sfc <- st_sfc(point1, point2)\nplot(points_sfc)\n\n\n\nReal-world vector datasets are more than just these geometries — they also\nassociate each geometry with some additional information about each feature. We\ncan add this information to the geometries above by associating each element\nwith a row of a data.frame. This merging is accomplished by st_sf, using\ngeometry to associate a raw st_geom each row of a data.frame.\n\n\nlnd_point <- st_point(c(0.1, 51.5))                \nlnd_geom <- st_sfc(lnd_point, crs = 4326)         \nlnd_attrib = data.frame(                        \n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n  )\n\nlnd_sf = st_sf(lnd_attrib, geometry = lnd_geom)\n\n\nVisualization\nVector data can be directly plotted using base R. For example, suppose we\nwant to plot the boundaries of India, within it’s local context. We can use the\nworld dataset, provided by the spData package. Each row of the world\nobject contains both the boundary of a country (in the geom column) and\ninformation about its location and population characteristics.\n\n\ndata(world)\nhead(world)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 11\n  iso_a2 name_long      continent   region_un subregion type  area_km2\n  <chr>  <chr>          <chr>       <chr>     <chr>     <chr>    <dbl>\n1 FJ     Fiji           Oceania     Oceania   Melanesia Sove…   1.93e4\n2 TZ     Tanzania       Africa      Africa    Eastern … Sove…   9.33e5\n3 EH     Western Sahara Africa      Africa    Northern… Inde…   9.63e4\n4 CA     Canada         North Amer… Americas  Northern… Sove…   1.00e7\n5 US     United States  North Amer… Americas  Northern… Coun…   9.51e6\n6 KZ     Kazakhstan     Asia        Asia      Central … Sove…   2.73e6\n# ℹ 4 more variables: pop <dbl>, lifeExp <dbl>, gdpPercap <dbl>,\n#   geom <MULTIPOLYGON [°]>\n\nThis makes the plot, using dplyr to filter down to just the row containing the\nIndia geometry.\n\n\nindia_geom <- world |>\n  filter(name_long == \"India\") |>\n  st_geometry()\n\nplot(india_geom)\n\n\n\nUsing base R, we can also layer on several vector objects, using add = TRUE.\n\n\nworld_asia <- world |>\n  filter(continent == \"Asia\")\n\nplot(india_geom, expandBB = c(0, 0.2, 0.1, 1), col = \"gray\", lwd = 3)\nplot(st_union(world_asia), add = TRUE)\n\n\n\nWe can also use tm_polygons in tmap. To change the coordinates of the\nviewing box, we can set the bbox (bounding box) argument.\n\n\nbbox <- c(60, 5, 110, 40)\ntm_shape(world_asia, bbox = bbox) +\n  tm_polygons(col = \"white\") +\n  tm_shape(india_geom) +\n  tm_polygons()\n\n\n\nWe can also encode data that’s contained in the vector dataset.\n\n\ntm_shape(world_asia, bbox = bbox) +\n  tm_polygons(col = \"lifeExp\") +\n  tm_polygons()\n\n\n\nEven in this more complex setup, where we work with background images and\nvector data rather than standard data.frames, we can still apply the kinds of\nvisual encoding ideas that we are familiar with. For example, we can still color\ncode or facet by fields in the vector dataset. To illustrate, we revisit the bus\nroute data from the last lecture and distinguish between buses operated by the\ncities of Madison vs. Monona. Before plotting, we fetch the underlying data.\n\n\nSys.setenv(MAPBOX_API_KEY=\"pk.eyJ1Ijoia3Jpc3JzMTEyOCIsImEiOiJjbDYzdjJzczQya3JzM2Jtb2E0NWU1a3B3In0.Mk4-pmKi_klg3EKfTw-JbQ\")\nbasemap <- cc_location(loc= c(-89.401230, 43.073051), buffer = 15e3)\nbus <- read_sf(\"https://uwmadison.box.com/shared/static/5neu1mpuh8esmb1q3j9celu73jy1rj2i.geojson\")\n\ntm_shape(basemap) +\n  tm_rgb() +\n  tm_shape(bus) +\n  tm_lines(col = \"#bc7ab3\", size = 1)\n\n\n\nNote that operator is the field containing information about which city is\noperating the buses. We can color code the routes by this attribute.\n\n\ntm_shape(basemap) +\n  tm_rgb() +\n  tm_shape(bus) +\n  tm_lines(col = \"operator\", size = 1) +\n  tm_layout(legend.bg.color = \"white\")\n\n\n\nAlternatively, we can facet.\n\n\ntm_shape(basemap) +\n  tm_rgb() +\n  tm_shape(filter(bus, !is.na(operator))) +\n  tm_lines(col = \"#bc7ab3\", size = 1) +\n  tm_facets(by = \"operator\")\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week07-02/week07-02_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-08-19T16:09:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week07-03/",
    "title": "Raster Data",
    "description": "Storing spatially gridded information in rasters.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-07",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(raster)\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(tmap)\ntheme_set(theme_bw())\n\n\n\n\nThe raster data format is used to store spatial data that lie along\nregular grids. The values along the grid are stored as entries in the\nmatrix. The raster object contains metadata that associates each entry\nin the matrix with a geographic coordinate.\n\n\n\n\nSince the data they must lie along a regular grid, rasters are most\noften used for continuously measured data, like elevation, temperature,\npopulation density, or landcover class.\n\n\n\n\nWe can create a raster using the rast command. The code\nblock below loads an elevation map measured by the space shuttle.\n\n\n\nf <- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\n zion <- rast(f)\n\n\n\n\n\nTyping the name of the object shows the metadata associated with it (but\nnot the actual grid values). We can see that the grid has 457 rows and\n465 columns. We also see its spatial extent: The minimum and maximum\nlongitude are both close to -113 and the latitudes are between 37.1 and\n37.5. A quick google map\nsearch shows that\nthis is located in Zion national park.\n\n\n\nzion\n\nclass       : SpatRaster \nsize        : 457, 465, 1  (nrow, ncol, nlyr)\nresolution  : 0.0008333333, 0.0008333333  (x, y)\nextent      : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : srtm.tif \nname        : srtm \nmin value   : 1024 \nmax value   : 2892 \n\n\n\nplot(zion)\n\n\n\n\n\n\n\nIn contrast, the raster command lets us create raster\nobjects from scratch. For example, the code below makes a raster with\nincreasing values in a 6 x 6 grid. Notice that we had to give a fake\nspatial extent.\n\n\n\ntest <- raster(\n   nrows = 6, ncols = 6, res = 0.5, \n   xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5,\n   vals = 1:36\n )\n \n plot(test)\n\n\n\n\n\n\n\nReal-world rasters typically have more than one layer of data. For\nexample, you might measure both elevation and slope along the same\nspatial grid, which would lead to a 2 layer raster. Or, for satellite\nimages, you might measure light at multiple wavelengths (usual RGB, plus\ninfrared or thermal for example).\n\n\n\n\nMulti-layer raster data can be read in using rast. You can\nrefer to particular layers in a multi-layer raster by indexing.\n\n\n\nf <- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\n satellite <- rast(f)\n \n satellite # all 4 channels\n\nclass       : SpatRaster \nsize        : 1428, 1128, 4  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       : landsat_1, landsat_2, landsat_3, landsat_4 \nmin values  :      7550,      6404,      5678,      5252 \nmax values  :     19071,     22051,     25780,     31961 \n\nsatellite[[1:2]] # only first two channels\n\nclass       : SpatRaster \nsize        : 1428, 1128, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       : landsat_1, landsat_2 \nmin values  :      7550,      6404 \nmax values  :     19071,     22051 \n\n\n\n\nBase R’s plot function supports plotting one layer of a\nraster at a time. To plot more than one layer in a multichannel image\n(like ordinary RGB images) you can use the plotRGB\nfunction.\n\n\n\nplotRGB(satellite, stretch = \"lin\")\n\n\n\n\n\n\n\nSometimes, it’s useful to overlay several visual marks on top of a\nraster image.\n\n\n\nsatellite <- project(satellite, \"EPSG:4326\")\n point <- data.frame(geom = st_sfc(st_point(c(-113, 37.3)))) %>%\n   st_as_sf()\n \n tm_shape(satellite) +\n   tm_raster() +\n   tm_shape(point) +\n   tm_dots(col = \"blue\", size = 5)\n\n\n\n\n\n\n\nIf we want to visualize just a single layer, we can use\ntm_rgb with all color channels set to the layer of\ninterest. Note that, here, I’ve rescaled the maximum value of each pixel\nto 255, since this is the default maximum value for a color image.\n\n\n\ntm_shape(satellite / max(satellite) * 255) +\n  tm_rgb(r = 1, g = 1, b = 1)\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:09:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week07-04/",
    "title": "Coordinate Reference Systems",
    "description": "The projection problem, and how to check your CRS.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-06",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(sf)\nlibrary(spData)\ntheme_set(theme_minimal())\n\n\n\n\nAn important subtlety of geographic data visualization is that all our\nmaps are in 2D, but earth is 3D. The process of associated points on the\nearth with 2D coordinates is called « projection. » All projections\nintroduce some level of distortion, and there is no universal, ideal\nprojection.\n\n\n\n\nHere are a few examples of famous global projections. There are also\nmany projections designed to give optimal representations locally within\na particular geographic area.\n\n\n\n\nThis means that there is no universal standard for how to represent\ncoordinates on the earth, and it’s common to find different projections\nin practice. For example, the block below shows how North America gets\nprojected according to two different CRSs.\n\n\n\n# some test projections, don't worry about this syntax\n miller <- \"+proj=mill +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n lambert <- \"+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n \n north_america <- world %>%\n   filter(continent == \"North America\")\n \n tm_shape(st_transform(north_america, miller)) +\n   tm_polygons()\n\n\ntm_shape(st_transform(north_america, lambert)) +\n   tm_polygons()\n\n\n\n\n\n\n\nBoth vector and raster spatial data will be associated with CRS’s. In\neither sf or raster objects, they can be\naccessed using the crs function.\n\n\n\n\nA common source of bugs is to use two different projections for the same\nanalysis. In this class, we will always use the EPSG:4326 projection,\nwhich is what is used in most online maps. But in your own projects, you\nshould always check that the projections are consistent across data\nsources. If you find an inconsistency, it will be important to\n« reproject » the data into the same CRS.\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:09:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week07-05/",
    "title": "Geospatial Interaction",
    "description": "Idioms for interacting with geographic data.",
    "author": [],
    "date": "2024-12-05",
    "categories": [],
    "contents": "\n\nCode,\nRecording\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\n\n\n\n\nMaps can be information dense, so it’s often useful to make them\ninteractive. These notes review some basic strategies for interactive\nspatial data visualization.\n\n\n\n\nleaflet is an easy-to-use R package that’s often sufficient\nfor routine visualization. It offers several types of marks (marks,\ncircles, polygons) and allows them to encode fields in a dataset. Note\nthat its interface is more like base R than ggplot2 — we specify each\nattribute in one plot command. For example, in the code block below,\naddTiles fetches the background map. addCircles overlays\nthe new vector features on top of the map. It’s worth noting that the\nvector features were created automatically – there was no need to create\nor read in any type of sf object.\n\n\n\ncities <- read_csv(\"https://uwmadison.box.com/shared/static/j98anvdoasfb1h651qxzrow2ua45oap1.csv\")\n leaflet(cities) %>%\n   addTiles() %>%\n   addCircles(\n     lng = ~Long,\n     lat = ~Lat,\n     radius = ~sqrt(Pop) * 30\n   )\n\n\n\n\n\n\n\n\n\n\n\n\nLeaflet maps can be\nembedded into\nShiny apps using leafletOutput and\nrenderLeaflet. For example, the Superzip Explorer is a\nvisualization designed for showing income and education levels across\nZIP codes in the US. In the\nserver,\nthe map is initialized using the leaflet command (without even adding\nany data layers).\n\n\n\n# Create the map\n output$map <- renderLeaflet({\n   leaflet() %>%\n     addTiles() %>%\n     setView(lng = -93.85, lat = 37.45, zoom = 4)\n })\n\n\n\n\n\n\n\n\n\nThe most interesting aspect of the explorer is that it lets us zoom into\nregions and study properties of ZIP codes within the current view.\nleaflet automatically creates an input\\(map_bounds&lt;/code&gt; input which is\ntriggered\nanytime we pan or zoom the map. It returns a subset of the full dataset\nwithin\nthe current view.&lt;/p&gt;\n&lt;div class=&quot;layout-chunk&quot;\ndata-layout=&quot;l-body&quot;&gt;\n&lt;div class=&quot;sourceCode&quot;&gt;\n&lt;pre class=&quot;sourceCode r&quot;&gt;&lt;code\nclass=&quot;sourceCode\nr&quot;&gt;&lt;span&gt;&lt;span\nclass=&#39;va&#39;&gt;zipsInBounds&lt;/span&gt;\n&lt;span\nclass=&#39;op&#39;&gt;&amp;lt;-&lt;/span&gt;\n&lt;span\nclass=&#39;fu&#39;&gt;reactive&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;(&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;{&lt;/span&gt;&lt;/span&gt;\n&lt;span&gt;  &lt;span\nclass=&#39;kw&#39;&gt;if&lt;/span&gt; &lt;span\nclass=&#39;op&#39;&gt;(&lt;/span&gt;&lt;span\nclass=&#39;fu&#39;&gt;&lt;a\nhref=&#39;https://rdrr.io/r/base/NULL.html&#39;&gt;is.null&lt;/a&gt;&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;(&lt;/span&gt;&lt;span\nclass=&#39;va&#39;&gt;input&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;\\)map_bounds)) return(zipdata[FALSE,]) # return empty data\n bounds <-\ninput\\(&lt;/span&gt;&lt;span\nclass=&#39;va&#39;&gt;map_bounds&lt;/span&gt;&lt;/span&gt;\n&lt;span&gt;  &lt;span\nclass=&#39;va&#39;&gt;latRng&lt;/span&gt;\n&lt;span\nclass=&#39;op&#39;&gt;&amp;lt;-&lt;/span&gt;\n&lt;span\nclass=&#39;fu&#39;&gt;&lt;a\nhref=&#39;https://rdrr.io/r/base/range.html&#39;&gt;range&lt;/a&gt;&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;(&lt;/span&gt;&lt;span\nclass=&#39;va&#39;&gt;bounds&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;\\)north, bounds\\(&lt;/span&gt;&lt;span\nclass=&#39;va&#39;&gt;south&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;)&lt;/span&gt;&lt;/span&gt;\n&lt;span&gt;  &lt;span\nclass=&#39;va&#39;&gt;lngRng&lt;/span&gt;\n&lt;span\nclass=&#39;op&#39;&gt;&amp;lt;-&lt;/span&gt;\n&lt;span\nclass=&#39;fu&#39;&gt;&lt;a\nhref=&#39;https://rdrr.io/r/base/range.html&#39;&gt;range&lt;/a&gt;&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;(&lt;/span&gt;&lt;span\nclass=&#39;va&#39;&gt;bounds&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;\\)east, bounds\\(&lt;/span&gt;&lt;span\nclass=&#39;va&#39;&gt;west&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;)&lt;/span&gt;&lt;/span&gt;\n&lt;span&gt;&lt;/span&gt;\n&lt;span&gt;  &lt;span class=&#39;co&#39;&gt;#\nfilters to current\nview&lt;/span&gt;&lt;/span&gt;\n&lt;span&gt;  &lt;span\nclass=&#39;fu&#39;&gt;&lt;a\nhref=&#39;https://rdrr.io/r/base/subset.html&#39;&gt;subset&lt;/a&gt;&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;(&lt;/span&gt;&lt;span\nclass=&#39;va&#39;&gt;zipdata&lt;/span&gt;,&lt;/span&gt;\n&lt;span&gt;    &lt;span\nclass=&#39;va&#39;&gt;latitude&lt;/span&gt;\n&lt;span\nclass=&#39;op&#39;&gt;&amp;gt;=&lt;/span&gt;\n&lt;span\nclass=&#39;va&#39;&gt;latRng&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;[&lt;/span&gt;&lt;span\nclass=&#39;fl&#39;&gt;1&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;]&lt;/span&gt; &lt;span\nclass=&#39;op&#39;&gt;&amp;amp;&lt;/span&gt;\n&lt;span\nclass=&#39;va&#39;&gt;latitude&lt;/span&gt;\n&lt;span\nclass=&#39;op&#39;&gt;&amp;lt;=&lt;/span&gt;\n&lt;span\nclass=&#39;va&#39;&gt;latRng&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;[&lt;/span&gt;&lt;span\nclass=&#39;fl&#39;&gt;2&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;]&lt;/span&gt; &lt;span\nclass=&#39;op&#39;&gt;&amp;amp;&lt;/span&gt;&lt;/span&gt;\n&lt;span&gt;      &lt;span\nclass=&#39;va&#39;&gt;longitude&lt;/span&gt;\n&lt;span\nclass=&#39;op&#39;&gt;&amp;gt;=&lt;/span&gt;\n&lt;span\nclass=&#39;va&#39;&gt;lngRng&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;[&lt;/span&gt;&lt;span\nclass=&#39;fl&#39;&gt;1&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;]&lt;/span&gt; &lt;span\nclass=&#39;op&#39;&gt;&amp;amp;&lt;/span&gt;\n&lt;span\nclass=&#39;va&#39;&gt;longitude&lt;/span&gt;\n&lt;span\nclass=&#39;op&#39;&gt;&amp;lt;=&lt;/span&gt;\n&lt;span\nclass=&#39;va&#39;&gt;lngRng&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;[&lt;/span&gt;&lt;span\nclass=&#39;fl&#39;&gt;2&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;]&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;)&lt;/span&gt;&lt;/span&gt;\n&lt;span&gt;&lt;span\nclass=&#39;op&#39;&gt;}&lt;/span&gt;&lt;span\nclass=&#39;op&#39;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;p&gt;Whenever this reactive is run, the histogram\n(&lt;code&gt;output\\)histCentile) and\nscatterplot (output$scatterCollegeIncome) on the side of\nthe app are updated.\n\n\n\n\nNotice that an observer was created to monitor any interactions with the\nmap. Within this observe block, a leafletProxy call is\nused. This function makes it possible to modify a leaflet map without\nredrawing the entire map. It helps support efficient rendering – we’re\nable to change the colors of the circles without redrawing the entire\nleaflet view.\n\n\n\nleafletProxy(\"map\", data = zipdata) %>%\n   clearShapes() %>%\n   addCircles(~longitude, ~latitude, radius=radius, layerId=~zipcode,\n     stroke=FALSE, fillOpacity=0.4, fillColor=pal(colorData)) %>%\n   addLegend(\"bottomleft\", pal=pal, values=colorData, title=colorBy,\n     layerId=\"colorLegend\")\n\n\n\n\n\nWe can often dynamically query spatial data. Querying a map can\nhighlight properties of samples within a geographic region. For example,\nhere is a map of in which each US county has been associated with a\n(random) pair of data features. Clicking on counties (or hovering with\nthe shift key pressed) updates the bars on the right. Each bar shows the\naverage from one of the data fields, across all selected counties.\n\n\n\n\n\n\n\n\nThis linking is accomplished using event listeners. For example, the map\nincludes the call .on(“mouseover”, update_selection), and\nupdate_selection changes the fill of the currently hovered\ncounty,\n\n\n\nsvg.selectAll(\"path\")\n  .attr(\"fill\", (d) => selected.indexOf(d.id) == -1 ? \"#f7f7f7\" : \"#4a4a4a\");\n\n\n\nThe full implementation can be read\nhere.\nNote that interactivity here is done just like in any other D3\nvisualization. We can treat the map as just another collection of SVG\npaths, and all our interaction events behave in the same way.\n\n\n\n\nWe can also imagine selecting geographic regions by interacting with\nlinked views. This is used in Nadieh Bremer’s Urbanization in East Asia\nvisualization, for\nexample, where we can see all the urban areas within a country by\nhovering its associated bar.\n\n\n\n\n\n\n\n\nHere is a somewhat more complex version of the earlier random data\nexample where acounties are associated with (random) time series.\nRedrawing the lower and upper bounds on the time series changes which\ncounties are highlighted.\n\n\n\n\n\n\n\n\nThough it’s not exactly interaction, another common strategy for\nspatiotemporal data is animation. The major trends often become apparent\nby visualizing the flow of visual marks on the screen. For example, how\ncan we visualize where football players go on a field before they score\na goal? One approach is to animate the trajectories leading up to the\ngoal. Here is one beautiful visualization (in D3!) by Karim Douieb that\nshows the most common paths and the speed at which the players run.\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:10:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week08-01/",
    "title": "Introduction to Networks and Trees",
    "description": "Typical tasks and example network datasets.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-04",
    "categories": [],
    "contents": "\n\nReading\n1 (Chapter 9),\nReading\n2,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\n\n\nNetworks and trees can be used to represent information in a variety of\ncontexts. Abstractly, networks and trees are types of graphs,\nwhich are defined by (a) a set \\(V\\) of vertices and (b) a set \\(E\\) of\nedges between pairs of vertices.\n\n\n\n\nIt is helpful to have a few specific examples in mind,\n\n\nThe Internet: \\(V =\n\\{\\text{All Webpages}\\}, \\left(v, v^{\\prime}\\right) \\in\nE\\) if there is a hyperlink between pages \\(v\\) and\n\\(v^{\\prime}\\).\n\n\nEvolutionary Tree: \\(V = \\{\\text{All past and present species}\\},\n\\left(v,\nv^{\\prime}\\right) \\in E\\) if one of the species \\(v\\) or\n\\(v^{\\prime}\\) is a descendant of the\nother.\n\n\nDisease Transmission: \\(V = \\{\\text{Community Members}\\}, \\left(v,\nv^{\\prime}\\right) \\in E\\) if the two community members\nhave come in close contact.\n\n\nDirectory Tree: \\(V\n= \\{\\text{All directories in a computer}\\}, \\left(v,\nv^{\\prime}\\right) \\in E\\) if one directory is contained in\nthe other.\n\n\n\n\n\n\nFigure 1: A visualization of the internet, from the\nOpte Project.\n\n\n\n\n\n\n\nFigure 2: An evolutionary tree, from the\nInteractive Tree of Life.\n\n\n\n\n\n\n\nFigure 3: A COVID-19 transmission network, from Clustering and\nsuperspreading potential of SARS-CoV-2 infections in Hong Kong.\n\n\n\n\n\n\n\nFigure 4: Directories in a file system can be organized into a tree,\nwith parent and child directories.\n\n\n\n\n\nEither vertices or edges might have attributes. For example, in the\ndirectory tree, we might know the sizes of the files (vertex attribute),\nand in the disease transmission network we might know the duration of\ncontact between individuals (edge attribute).\n\n\n\n\nAn edge may be either undirected or directed. In a directed edge, one\nvertex leads to the other, while in an undirected edge, there is no\nsense of ordering.\n\n\n\n\nIn R, the tidygraph package can be used to manipulate graph\ndata. It’s tbl_graph class stores node and edge attributes\nin a single data structure. and ggraph extends the usual\nggplot2 syntax to graphs.\n\n\n\n\nE <- data.frame(\n  source = c(1, 2, 3, 4, 5),\n  target = c(3, 3, 4, 5, 6)\n)\n\nG <- tbl_graph(edges = E)\nG\n\n# A tbl_graph: 6 nodes and 5 edges\n#\n# A rooted tree\n#\n# Node Data: 6 × 0 (active)\n#\n# Edge Data: 5 × 2\n   from    to\n  <int> <int>\n1     1     3\n2     2     3\n3     3     4\n# ℹ 2 more rows\n\n\nThis tbl_graph can be plotted using the code below. There\nare different geoms available for nodes and edges – for example, what\nhappens if you replace geom_edge_link() with\ngeom_edge_arc()?\n\n\n\nggraph(G, layout = 'kk') + \n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\nWe can mutate node and edge attributes using dplyr-like syntax. Before\nmutating edges, it’s necessary to call activate(edges).\n\n\n\nG <- G %>%\n  mutate(\n    id = row_number(),\n    group = id < 4\n  ) %>%\n  activate(edges) %>%\n  mutate(width = runif(n()))\nG\n\n# A tbl_graph: 6 nodes and 5 edges\n#\n# A rooted tree\n#\n# Edge Data: 5 × 3 (active)\n   from    to  width\n  <int> <int>  <dbl>\n1     1     3 0.0539\n2     2     3 0.946 \n3     3     4 0.945 \n4     4     5 0.959 \n5     5     6 0.603 \n#\n# Node Data: 6 × 2\n     id group\n  <int> <lgl>\n1     1 TRUE \n2     2 TRUE \n3     3 TRUE \n# ℹ 3 more rows\n\n\nNow we can visualize these derived attributes using an aesthetic mapping\nwithin the geom_edge_link and geom_node_point\ngeoms.\n\n\n\nggraph(G, layout = \"kk\") +\n  geom_edge_link(aes(width = width)) +\n  geom_node_label(aes(label = id))\n\n\n\n\nFigure 5: The same network as above, but with edge size encoding the\nweight attribute.\n\n\n\n\nExample Tasks\n\n\n\nWhat types of data that are amenable to representation by networks or\ntrees? What visual comparisons do networks and trees facilitate?\n\n\n\n\nOur initial examples suggest that trees and networks can be used to\nrepresent either physical interactions or conceptual relationships.\nTypical tasks include,\n\n\nSearching for groupings\n\n\nFollowing paths\n\n\nIsolating key nodes\n\n\n\n\nBy “searching for groupings,” we mean finding clusters of nodes that are\nhighly interconnected, but which have few links outside the cluster.\nThis kind of modular structure might lend itself to deeper investigation\nwithin each of the clusters.\n\n\nClusters in a network of political blogs might suggest an echo chamber\neffect.\n\n\nGene clusters in a differential expression study might suggest pathways\nneeded for the production of an important protein.\n\n\nClusters in a recipe network could be used identify different culinary\ntechniques or cuisines.\n\n\n\n\n\n\nFigure 6: A representation of 1200 blogs before the 2004 election, from\nThe political blogosphere and the 2004 US election: divided they\nblog.\n\n\n\n\n\nBy “following paths,” we mean tracing the paths out from a particular\nnode, to see which other nodes it is close to.\n\n\nFollowing paths in a citation network might reveal the chain of\npublications that led to an important discovery.\n\n\nFollowing paths in a recommendation network might suggest other users\nwho might be interested in watching a certain movie.\n\n\n\n\n\n\nFigure 7: A recommendation network, linking individuals and the movies\nthat they viewed.\n\n\n\n\n\n“Isolating key nodes” is a more fuzzy concept, usually referring to the\ntask of finding nodes that are exceptional in some way. For example,\nit’s often interesting to find nodes with many more connections than\nothers, or which link otherwise isolated clusters.\n\n\nA node with many edges in a disease transmission network is a\nsuperspreader.\n\n\nA node that links two clusters in a citation network might be especially\ninterdisciplinary.\n\n\nA node with large size in a directory tree might be a good target for\nreducing disk usage.\n\n\n\n\n\n\nFigure 8: The scientific journal, Social Networks, links several\npublication communities, as found by Betweenness Centrality as an\nIndicator of the Interdisciplinarity of Scientific Journals.\n\n\n\n\nIf you find these questions interesting, you might enjoy the catalog of\nexamples on the website\nVisualComplexity.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:04:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week08-02/",
    "title": "Node - Link Diagrams",
    "description": "The most common network visualization strategy.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-03",
    "categories": [],
    "contents": "\n\nReading\n(Chapter 9),\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(ggraph)\nlibrary(gridExtra)\nlibrary(networkD3)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\n\nA node-link diagram is a visual encoding strategy for network data,\nwhere nodes are drawn as points and links between nodes are drawn as\nlines between them. The dataset below is a friendship network derived\nfrom a survey of high schoolers in 1957 and 1958, available in the\ntidygraph package.\n\n\n\nG_school <- as_tbl_graph(highschool) %>%\n  activate(edges) %>%\n  mutate(year = factor(year))\n\nggraph(G_school) +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\n\n\n\n\nFor trees, the vertical or radial position can further encode the depth\nof a node in the tree. The data below represent the directory structure\nfrom a widely used web package called flare.\n\n\n\nG_flare <- tbl_graph(flare$vertices, flare$edges)\np1 <- ggraph(G_flare, 'tree') + \n  geom_edge_link() +\n  geom_node_label(aes(label = shortName), size = 3)\n\np2 <- ggraph(G_flare, 'tree', circular = TRUE) + \n  geom_edge_link() +\n  geom_node_label(aes(label = shortName), size = 3)\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nFigure 1: The same node-link diagram, with either height or radius\nencoding depth in the tree.\n\n\n\n\n\nIn either trees or networks, attributes of nodes and edges can be\nencoded using size (node radius or edge width) or color.\n\n\n\n\nThe node-link representation is especially effective for the task of\nfollowing paths. It’s an intuitive visualization for examining the local\nneighborhood of one node or describing the shortest path between two\nnodes.\n\n\n\n\nIn node-link diagrams, spatial position is subtle. It does not directly\nencode any attribute in the dataset, but layout algorithms (i.e.,\nalgorithms that try to determine the spatial positions of nodes in a\nnode-link diagram) try to ensure that nodes that are close to one\nanother in the shortest-path-sense also appear close to one another on\nthe page.\n\n\n\n\np1 <- ggraph(G_school, layout = \"kk\") +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\np2 <- ggraph(G_school, layout = \"fr\") +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nFigure 2: A comparison of two layout algorithms for the same network.\n\n\n\n\n\nOne common layout algorithm uses force-directed placement. The edges\nhere are interpreted as physical springs, and the node positions are\niteratively updated according to the forces induced by the springs.\n\n\n\n\nHere is an interactive force-directed layout of the network from above,\ngenerated using the networkD3 package.\n\n\n\n\nschool_edges <- G_school %>%\n  activate(edges) %>%\n  as.data.frame()\nsimpleNetwork(school_edges)\n\n\n\n\n\n\n\nThe key drawback of node-link diagrams is that they do not scale well to\nnetworks with a large number of nodes or with a large number of edges\nper node. The nodes and edges begin to overlap too much, and the result\nlooks like a « hairball. »\n\n\n\n\nIn this situation, it is possible to use additional structure in the\ndata to salvage the node-link display. For example, in a large tree, a\nrectangular or BubbleTree layout can be used.\n\n\n\n\n\n\nFigure 3: Example rectangular and BubbleTree layouts, for very large\ntrees, as shown in Visualization Analysis and Design.\n\n\n\n\nIf a large network has a modular structure, then it is possible to first\nlay out the separate clusters far apart from one another, before running\nforce directed placement.\n\n\n\n\n\nFigure 4: A hierarchical force directed layout algorithm, as shown in\nVisualization Analysis and Design.\n\n\n\n\nIf many edges go through a few shared paths, it may be possible to\nbundle them.\n\n\n\n\n\nFigure 5: In edge bundling, similar paths are placed close to one\nanother.\n\n\n\n\nBundled connections can be visualized using the\ngeom_conn_bundle geometry in ggraph. Before using this\nlayout, it is necessary to have a hierarchy over all the nodes, since\nshared ancestry among connected nodes is how the proximity of paths is\ndetermined.\n\n\n\nfrom <- match(flare$imports$from, flare$vertices$name)\nto <- match(flare$imports$to, flare$vertices$name)\nggraph(G_flare, layout = 'dendrogram', circular = TRUE) + \n  geom_conn_bundle(data = get_con(from = from, to = to), alpha = 0.1) + \n  geom_node_label(aes(label = shortName), size = 2) +\n  coord_fixed()\n\n\n\n\nFigure 6: An example of hierarchical edge bundling in R.\n\n\n\n\nTo summarize, node-link diagrams are very good for characterizing local\nstructure, but struggle with large networks.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:05:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week08-03/",
    "title": "Adjacency Matrix Views",
    "description": "A scalable network visualization strategy.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-02",
    "categories": [],
    "contents": "\n\nReading\n(Chapter 9),\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(gridExtra)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\n\n\nThe adjacency matrix of an undirected graph is the matrix with a 1 in\nentry \\(ij\\) if nodes \\(i\\) and\n\\(j\\)\nare linked by an edge and 0 otherwise. It has one row and one column for\nevery node in the graph. Visually, these 1’s and 0’s can be encoded as a\nblack and white squares.\n\n\n\n\nThe example below shows the node-link and adajcency matrix plots\nassociated with the toy example from the first lecture.\n\n\n\n\nE <- matrix(c(1, 3, 2, 3, 3, 4, 4, 5, 5, 6),\n  byrow = TRUE, ncol = 2) %>%\n  as_tbl_graph() %>%\n  mutate(\n    id = row_number(),\n    group = id < 4\n  ) \n\np1 <- ggraph(E, layout = 'kk') + \n    geom_edge_fan() +\n    geom_node_label(aes(label = id))\n\np2 <- ggraph(E, \"matrix\") +\n    geom_edge_tile(mirror = TRUE, show.legend = TRUE) +\n    geom_node_text(aes(label = id), x = -.5, nudge_y = 0.5) +\n    geom_node_text(aes(label = id), y = 0.5, nudge_x = -0.5) +\n    scale_y_reverse(expand = c(0, 0, 0, 1.5)) + # make sure the labels aren't hidden\n    scale_x_discrete(expand = c(0, 1.5, 0, 0)) +\n    coord_fixed() # make sure squares, not rectangles\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\nIn a directed graph, the \\(ij^{th}\\) entry is set to 1 if node\n\\(i\\)\nleads directly to node \\(j\\). Unlike the undirected graph, it\nis not necessarily symmetric.\n\n\n\n\nThe example below shows the adjacency matrix associated with the\nhigh-school student friendship network from last lecture. This is a\ndirected graph, because the students were surveyed about their closest\nfriends, and this was not always symmetric.\n\n\n\n\nG_school <- as_tbl_graph(highschool) %>%\n  activate(edges) %>%\n  mutate(year = factor(year))\n\nggraph(G_school, \"matrix\") +\n  geom_edge_tile() +\n  coord_fixed() +\n  facet_wrap(~ year)\n\n\n\n\n\nNode properties can be encoded along the rows and columns of the matrix.\nEdge properties can be encoded by the color or opacity of cells in the\nmatrix. The example below simulates clustered data and draws the cluster\nlabel for each node along rows / columns.\n\n\n\nG <- sample_islands(4, 40, 0.4, 15) %>%\n  as_tbl_graph() %>%\n  mutate(\n    group = rep(seq_len(4), each = 40),\n    group = as.factor(group)\n  )\n\nggraph(G, \"matrix\") +\n  geom_node_point(aes(col = group), x = -1) +\n  geom_node_point(aes(col = group), y = 1) +\n  geom_edge_tile(mirror = TRUE) +\n  scale_y_reverse() +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()\n\n\n\n\n\nThe example below takes a protein network and colors each edge according\nto the weight of experimental evidence behind that interaction.\n\n\n\nproteins <- read_tsv(\"https://uwmadison.box.com/shared/static/t97v5315isog0wr3idwf5z067h4j9o7m.tsv\") %>%\n  as_tbl_graph(from = node1, to = node2)\n\nggraph(proteins, \"matrix\") +\n  geom_edge_tile(aes(fill = combined_score), mirror = TRUE) +\n  coord_fixed() +\n  geom_node_text(aes(label = name), size = 3, x = -2.5, nudge_y = 0.5, hjust = 0) +\n  geom_node_text(aes(label = name), size = 3, angle = 90, y = 0.5, nudge_x = -0.5, hjust = 0) +\n  scale_y_reverse(expand = c(0, 0, 0, 2.7)) + # make sure the labels aren't hidden\n  scale_x_discrete(expand = c(0, 3, 0, 0)) +\n  scale_edge_fill_distiller() +\n  labs(edge_fill = \"Edge Confidence\")\n\n\n\n\n\n\nThe key advantage of visualization using adjacency matrices is that they\ncan scale to large and dense networks. It’s possible to perceive\nstructure even when the squares are quite small, and there is no risk of\nedges overlapping with one another.\n\n\n\n\nAdjacency matrices can be as useful as node-link diagrams for the task\nof finding group structure. The example below shows how cliques (fully\nconnected subsets), bicliques (sets of nodes that are fully connected\nbetween one another), and clusters (densely, but not completely,\nconnected subsets) are clearly visible in both node-link and adjacency\nmatrix visualizations.\n\n\n\n\n\n\nFigure 1: Cliques, bicliques, and clusters are visually salient using\neither node-link or adjacency matrix representations of a matrix.\n\n\n\n\nThe key disadvantage of adjacency matrix visualization is that it’s\nchallenging to make sense of the local topology around a node. Finding\nthe « friends of friends » of a node requires effort.\n\n\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nAnother issue is that different orderings of rows and columns can have a\ndramatic effect on the structure that’s\nvisible1.\nFor example, here is the same clustered network from 5., but with a\nrandom reordering of rows and columns. This is also beautifully\nillustrated at this\npage (change the\n“order” dropdown menu).\n\n\n\nggraph(G, \"matrix\", sort.by = sample(1:160)) +\n  geom_node_point(aes(col = group), x = -1) +\n  geom_node_point(aes(col = group), y = 1) +\n  geom_edge_tile(mirror = TRUE) +\n  scale_y_reverse() +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()\n\n\n\n\n\n\nThese visualizations also tend to be less intuitive for audiences that\nhaven’t viewed them before.\n\n\n\n\nIn a sense, adjacency matrix and node-link visualizations are\ncomplementary to one another.\nSome\napproaches try to use the adjacency matrix to give a global view of\na network, and dynamic queries on the adjacency matrix can reveal the\nassociated, local node-link diagrams.\n\n\n\n\n\n\n\nThough viewed differently, this additional degree of freedom can\nfacilitate richer\ncomparisons.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:05:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week08-04/",
    "title": "Enclosure",
    "description": "Visualization of hierarchical structure using containment.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-12-01",
    "categories": [],
    "contents": "\n\nReading\n(Chapter 9),\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(gridExtra)\ntheme_set(theme_graph())\n\n\n\nIf nodes can be conceptually organized into a hierarchy, then it’s\npossible to use enclosure (i.e., the containment of some visual marks\nwithin others) to encode those relationships.\n\n\n\ngraph <- tbl_graph(flare$vertices, flare$edges)\n\np1 <- ggraph(graph, \"tree\") +\n  geom_edge_link() +\n  geom_node_point(aes(size = size)) +\n  scale_size(range = c(0.1, 5))\np2 <- ggraph(graph, \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth)) +\n  scale_fill_distiller(direction = 1) +\n  coord_fixed()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nFigure 1: A tree and the equivalent representation using containment.\nThe outer circle corresponds to the root node in the tree, and paths\ndown the tree are associated with sequences of nested circles.\n\n\n\n\n\nHierarchy most obviously occurs in trees, but it can also be present in\nnetworks with clustering structure at several levels. (see point 6\nbelow).\n\n\n\n\nEnclosure is used in treemaps. In this visualization, each node is\nallocated an area, and all its children are drawn within that area (and\nso on, recursively, down to the leaves).\n\n\n\n\nggraph(graph, \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth, size = depth), size = 0.25) +\n  scale_fill_distiller(direction = 1) +\n  coord_fixed()\n\n\n\n\nFigure 2: A treemap representation associated with the tree from above.\n\n\n\n\n\nThis is a particularly useful visualization when it’s important to\nvisualize a continuous attribute associated with each node. For example,\na large node might correspond to a large part of a budget or a large\ndirectory in a filesystem. Here is an example\nvisualization\nof Obama’s budget proposal in 2013.\n\n\n\n\nA caveat: treemaps are not so useful for making topological comparisons,\nlike the distance between two nodes in the tree.\n\n\n\n\nIn situations where network nodes can be organized hierarchically,\ncontainment marks can directly represent the these relationships. For\nexample, in the network below, the red and yellow clusters are contained\nin a green supercluster. The combination of node-link diagram and\ncontainment sets makes this structure clear.\n\n\n\n\n\n\nFigure 3: An example of how hierarchy across groups of nodes can be\nencoded within a network.\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:05:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week09-01/",
    "title": "K-means",
    "description": "An introduction to clustering and how to manage its output.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-30",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(\"dslabs\")\nlibrary(\"ggplot2\")\nlibrary(\"knitr\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\ntheme_set(theme_minimal())\n\n\n\n\nThe goal of clustering is to discover distinct groups within a dataset.\nIn an ideal clustering, samples are very different between groups, but\nrelatively similar within groups. At the end of a clustering routine,\n\\(K\\) clusters have been\nidentified, and each sample is assigned to one of these \\(K\\) clusters. \\(K\\) must be chosen by the\nuser.\n\n\n\n\nClustering gives a compressed representation of the dataset. Therefore,\nclustering is useful for getting a quick overview of the high-level\nstructure in a dataset.\n\n\n\n\nFor example, clustering can be used in the following applications,\n\n\nUser segmentation: A marketing study might try to summarize different\nuser behaviors by clustering into a few key “segments.” Each segment\nmight motivate a different type of marketing campaign.\n\n\nGene expression profiling: A genomics study might try to identify genes\nwhose expression levels are similar across different experimental\nconditions. These gene clusters might be responsible for a shared\nbiological function.\n\n\n\n\n\\(K\\)-means is a particular\nalgorithm for finding clusters. First, it randomly initializes \\(K\\) cluster centroids. Then,\nit alternates the following two steps until convergence,\n\n\nAssign points to their nearest cluster centroid.\n\n\nUpdate the \\(K\\) centroids to be the\naverages of points within their cluster.\n\n\n\nHere is an animation from the tidymodels page on \\(K\\)-means,\n\n\n\n\n\n\n\nNote that, since we have to take an average for each coordinate, we\nrequire that our data be quantitative, not\ncategorical1.\n\n\n\n\nWe illustrate this idea using the movielens dataset from\nthe reading. This dataset has ratings (0.5 to 5) given by 671 users\nacross 9066 movies. We can think of this as a matrix of movies\nvs. users, with ratings within the entries. For simplicity, we filter\ndown to only the 50 most frequently rated movies. We will assume that if\na user never rated a movie, they would have given that movie a\nzero2.\nWe’ve skipped a few steps used in the reading (subtracting movie / user\naverages and filtering to only active users), but the overall results\nare comparable.\n\n\n\n\ndata(\"movielens\")\nfrequently_rated <- movielens %>%\n  group_by(movieId) %>%\n  summarize(n=n()) %>%\n  top_n(50, n) %>%\n  pull(movieId)\n\nmovie_mat <- movielens %>% \n  filter(movieId %in% frequently_rated) %>%\n  select(title, userId, rating) %>%\n  pivot_wider(title, names_from = userId, values_from = rating, values_fill = 0)\n\nmovie_mat[1:10, 1:20]\n\n# A tibble: 10 × 20\n   title     `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Seven …     4   0       0     0     0     0   5       3     0     0\n 2 Usual …     4   0       0     0     0     0   5       0     5     5\n 3 Braveh…     4   4       0     0     0     5   4       0     0     0\n 4 Apollo…     5   0       0     4     0     0   0       0     0     0\n 5 Pulp F…     4   4.5     5     0     0     0   4       0     0     5\n 6 Forres…     3   5       5     4     0     3   4       0     0     0\n 7 Lion K…     3   0       5     4     0     3   0       0     0     0\n 8 Mask, …     3   0       4     4     0     3   0       0     0     0\n 9 Speed       3   2.5     0     4     0     3   0       0     0     0\n10 Fugiti…     3   0       0     0     0     0   4.5     0     0     0\n# ℹ 9 more variables: `12` <dbl>, `13` <dbl>, `14` <dbl>, `15` <dbl>,\n#   `16` <dbl>, `17` <dbl>, `18` <dbl>, `19` <dbl>, `20` <dbl>\n\n\nNext, we run kmeans on this dataset. I’ve used the dplyr\npipe notation to run kmeans on the data above with “title”\nremoved. augment is a function from the tidymodels package\nthat adds the cluster labels identified by kmeans to the\nrows in the original dataset.\n\n\n\nkclust <- movie_mat %>%\n  select(-title) %>%\n  kmeans(centers = 10)\n\nmovie_mat <- augment(kclust, movie_mat) # creates column \".cluster\" with cluster label\nkclust <- tidy(kclust)\n\nmovie_mat %>%\n  select(title, .cluster) %>%\n  arrange(.cluster)\n\n# A tibble: 50 × 2\n   title                                          .cluster\n   <chr>                                          <fct>   \n 1 Braveheart                                     1       \n 2 Forrest Gump                                   1       \n 3 Jurassic Park                                  1       \n 4 Terminator 2: Judgment Day                     1       \n 5 Pulp Fiction                                   2       \n 6 Silence of the Lambs, The                      2       \n 7 Shawshank Redemption, The                      2       \n 8 Fight Club                                     3       \n 9 Lord of the Rings: The Return of the King, The 3       \n10 Shrek                                          3       \n# ℹ 40 more rows\n\n\nThere are two pieces of derived data generated by this routine,\n\nThe cluster assignments\n\n\nThe cluster centroids and both can be the subjects of visualization.\n\n\n\nIn our movie example, the cluster centroids are imaginary pseudo-movies\nthat are representative of their cluster. They are represented by the\nscores they would have received by each of the users in the dataset.\nThis is visualized below. In a more realistic application, we would also\nwant to display some information about each user; e.g., maybe some\nmovies are more popular among certain age groups or in certain regions.\n\n\n\nkclust_long <- kclust %>%\n  pivot_longer(`2`:`671`, names_to = \"userId\", values_to = \"rating\")\n\nggplot(kclust_long) +\n  geom_bar(\n    aes(x = reorder(userId, rating), y = rating),\n    stat = \"identity\"\n  ) +\n  facet_grid(cluster ~ .) +\n  labs(x = \"Users (sorted)\", y = \"Rating\") +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 5),\n    strip.text.y = element_text(angle = 0)\n  )\n\n\n\n\nFigure 1: We can visualize each cluster by seeing the average ratings\neach user gave to the movies in that cluster (this is the definition of\nthe centroid). An alternative visualization strategy would be to show a\nheatmap – we’ll discuss this soon in the superheat lecture.\n\n\n\n\n\nIt’s often of interest to relate the cluster assignments to\ncomplementary data, to see whether the clustering reflects any\npreviously known differences between the observations, which weren’t\ndirectly used in the clustering algorithm.\n\n\n\n\nBe cautious: Outliers, nonspherical shapes, and variations in density\ncan throw off \\(K\\)-means.\n\n\n\n\n\n\nFigure 2: The difficulty that variations in density poses to k-means,\nfrom Cluster Analysis using K-Means Explained.\n\n\n\n\nThe goals of clustering are highly problem dependent, and different\ngoals might call for alternative algorithms. For example, consider the\nways clustering might be used to understand disease transmission. One\nproblem might be to cluster the DNA sequences of the pathogenic agent,\nto recover its evolutionary history. This could be done using\nhierarchical clustering (next lecture). A second problem might be to\ndetermine whether patient outcomes might be driven by one of a few\nenvironmental factors, in which case a \\(K\\)-means clustering across\nthe typical environmental factors would be reasonable. A third use would\nbe to perform contact tracing, based on a network clustering algorithm.\nThe point is that no clustering algorithm is uniformly better than any\nother in all situations, and the choice of which one to use should be\nguided by the problem requirements.\n\n\n\n\n\n\nTo work with data of different types, it’s possible to use a variant\ncalled \\(K\\)-medoids, but this is\nbeyond the scope of this\nclass.↩︎\n\n\n\n\nThis is an approximation – there are probably many movies that the users\nwould have enjoyed had they had a chance to watch\nthem.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:13:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week09-03/",
    "title": "Heatmaps",
    "description": "Visualizing table values, ordered by clustering results.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-28",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"superheat\")\nlibrary(\"tibble\")\ntheme_set(theme_minimal())\n\n\n\n\nThe direct outputs of a standard clustering algorithim are (a) cluster\nassignments for each sample, (b) the centroids associated with each\ncluster. A hierarchical clustering algorithm enriches this output with a\ntree, which provide (a) and (b) at multiple levels of resolution.\n\n\n\n\nThese outputs can be used to improve visualizations. For example, they\ncan be used to define small multiples, faceting across clusters. One\nespecially common idea is to reorder the rows of a heatmap using the\nresults of a clustering, and this is the subject of these notes.\n\n\n\n\nIn a heatmap, each mark (usually a small tile) corresponds to an entry\nof a matrix. The \\(x\\)-coordinate of the mark\nencodes the index of the observation, while the \\(y\\)-coordinate encodes the\nindex of the feature. The color of each tile represents the value of\nthat entry. For example, here are the first few rows of the movies data,\nalong with the corresponding heatmap, made using the superheat package.\n\n\n\n\nmovies_mat <- read_csv(\"https://uwmadison.box.com/shared/static/wj1ln9xtigaoubbxow86y2gqmqcsu2jk.csv\") %>%\n  column_to_rownames(var = \"title\")\n\n\n\n\ncols <- c('#f6eff7','#bdc9e1','#67a9cf','#1c9099','#016c59')\nsuperheat(movies_mat, left.label.text.size = 4, heat.pal = cols, heat.lim = c(0, 5))\n\n\n\n\n\nJust like in adjacency matrix visualizations, the effectiveness of a\nheatmap can depend dramatically on the way in which rows and columns are\nordered. To provide a more coherent view, we cluster both rows and\ncolumns, placing rows / columns belonging to the same cluster next to\none another.\n\n\n\nmovies_clust <- movies_mat %>%\n  kmeans(centers = 10)\n\nusers_clust <- movies_mat %>%\n  t() %>%\n  kmeans(centers = 10)\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  order.rows = order(movies_clust$cluster),\n  order.cols = order(users_clust$cluster),\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\n\nsuperheat also makes it easy to visualize plot statistics adjacent ot\nthe adjacent to the main heatmap. These statistics can be plotted as\npoints, lines, or bars. Points are useful when we want to highlight the\nraw value, lines are effective for showing change, and bars give a sense\nof the area below a set of observations. In this example, we use an\nadded panel on the right hand side (yr) to encode the total\nnumber of ratings given to that movie. The yr.obs.cols\nallows us to change the color of each point in the adjacent plot. In\nthis example, we change color depending on which cluster the movie was\nfound to belong to.\n\n\n\ncluster_cols <- c('#8dd3c7','#ccebc5','#bebada','#fb8072','#80b1d3','#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd')\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  order.rows = order(movies_clust$cluster),\n  order.cols = order(users_clust$cluster),\n  heat.pal = cols,\n  heat.lim = c(0, 5),\n  yr = rowSums(movies_mat > 0),\n  yr.axis.name = \"Number of Ratings\",\n  yr.obs.col = cluster_cols[movies_clust$cluster],\n  yr.plot.type = \"bar\"\n)\n\n\n\n\n\nIt also makes sense to order the rows / columns using hierarchical\nclustering. This approach is especially useful when the samples fall\nalong a continuous gradient, rather than belonging to clearly delineated\ngroups. The pretty.order.rows and\npretty.order.cols arguments use hierarchical clustering to\nreorder the heatmap.\n\n\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  pretty.order.cols = TRUE,  \n  pretty.order.rows = TRUE,\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\n\nThe hierarchical clustering trees estimated by\npretty.order.rows and pretty.order.cols can be\nalso visualized.\n\n\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  pretty.order.cols = TRUE,  \n  pretty.order.rows = TRUE, \n  row.dendrogram = TRUE,\n  col.dendrogram = TRUE,\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:13:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week09-04/",
    "title": "Silhouette Statistics",
    "description": "Diagnostics for the quality of a clustering.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-27",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(\"cluster\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"tidymodels\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\ntheme_set(theme_bw())\nset.seed(123)\n\n\n\n\nClustering algorithms usually require the number of clusters \\(K\\) as an argument. How\nshould it be chosen?\n\n\n\n\nThere are many possible criteria, but one common approach is to compute\nthe silhouette statistic. It is a statistic that can be computed for\neach observation in a dataset, measuring how strongly it is tied to its\nassigned cluster. If a whole cluster has large silhouette statistics,\nthen that cluster is well-defined and clearly isolated other clusters.\n\n\n\n\nThe plots below illustrate the computation of silhouette statistics for\na clustering of the penguins dataset that used \\(K = 3\\). To set up, we first\nneed to cluster the penguins dataset. The idea is the same as in the\n\\(K\\)-means notes, but we\nencapsulate the code in a function, so that we can easily extract data\nfor different values of \\(K\\).\n\n\n\n\npenguins <- read_csv(\"https://uwmadison.box.com/shared/static/ijh7iipc9ect1jf0z8qa2n3j7dgem1gh.csv\") %>%\n  na.omit() %>%\n  mutate(id = row_number())\n\ncluster_penguins <- function(penguins, K) {\n  x <- penguins %>%\n    select(matches(\"length|depth|mass\")) %>%\n    scale()\n    \n  kmeans(x, center = K) %>%\n    augment(penguins) %>% # creates column \".cluster\" with cluster label\n    mutate(silhouette = silhouette(as.integer(.cluster), dist(x))[, \"sil_width\"])\n}\n\n\n\nDenote the silhouette statistic of observation \\(i\\) by \\(s_{i}\\). We will compute\n\\(s_i\\) for the observation\nwith the black highlight\nbelow1.\n\n\n\ncur_id <- 2\npenguins3 <- cluster_penguins(penguins, K = 3)\nobs_i <- penguins3 %>%\n  filter(id == cur_id)\n\n\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nFigure 1: The observation on which we will compute the silhouette\nstatistic.\n\n\n\n\nThe first step in the calculation of the silhouette statistic is to\nmeasure the pairwise distances between the observation \\(i\\) and all observations in\nthe same cluster. These distances are the lengths of the small lines\nbelow. Call average of these lengths \\(a_{i}\\).\n\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_segment(\n    data = penguins3 %>% filter(.cluster == obs_i$.cluster), \n    aes(xend = obs_i$bill_length_mm, yend = obs_i$bill_depth_mm),\n    size = 0.6, alpha = 0.3\n  ) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1)) +\n  labs(title = expression(paste(\"Distances used for \", a[i])))\n\n\n\n\nFigure 2: The average distance between the target observation and all\nothers in the same cluster.\n\n\n\n\nNext, we compute pairwise distances to all observations in clusters 2\nand 3. The average of these pairwise distances are called \\(b_{i2}\\) and \\(b_{i3}\\). Choose the smaller\nof \\(b_{i2}\\) and \\(b_{i3}\\), and call it \\(b_{i}\\). In a sense, this is\nthe “next best” cluster to put observation \\(i\\). For a general \\(K\\), you would compute \\(b_{ik}\\) for all \\(k\\) (other than observation\n\\(i\\)’s cluster) and take the\nminimum across all of them. In this case, the orange segments are on\naverage smaller than the blue segments, so \\(b_i\\) is defined as the\naverage length of the orange segments.\n\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_segment(\n    data = penguins3 %>% filter(.cluster != obs_i$.cluster), \n    aes(xend = obs_i$bill_length_mm, yend = obs_i$bill_depth_mm, col = .cluster),\n    size = 0.5, alpha = 0.3\n  ) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1)) +\n  labs(title = expression(paste(\"Distances used for \", b[i][1], \" and \", b[i][2])))\n\n\n\n\nFigure 3: The average distance between the target observation and all\nothers in different clusters.\n\n\n\n\n\nThe silhouette statistic for observation \\(i\\)\nis derived from the relative lengths of the orange vs. green segments.\nFormally, the silhouette statistic for observation \\(i\\) is \\(s_{i}:= \\frac{b_{i} -\na_{i}}{\\max\\left({a_{i},\nb_{i}}\\right)}\\). This number is close to 1 if the\norange segments are much longer than the green segments, close to 0 if\nthe segments are about the same size, and close to -1 if the the orange\nsegments are much shorter than the green\nsegments2.\n\n\n\n\nThe median of these \\(s_{i}\\) for all observations\nwithin cluster \\(k\\) is a measure of how\nwell-defined cluster \\(k\\)\nis overall. The higher this number, the more well-defined the cluster.\n\n\n\n\nDenote the median of the silhouette statistics within cluster \\(k\\) by \\(SS_{k}\\). A measure how good\na choice of \\(K\\) is can be determined by\nthe median of these medians: \\(\\text{Quality}(K) :=\n\\text{median}_{k = 1 \\dots,\nK} SS_{k}\\).\n\n\n\n\nIn particular, this can be used to define (a) a good cut point in a\nhierarchical clustering or (b) a point at which a cluster should no\nlonger be split into subgroups.\n\n\n\n\nIn R, we can use the silhouette function from the cluster\npackage to compute the silhouette statistic. The syntax is\nsilhouette(cluster_labels, pairwise_distances) where\ncluster_labels is a vector of (integer) cluster ID’s for\neach observation and pairwise_distances gives the lengths\nof the segments between all pairs of observations. An example of this\nfunction’s usage is given in the function at the start of the\nillustration.\n\n\n\n\nThis is what the silhouette statistic looks like in the penguins dataset\nwhen we choose 3 clusters. The larger points have lower silhouette\nstatistics. This points between clusters 2 and 3 have large silhouette\nstatistics because those two clusters blend into one another.\n\n\n\n\nggplot(penguins3) +\n  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster, size = silhouette)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nFigure 4: The silhouette statistics on the Palmers Penguins dataset,\nwhen using \\(K\\)-means with \\(K = 3\\).\n\n\n\n\nWe can also visualize the histogram of silhouette statistics within each\ncluster. Since the silhouette statistics for cluster 2 are generally\nlower than those for the other two clusters (in particular, its median\nis lower), we can conclude that it is less well-defined.\n\n\n\nggplot(penguins3) +\n  geom_histogram(aes(x = silhouette), binwidth = 0.05) +\n  facet_grid(~ .cluster)\n\n\n\n\nFigure 5: The per-cluster histograms of silhouette statistics summarize\nhow well-defined each cluster is.\n\n\n\n\nIf we choose even more clusters, then there are more points lying along\nthe boundaries of poorly defined clusters. Their associated silhouette\nstatistics end up becoming larger. From the histogram, we can also see a\ndeterioration in the median silhouette scores across all clusters.\n\n\n\npenguins4 <- cluster_penguins(penguins, K = 4)\nggplot(penguins4) +\n  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster, size = silhouette)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nFigure 6: We can repeat the same exercise, but with \\(K = 4\\) clusters instead.\n\n\n\n\n\nggplot(penguins4) +\n  geom_histogram(aes(x = silhouette), binwidth = 0.05) +\n  facet_grid(~ .cluster)\n\n\n\n\n\n\n\n\n\nYou can change cur_id to try different\nobservations.↩︎\n\n\n\n\nThis last case likely indicates a\nmisclustering.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:13:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week09-05/",
    "title": "Cluster Stability",
    "description": "How reliable are the results of a clustering?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-26",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(\"MASS\")\nlibrary(\"Matrix\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"pdist\")\nlibrary(\"superheat\")\nlibrary(\"tidyr\")\ntheme_set(theme_minimal())\nset.seed(1234)\n\n\n\n\nOne of the fundamental principles in statistics is that, no matter how\nthe experiment / study was conducted, if we ran it again, we would get\ndifferent results. More formally, sampling variability creates\nuncertainty in our inferences.\n\n\n\n\nHow should we think about sampling variability in the context of\nclustering? This is a tricky problem, because you can permute the labels\nof the clusters without changing the meaning of the clustering. However,\nit is possible to measure and visualize the stability of a point’s\ncluster assignment.\n\n\n\n\nTo make this less abstract, consider an example. A study has found a\ncollection of genes that are differentially expressed between patients\nwith two different subtypes of a disease. There is an interest in\nclustering genes that have similar expression profiles across all\npatients — these genes probably belong to similar biological processes.\n\n\n\n\nOnce you run the clustering, how sure can you be that, if the study\nwould run again, you would recover a similar clustering? Are there some\ngenes that you are sure belong to a particular cluster? Are there some\nthat lie between two clusters?\n\n\n\n\nTo illustrate, consider the simulated dataset below. Imagine that the\nrows are patients, the column are genes, and the colors are the\nexpression levels of genes within patients. There are 5 clusters of\ngenes here (columns 1 - 20 are cluster 1, 21 - 41 are cluster 2, …). The\nfirst two clusters are only weakly visible, while the last three stand\nout strongly.\n\n\n\n\nn_per <- 20\np <- n_per * 5\nSigma1 <- diag(2) %x% matrix(rep(0.3, n_per ** 2), nrow = n_per)\nSigma2 <- diag(3) %x% matrix(rep(0.6, n_per ** 2), nrow = n_per)\nSigma <- bdiag(Sigma1, Sigma2)\ndiag(Sigma) <- 1\nmu <- rep(0, 100)\nx <- mvrnorm(25, mu, Sigma)\n\ncols <- c('#f6eff7','#bdc9e1','#67a9cf','#1c9099','#016c59')\nsuperheat(\n  x, \n  pretty.order.rows = TRUE, \n  bottom.label = \"none\", \n  heat.pal = cols,\n  left.label.text.size = 3,\n  legend = FALSE\n)\n\n\n\n\nFigure 1: A simulated clustering of genes (columns) across rows\n(patients).\n\n\n\n\nThe main idea for how to compute cluster stability is to bootstrap\n(i.e., randomly resample) the patients and see whether the cluster\nassignments for each gene change. More precisely, we use the following\nstrategy,\n\n\nUsing all the patients, \\(X\\), estimate the cluster\ncentroids \\(c_{1}, \\dots,\nc_{K}\\).\n\n\nFor \\(B\\) bootstrap iterations,\nperform the following.\n\nSample the patients with replacement, generating a bootstrap resampled\nversion of the dataset \\(X_{b}^{\\ast}\\).\n\n\nPermute the original cluster centroids to reflect the order of patients\nin \\(X_{b}^{\\ast}\\). Call the\npermuted centroids \\(c_{1b}^{\\ast}, \\dots,\nc_{Kb}^{\\ast}\\).\n\n\nAssign genes in \\(X_{b}^{\\ast}\\) to the cluster\n\\(k\\) of the closest \\(c_{bk}^{\\ast}\\).\n\n\n\nWe quantify our certainty that gene \\(j\\)\nbelongs to cluster \\(k\\)\nby counting the number of times that gene \\(j\\) was assigned to cluster\n\\(k\\).\n\n\nThe picture below describes the bootstrapping process for a gene. The\ntwo rows correspond to the original and bootstrapped representations a\nspecific gene, respectively. Each bar gives the expression level of the\ngene for one individual. Due to the random sampling in the bootstrapped\ndataset, some individuals become overrepresented and some are removed.\nIf we also permute the centroids in the same way, we get a new distance\nbetween genes and their centroids. Since the patients who are included\nchanges, the distances between each gene and each centroid changes, so\nthe genes might be assigned to different clusters.\n\n\n\n\n\n\n\nK <- 5\nB <- 1000\ncluster_profiles <- kmeans(t(x), centers = K)$centers\ncluster_probs <- matrix(nrow = ncol(x), ncol = B)\n\nfor (b in seq_len(B)) {\n  b_ix <- sample(nrow(x), replace = TRUE)\n  dists <- as.matrix(pdist(t(x[b_ix, ]), cluster_profiles[, b_ix]))\n  cluster_probs[, b] <- apply(dists, 1, which.min)\n}\n\ncluster_probs <- as_tibble(cluster_probs) %>%\n  mutate(gene = row_number()) %>%\n  pivot_longer(-gene, names_to = \"b\", values_to = \"cluster\")\n\n\n\nThe table below shows the result of this procedure. In each bootstrap\niteration, gene 1 was assigned to cluster 4, so we can rely on that\nassignment. On the other hand, gene 3 is assigned to cluster 4 75% of\nthe time, but occasionally appears in clusters 1, 2, and 5.\n\n\n\ncluster_probs <- cluster_probs %>%\n  mutate(cluster = as.factor(cluster)) %>%\n  group_by(gene, cluster) %>%\n  summarise(prob = n() / B)\n\ncluster_probs\n\n# A tibble: 277 × 3\n# Groups:   gene [100]\n    gene cluster  prob\n   <int> <fct>   <dbl>\n 1     1 1       0.334\n 2     1 2       0.006\n 3     1 3       0.001\n 4     1 4       0.659\n 5     2 1       0.981\n 6     2 3       0.002\n 7     2 4       0.017\n 8     3 1       0.754\n 9     3 2       0.008\n10     3 3       0.112\n# ℹ 267 more rows\n\n\nThese fractions for all genes are summarized by the plot below. Each row\nis a gene. The length of each color gives the number of times that gene\nwas assigned to that cluster. The genes from rows 41 - 100 are all\nclearly distinguished, which is in line with what we saw visually in the\nheatmap above. The first two clusters are somewhat recovered, but since\nthey were often assigned to alternative clusters, we can conclude that\nthey were harder to demarcate out than the others.\n\n\n\nggplot(cluster_probs) +\n  geom_bar(aes(y = as.factor(gene), x = prob, col = cluster, fill = cluster), stat = \"identity\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = \"Gene\", x = \"Proportion\") +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_text(size = 7),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:13:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week10-1/",
    "title": "Introduction to Dimensionality Reduction",
    "description": "Examples of high-dimensional data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-25",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\nHigh-dimensional data are data where many features are\ncollected for each observation. These tend to be wide datasets with many\ncolumns. The name comes from the fact that each row of the dataset can\nbe viewed as a vector in a high-dimensional space (one dimension for\neach feature). These data are common in modern applications,\n\n\nEach cell in a genomics dataset might have measurements for hundreds of\nmolecules.\n\n\nEach survey respondent might provide answers to dozens of questions.\n\n\nEach image might have several thousand pixels.\n\n\nEach document might have counts across several thousand relevant words.\n\n\n\nFor low-dimensional data, we could visually encode all the features in\nour data directly, either using properties of marks or through faceting.\nIn high-dimensional data, this is no longer possible.\n\n\n\n\nHowever, though there are many features associated with each\nobservation, it may still be possible to organize samples across a\nsmaller number of meaningful, derived features.\n\n\n\n\nFor example, consider the Metropolitan Museum of Art dataset, which\ncontains images of many artworks. Abstractly, each artwork is a\nhigh-dimensional object, containing pixel intensities across many\npixels. But it is reasonable to derive a feature based on the average\nbrightness.\n\n\n\n\n\n\nFigure 1: An arrangement of artworks according to their average pixel\nbrightness, as given in the reading.\n\n\n\n\nIn general, manual feature construction can be difficult. Algorithmic\napproaches try streamline the process of generating these maps by\noptimizing some more generic criterion. Different algorithms use\ndifferent criteria, which we will review in the next couple of lectures.\n\n\n\n\n\nFigure 2: The dimensionality reduction algorithm in this animation\nconverts a large number of raw features into a position on a\none-dimensional axis defined by average pixel brightness. In general, we\nmight reduce to dimensions other than 1D, and we will often want to\ndefine features tailored to the dataset at hand.\n\n\n\n\nInformally, the goal of dimensionality reduction techniques is to\nproduce a low-dimensional “atlas” relating members of a collection of\ncomplex objects. Samples that are similar to one another in the\nhigh-dimensional space should be placed near one another in the\nlow-dimensional view. For example, we might want to make an atlas of\nartworks, with similar styles and historical periods being placed near\nto one another.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:55:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week10-2/",
    "title": "Principal Components Analysis I",
    "description": "Linear dimensionality reduction using PCA.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-24",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidymodels)\nlibrary(readr)\n\n\n\n\nIn our last notes, we saw how we could organize a collection of images\nbased on average pixel brightness. We can think of average pixel\nbrightness as a derived feature that can be used to build a\nlow-dimensional map.\n\n\n\n\nWe can partially automate the process of deriving new features. Though,\nin general, finding the best way to combine raw features into derived\nones is a complicated problem, we can simplify things by restricting\nattention to,\n\n\n\nFeatures that are linear combinations of the raw input columns.\n\n\nFeatures that are orthogonal to one another.\n\n\nFeatures that have high variance.\n\n\n\nRestricting to linear combinations allows for an analytical\nsolution. We will relax this requirement when discussing UMAP.\n\n\n\n\nOrthogonality means that the derived features will be\nuncorrelated with one another. This is a nice property, because it would\nbe wasteful if features were redundant.\n\n\n\n\nHigh variance is desirable because it means we preserve more of\nthe essential structure of the underlying data. For example, if you look\nat this 2D representation of a 3D object, it’s hard to tell what it is,\n\n\n\n\n\n\nFigure 1: What is this object?\n\n\n\n\nBut when viewing an alternative reduction which has higher variance…\n\n\n\n\n\nFigure 2: Not so complicated now. Credit for this example goes to\nProfessor Julie Josse, at Ecole Polytechnique.\n\n\n\n\n\nPrincipal Components Analysis (PCA) is the optimal dimensionality\nreduction under these three restrictions, in the sense that it finds\nderived features with the highest variance. Formally, PCA finds a matrix\n\\(\\Phi \\in\n\\mathbb{R}^{D\n\\times K}\\) and a set of vector \\(z_{i} \\in\n\\mathbb{R}^{K}\\) such that \\(x_{i}\n\\approx \\Phi z_{i}\\) for all \\(i\\). The\ncolumns of \\(\\Phi\\) are called principal\ncomponents, and they specify the structure of the derived linear\nfeatures. The vector \\(z_{i}\\) is called the score of \\(x_{i}\\)\nwith respect to these components. The top component explains the most\nvariance, the second captures the next most, and so on.\n\n\n\n\nFor example, if one of the columns of \\(\\Phi\\) was equal to \\(\\left(\\frac{1}{D},\n\\dots, \\frac{1}{D}\\right)\\), then that feature computes\nthe average of all coordinates (e.g., to get average brightness), and\nthe corresponding \\(z_{i}\\) would be a measure of the\naverage brightness of sample \\(i\\).\n\n\n\n\nGeometrically, the columns of \\(\\Phi\\) span a plane that\napproximates the data. The \\(z_{i}\\) provide coordinates of\npoints projected onto this plane.\n\n\n\n\n\n\nFigure 3: PCA finds a low-dimensional linear subspace that closely\napproximates the high-dimensional data.\n\n\n\n\nIn R, PCA can be conveniently implemented using the tidymodels package.\nWe will see a base R implementation in the next lecture. The\ndataset\nbelow contains properties of a variety of cocktails, from the Boston\nBartender’s guide. The first two columns are qualitative descriptors,\nwhile the rest give numerical ingredient information.\n\n\n\ncocktails_df <- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\ncocktails_df[, 1:6]\n\n# A tibble: 937 × 6\n   name       category light_rum lemon_juice lime_juice sweet_vermouth\n   <chr>      <chr>        <dbl>       <dbl>      <dbl>          <dbl>\n 1 Gauguin    Cocktai…      2           1          1               0  \n 2 Fort Laud… Cocktai…      1.5         0          0.25            0.5\n 3 Cuban Coc… Cocktai…      2           0          0.5             0  \n 4 Cool Carl… Cocktai…      0           0          0               0  \n 5 John Coll… Whiskies      0           1          0               0  \n 6 Cherry Rum Cocktai…      1.25        0          0               0  \n 7 Casa Blan… Cocktai…      2           0          1.5             0  \n 8 Caribbean… Cocktai…      0.5         0          0               0  \n 9 Amber Amo… Cordial…      0           0.25       0               0  \n10 The Joe L… Whiskies      0           0.5        0               0  \n# ℹ 927 more rows\n\n\nThe pca_rec object below defines a tidymodels recipe for\nperforming PCA. Computation of the lower-dimensional representation is\ndeferred until prep() is called. This delineation between\nworkflow definition and execution helps clarify the overall workflow,\nand it is typical of the tidymodels package.\n\n\n\npca_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors())\n\npca_prep <- prep(pca_rec)\n\n\n\n\nThe step_normalize call is used to center and scale all the\ncolumns. This is needed because otherwise columns with larger variance\nwill have more weight in the final dimensionality reduction, but this is\nnot conceptually meaningful. For example, if one of the columns in a\ndataset were measuring length in kilometers, then we could artificially\nincrease its influence in a PCA by expressing the same value in meters.\nTo achieve invariance to this change in units, it would be important to\nnormalize first.\n\n\n\n\nWe can tidy each element of the workflow object. Since PCA\nwas the second step in the workflow, the PCA components can be obtained\nby calling tidy with the argument “2.” The scores of each sample with\nrespect to these components can be extracted using juice.\nThe amount of variance explained by each dimension is also given by\ntidy, but with the argument type = “variance”.\nWe’ll see how to visualize and interpret these results in the next\nlecture.\n\n\n\n\ntidy(pca_prep, 2)\n\n# A tibble: 1,600 × 4\n   terms             value component id       \n   <chr>             <dbl> <chr>     <chr>    \n 1 light_rum        0.163  PC1       pca_cUzoa\n 2 lemon_juice     -0.0140 PC1       pca_cUzoa\n 3 lime_juice       0.224  PC1       pca_cUzoa\n 4 sweet_vermouth  -0.0661 PC1       pca_cUzoa\n 5 orange_juice     0.0308 PC1       pca_cUzoa\n 6 powdered_sugar  -0.476  PC1       pca_cUzoa\n 7 dark_rum         0.124  PC1       pca_cUzoa\n 8 cranberry_juice  0.0954 PC1       pca_cUzoa\n 9 pineapple_juice  0.119  PC1       pca_cUzoa\n10 bourbon_whiskey  0.0963 PC1       pca_cUzoa\n# ℹ 1,590 more rows\n\n\n\njuice(pca_prep)\n\n# A tibble: 937 × 7\n   name                 category    PC1     PC2     PC3     PC4    PC5\n   <chr>                <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 Gauguin              Cocktai…  1.38  -1.15    1.34   -1.12    1.52 \n 2 Fort Lauderdale      Cocktai…  0.684  0.548   0.0308 -0.370   1.41 \n 3 Cuban Cocktail No. 1 Cocktai…  0.285 -0.967   0.454  -0.931   2.02 \n 4 Cool Carlos          Cocktai…  2.19  -0.935  -1.21    2.47    1.80 \n 5 John Collins         Whiskies  1.28  -1.07    0.403  -1.09   -2.21 \n 6 Cherry Rum           Cocktai… -0.757 -0.460   0.909   0.0154 -0.748\n 7 Casa Blanca          Cocktai…  1.53  -0.392   3.29   -3.39    3.87 \n 8 Caribbean Champagne  Cocktai…  0.324  0.137  -0.134  -0.147   0.303\n 9 Amber Amour          Cordial…  1.31  -0.234  -1.55    0.839  -1.19 \n10 The Joe Lewis        Whiskies  0.138 -0.0401 -0.0365 -0.100  -0.531\n# ℹ 927 more rows\n\n\n\ntidy(pca_prep, 2, type = \"variance\")\n\n# A tibble: 160 × 4\n   terms    value component id       \n   <chr>    <dbl>     <int> <chr>    \n 1 variance  2.00         1 pca_cUzoa\n 2 variance  1.71         2 pca_cUzoa\n 3 variance  1.50         3 pca_cUzoa\n 4 variance  1.48         4 pca_cUzoa\n 5 variance  1.37         5 pca_cUzoa\n 6 variance  1.32         6 pca_cUzoa\n 7 variance  1.30         7 pca_cUzoa\n 8 variance  1.20         8 pca_cUzoa\n 9 variance  1.19         9 pca_cUzoa\n10 variance  1.18        10 pca_cUzoa\n# ℹ 150 more rows\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:55:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week10-3/",
    "title": "Principal Components Analysis II",
    "description": "Visualizing and interpreting PCA.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-23",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(tidymodels)\nlibrary(tidytext)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", linewidth = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nHow should we visualize the results from PCA? There are three artifacts\nproduced by the procedure worth considering — components, scores, and variances.\nThe components describe derived features, the scores lay samples out on a map,\nand the variances summarize how much information was preserved by each\ndimension.\n\n\n# produced by code in previous notes\ncomponents <- read_csv(\"https://uwmadison.box.com/shared/static/dituepd0751qqsims22v2liukuk0v4bf.csv\") %>%\n  filter(component %in% str_c(\"PC\", 1:5))\nscores <- read_csv(\"https://uwmadison.box.com/shared/static/qisbw1an4lo8naifoxyu4dqv4bfsotcu.csv\")\nvariances <- read_csv(\"https://uwmadison.box.com/shared/static/ye125xf8800zc5eh3rfeyzszagqkaswf.csv\") %>%\n  filter(terms == \"percent variance\")\n\n\nFirst, let’s see how much variance is explained by each dimension of the PCA.\nWithout detouring into the mathematical explanation, the main idea is that, the\nmore rapid the dropoff in variance explained, the better the low-dimensional\napproximation. For example, if the data actually lie on a 2D plane in a\nhigh-dimensional space, then the first two bars would contain all the variance\n(the rest would be zero).\n\n\nggplot(variances) +\n  geom_col(aes(component, value))\n\n\n\nFigure 1: Proportion of variance explained by each component in the PCA.\n\n\n\nWe can interpret components by looking at the linear coefficients of the\nvariables used to define them. From the plot below, we see that the first PC\nmostly captures variation related to whether the drink is made with powdered\nsugar or simple syrup. Drinks with high values of PC1 are usually to be made\nfrom simple syrup, those with low values of PC1 are usually made from powdered\nsugar. From the two largest bars in PC2, we can see that it highlights the\nvermouth vs. non-vermouth distinction.\n\n\nggplot(components, aes(value, terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  labs(y = NULL) +\n  theme(axis.text = element_text(size = 7))\n\n\n\nFigure 2: The top 5 principal components associated with the cocktails dataset.\n\n\n\nIt is often easier read the components when the bars are sorted according to\ntheir magnitude. The usual ggplot approach to reordering axes labels, using\neither reorder() or releveling the associated factor, will reorder all the\nfacets in the same way. If we want to reorder each facet on its own, we can use\nthe reorder_within function coupled with scale_*_reordered, both from the\ntidytext package.\n\n\ncomponents_ <- components %>%\n  filter(component %in% str_c(\"PC\", 1:3)) %>%\n  mutate(terms = reorder_within(terms, abs(value), component))\nggplot(components_, aes(value, terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(y = NULL) +\n  theme(axis.text = element_text(size = 7))\n\n\n\nFigure 3: The top 3 principal components, with defining variables sorted by the magnitude of their coefficient.\n\n\n\nNext, we can visualize the scores of each sample with respect to these\ncomponents. The plot below shows \\(\\left(z_{i1}, z_{i2}\\right)\\). Suppose that the\ncolumns of \\(\\Phi\\) are \\(\\varphi_{1}, \\dots, \\varphi_{K}\\). Then, since \\(x_{i}\n\\approx \\varphi_{1}z_{i1} + \\varphi_{2} z_{i2}\\), the samples have large values\nfor variables with large component values in the coordinate directions where\n\\(z_{i}\\) is farther along.\n\n\nggplot(scores, aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 1.5) +\n  geom_text_repel(check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) # rescale axes to reflect variance\n\n\n\nFigure 4: The scores associated with the cocktails dataset.\n\n\n\nFor example, El Nino has high value for PC1, which means it has a high value\nof variables that are positive for PC1 (like simple syrup) and low value for\nthose variables that are negative (like powdered sugar). Similarly, since\n\\(\\varphi_{2}\\) puts high positive weight on vermouth-related variables, so H. P.\nW. Cocktail has many vermouth-related ingredients.\nIn practice, it will often be important to visualize several pairs of PC\ndimensions against one another, not just the top 2.\nLet’s examine the original code in a little more detail. We are using\ntidymodels, which is a package for decoupling the definition and execution of a\ndata pipeline. This compartmentalization makes it easier to design and reuse\nacross settings.\n\n\ncocktails_df <- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\npca_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors())\n\npca_prep <- prep(pca_rec)\n\n\nHere is how you would apply PCA without the tidymodels package. You have to\nfirst split the data into the “metadata” that is used to interpret the scores\nand the numerical variables used as input to PCA. Then at the end, you have to\njoin the metadata back in. It’s not impossible, but the code is not as readable.\n\n\n# split name and category out of the data frame\npca_result <- cocktails_df %>%\n  select(-name, -category) %>%\n  scale() %>%\n  princomp()\n\n# join them back into the PCA result\nmetadata <- cocktails_df %>%\n  select(name, category)\nscores_direct <- cbind(metadata, pca_result$scores)\n\nggplot(scores_direct, aes(Comp.1, Comp.2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 1.5) +\n  geom_text_repel(check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) # rescale axes to reflect variance\n\n\n\nFigure 5: A plot of the PCA scores made without using tidymodels.\n\n\n\nThe equivalent tidymodels implementation handles the difference between\nsupplementary and modeling data less bluntly, setting the name and category\nvariables to id roles, so that all_predictors() knows to skip them.\nWe conclude with some characteristics of PCA, which can guide the choice\nbetween alternative dimensionality reduction methods.\nGlobal structure: Since PCA is looking for high-variance overall, it tends\nto focus on global structure.\nLinear: PCA can only consider linear combinations of the original\nfeatures. If we expect nonlinear features to be more meaningful, then another\napproach should be considered.\nInterpretable features: The PCA components exactly specify how to\nconstruct each of the derived features.\nFast: Compared to most dimensionality reduction methods, PCA is quite\nfast. Further, it is easy to implement approximate versions of PCA that scale\nto very large datasets.\nDeterministic: Some embedding algorithms perform an optimization process,\nwhich means there might be some variation in the results due to randomness in\nthe optimization. In contrast, PCA is deterministic, with the components being\nunique up to sign (i.e., you could reflect the components across an axis, but\nthat is the most the results might change).\n\n\n\n",
    "preview": "posts/2024-12-27-week10-3/week10-3_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2025-08-19T16:55:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week10-4/",
    "title": "Uniform Manifold Approximation and Projection",
    "description": "An overview of the UMAP algorithm.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-22",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(embed)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\n\nNonlinear dimension reduction methods can give a more faithful\nrepresentation than PCA when the data don’t lie on a low-dimensional\nlinear subspace.\n\n\n\n\nFor example, suppose the data were shaped like this. There is no\none-dimensional line through these data that separate the groups well.\nWe will need an alternative approach to reducing dimensionality if we\nwant to preserve nonlinear structure.\n\n\n\n\nmoons <- read_csv(\"https://uwmadison.box.com/shared/static/kdt9qqvonhcz2ssb599p1nqganrg1w6k.csv\")\nggplot(moons, aes(X, Y, col = Class)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nFigure 1: An example nonlinear dataset where projections onto any\nstraight line will necessarily cause the classes to bleed together.\n\n\n\n\n\nFrom a high-level, the intuition behind UMAP is to (a) build a graph\njoining nearby neighbors in the original high-dimensional space, and\nthen (b) layout the graph in a lower-dimensional space.\n\n\n\n\nFor example, consider the 2-dimensional sine wave below. If we build a\ngraph, we can try to layout the resulting nodes and edges on a\n1-dimensional line in a way that approximately preserves the ordering.\n\n\n\n\n\n\nFigure 2: UMAP (and many other nonlinear methods) begins by constructing\na graph in the high-dimensional space, whose layout in the lower\ndimensional space will ideally preserve the essential relationships\nbetween samples.\n\n\n\n\nA natural way to build a graph is to join each node to its \\(K\\)\nclosest neighbors. The choice of \\(K\\) will influence the final\nreduction, and it is treated as a hyperparameter of UMAP.\n\n\n\n\n\nFigure 3: When using fewer nearest neighbors, the final dimensionality\nreduction will place more emphasis on effectively preserving the\nrelationships between points in local neighborhoods.\n\n\n\n\nLarger values of \\(K\\) prioritize preservation of\nglobal structure, while smaller \\(K\\) will better reflect local\ndifferences. This property is not obvious a priori, but is suggested by\nthe simulations described in the reading.\n\n\n\n\n\nFigure 4: When using larger neighborhoods, UMAP will place more emphasis\non preserving global structure, sometimes at the cost of local\nrelationships between points.\n\n\n\n\n\nOne detail in the graph construction: In UMAP, the edges are assigned\nweights depending on the distance they span, normalized by the distance\nto the closest neighbor. Neighbors that are close, relative to the\nnearest neighbors, are assigned higher weights than those that are far\naway, and points that are linked by high weight edges are pulled\ntogether with larger force in the final graph layout. This is what the\nauthors mean by using a “fuzzy” nearest neighbor graph. The fuzziness\nallows the algorithm to distinguish neighbors that are very close from\nthose that are far, even though they all lie within a \\(K\\)-nearest-neighborhood.\n\n\n\n\nOnce the graph is constructed, there is the question of how the graph\nlayout should proceed. UMAP uses a variant of force-directed layout, and\nthe global strength of the springs is another hyperparameter. Lower\ntension on the springs allow the points to spread out more loosely,\nhigher tension forces points closer together. This is a second\nhyperparameter of UMAP.\n\n\n\n\n\n\n\n\nThese two hyperparameters — the number of nearest neighbors \\(K\\) and\nthe layout tension — are the only two hyperparameters of UMAP.\n\n\n\n\nYou can see more examples of what this algorithm does to toy datasets in\nthe\nreading.\nNote in particular the properties that the algorithm does not\npreserve. The distance between clusters should not be interpreted, since\nit just means that the graph components were not connected. Similarly,\nthe density of points is not preserved.\n\n\n\n\nIn R, we can implement this using almost the same code as we used for\nPCA. The step_umap command is available through the embed\npackage.\n\n\n\n\ncocktails_df <- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\numap_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_umap(all_predictors(), neighbors = 20, min_dist = 0.1)\numap_prep <- prep(umap_rec)\n\n\n\nUMAP returns a low-dimensional atlas relating the points, but it does\nnot provide any notion of derived features.\n\n\n\nggplot(juice(umap_prep), aes(UMAP1, UMAP2)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 0.8) +\n  geom_text(aes(label = name), check_overlap = TRUE, size = 3, hjust = \"inward\")\n\n\n\n\nFigure 5: The learned UMAP representation of the cocktails dataset.\n\n\n\n\nWe can summarize the properties of UMAP,\n\n\nGlobal or local structure: The number of nearest neighbors\n\\(K\\)\nused during graph construction can be used modulate the emphasis of\nglobal vs. local structure.\n\n\nNonlinear: UMAP can reflect nonlinear structure in\nhigh-dimensions.\n\n\nNo interpretable features: UMAP only returns the map between\npoints, and there is no analog of components to describe how the\noriginal features were used to construct the map.\n\n\nSlower: While UMAP is much faster than comparable nonlinear\ndimensionality reduction algorithms, it is still slower than linear\napproaches.\n\n\nNondeterministic: The output from UMAP can change from run to\nrun, due to randomness in the graph layout step. If exact\nreproducibility is required, a random seed should be set.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T16:55:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week10-5/",
    "title": "PCA and UMAP Examples",
    "description": "More examples of dimensionality reduction using PCA and UMAP.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-21",
    "categories": [],
    "contents": "\nReading 1 and 2, Recording, Rmarkdown\n\n\nlibrary(embed)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(tidymodels)\nlibrary(tidytext)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nset.seed(479)\n\n\nThese notes don’t introduce any new conceptual material. Instead, they give a\nfew examples of how PCA and UMAP can be used.\nFrom two dimensions to one\nWe presented the two moons dataset earlier as an example where UMAP, but not\nPCA, would be able to discover a one-dimensional representation that separates\nthe groups. The implication is that, if we anticipate some sort of nonlinearity\nin higher-dimensions (which we can’t directly visualize), then UMAP would be a\nmore suitable choice.\n\n\nmoons <- read_csv(\"https://uwmadison.box.com/shared/static/kdt9qqvonhcz2ssb599p1nqganrg1w6k.csv\")\nggplot(moons, aes(X, Y, col = Class)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\nFigure 1: The original two moons dataset. We will ask both PCA and UMAP to recover a 1D reduction of these 2D data.\n\n\n\nThe code block below defines both the PCA and UMAP recipes. There is no need\nto normalize the data, since the two dimensions are already on the same scale.\n\n\nmoons_ <- recipe(~ ., data = moons) %>%\n  update_role(Class, new_role = \"id\")\n\npca_rec <- step_pca(moons_, all_predictors(), num_comp = 1)\numap_rec <- step_umap(moons_, all_predictors(), num_comp = 1)\n\n\nThe block below shows both the UMAP and PCA representations. The PCA\nrepresentation seems to mostly reflect the variation on the \\(x\\)-axis of the\noriginal data, and the two classes mix together. On the other hand, the UMAP\nclearly separates the groups. This is expected, since the nearest neighborhood\ngraph that defines UMAP is likely separated into two major components, one for\neach moon.\n\n\nscores <- bind_cols(\n  prep(umap_rec) %>% juice() %>% mutate(UMAP1 = scale(UMAP1)),\n  prep(pca_rec) %>% juice() %>% select(-Class)\n) %>%\n  pivot_longer(-Class, names_to = \"method\")\n\nggplot(scores, aes(value, method, col = Class)) +\n  geom_point(position = position_jitter(h = 0.1), alpha = 0.8) +\n  scale_color_brewer(palette = \"Set2\") \n\n\n\nFigure 2: 1D PCA and UMAP representations of the 2D two moons dataset.\n\n\n\nAntibiotics Dataset\nWe can apply dimensionality reduction to the antibiotics dataset described in\nlecture 2 - 1.\nThere, we had filtered down to the 6 most abundant bacteria. Now we will\nconsider 147 most abundant, which means that each sample can be imagined as a\nvector in a 147-dimensional space. Ideally, a dimensionality-reduction procedure\nshould be able to place samples close to one another when they have similar\nspecies profiles. In addition to loading species counts, we load taxonomic\ninformation about each species, in the taxa variable.\n\n\nantibiotic <- read_csv(\"https://uwmadison.box.com/shared/static/t1lifegdz8s0a8lgckber32ytyh9hu4r.csv\")\ntaxa <- read_csv(\"https://uwmadison.box.com/shared/static/ng6y6etk79lrm0gtsgw2u0yq6gqcozze.csv\")\n\n\nWe now define a PCA recipe. Since the counts are relatively skewed, we\nlog-transform1, using step_log. The rest of the definition is like\nthe 2D example above.\n\n\nantibiotic_ <- recipe(~ ., data = antibiotic) %>%\n  update_role(sample:antibiotic, new_role = \"id\") %>%\n  step_log(all_predictors(), offset = 1) %>%\n  step_normalize(all_predictors())\n\npca_rec <- step_pca(antibiotic_, all_predictors())\npca_prep <- prep(pca_rec)\n\n\nWe generate a map of the PCA scores below. The primary difference is between\nthe three study participants – D, E, and F. Within each person, there is some\nvariation between the antibiotic periods, as indicated by the points’ colors.\n\n\nscores <- juice(pca_prep) \nvariances <- tidy(pca_prep, 2, type = \"variance\")\nggplot(scores, aes(PC1, PC2, col = antibiotic)) +\n  geom_point(aes(shape = ind), size = 1.5) +\n  geom_text_repel(aes(label = sample), check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) + \n  scale_color_brewer(palette = \"Set2\")\n\n\n\nFigure 3: PCA scores for the antibiotics dataset. The main difference is between study participants, with some secondary variation related to whether the participant was taking the antibiotic at that timepoint.\n\n\n\nWhich species contribute the most to the principal components? We can analyze\nthis like we did with the cocktails dataset. In addition to plotting the raw\ncomponent value, we also join in the taxa information. This allows us to color\nin each bar by the species group that each bacteria belongs to. For example, we\nsee that samples on the right side of the plot above (i.e., high PC1) likely\nhave more Firmicutes than Bacteroidetes. PC2 seems to pick up on two species\nthat have higher abundance when the rest drop-off.\n\n\ncomponents_ <- tidy(pca_prep, 3) %>%\n  filter(component %in% str_c(\"PC\", 1:6)) %>%\n  mutate(terms_ = reorder_within(terms, abs(value), component)) %>%\n  group_by(component) %>%\n  top_n(20, abs(value)) %>%\n  left_join(taxa)\n\nggplot(components_, aes(value, terms_, fill = Phylum)) +\n  geom_col() +\n  facet_wrap(~ component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(y = NULL) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(axis.text = element_text(size = 5))\n\n\n\nFigure 4: The first six principal components associated with the antibiotics dataset.\n\n\n\nWe can use similar code to compute a UMAP embedding. The UMAP seems to\nseparate the different timepoints more clearly. However, there is no analog of\ncomponents with which to interpret the different axes. Instead, a typical\napproach to interpret the representation is to find points that are close\ntogether (e.g., using \\(K\\)-means) and take their average species profile.\n\n\numap_rec <- step_umap(antibiotic_, all_predictors(), min_dist = 1.5)\numap_prep <- prep(umap_rec)\n\nscores <- juice(umap_prep) \nggplot(scores, aes(UMAP1, UMAP2, col = antibiotic)) +\n  geom_point(aes(shape = ind), size = 1.5) +\n  geom_text_repel(aes(label = sample), max.overlaps = 10) +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\nFigure 5: The UMAP representation associated with the antibiotics dataset.\n\n\n\nImage Data\nBoth PCA and UMAP can be used on image data. Here, each pixel in an image is\nconsidered a different feature. For example, the FashionMNIST dataset includes\n60,000 28 x 28 images of fashion objects. We can think of image as a\nvector2 \\(x_{i} \\in \\mathbb{R}^{784}\\). The goal of dimensionality\nreduction in this context is to build an atlas of images, where images with\nsimilar overall pixel values should be located next to one another. First, we\nread in the data and subsample it, so the code doesn’t take so long to run.\n\n\nfashion <- read_csv(\"https://uwmadison.box.com/shared/static/aur84ttkwa2rqvzo99qo7yhxemoc6om0.csv\") %>%\n  sample_frac(0.2) %>%\n  mutate(\n    image = row_number(),\n    label = as.factor(label)\n  )\n\n\nEach row of the matrix above is a separate image. We can prepare a PCA\nrecipe just like in the two examples above. Note that we are not normalizing the\nfeatures – the pixels are already on a common scale.\n\n\nfashion_ <- recipe(~ ., data = fashion) %>%\n  update_role(label, image, new_role = \"id\")\n\npca_rec <- step_pca(fashion_, all_predictors())\npca_prep <- prep(pca_rec)\n\n\n\n\n\nThe code below extracts the PCA scores and visualizes them as a cloud of\npoints. Each point corresponds to an image, and the different types of fashion\nitems are indicated by color. It seems that the types are well separated, but\nthe labels are not informative… to understand what the colors mean, we need to\nlook at the images.\n\n\nscores <- juice(pca_prep) %>%\n  rename(x = PC1, y = PC2)\nggplot(scores, aes(x, y, col = label)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set3\") +\n  coord_fixed()\n\n\n\nFigure 6: Principal component scores from the fashion dataset.\n\n\n\nThe block below overlays the first 300 images from the dataset at the\nlocations from the previous plot. We have prepared a function to generate this\nsort of image using ggplot2; however, we have hidden it to avoid cluttering\nthese notes. You can view the function in the rmarkdown link at the top of this\ndocument.\n\n\npivot_scores(scores, fashion) %>%\n  overlay_images()\n\n\n\nFigure 7: A subset of principal component scores expressed as the corresponding images.\n\n\n\nFinally, we can repeat the exercise above with UMAP. The first plot shows\nthe UMAP scores and the second overlays the same set of 300 images onto these\nnew coordinates. It seems that UMAP can more clearly separate shoes and pants\nfrom shirts and sweaters.\n\n\numap_rec <- step_umap(fashion_, all_predictors(), num_comp = 2, min_dist = 0.5)\numap_prep <- prep(umap_rec)\nscores <- juice(umap_prep) %>%\n  rename(x = UMAP1, y = UMAP2)\nggplot(scores, aes(x, y, col = label)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set3\") +\n  coord_fixed()\n\n\n\nFigure 8: The locations of all the images according to UMAP. Each color is a different class.\n\n\n\n\n\npivot_scores(scores, fashion, scale_factor = 0.05) %>%\n  overlay_images(scale_factor = 0.05)\n\n\n\nFigure 9: A sample of the images at the locations determined by UMAP.\n\n\n\n\nSpecifically, we use a \\(\\log\\left(1 + x\\right)\\) transform, since\nthere are many 0 counts.↩︎\n28 * 28 = 784↩︎\n",
    "preview": "posts/2024-12-27-week10-5/week10-5_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2025-08-19T16:56:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week11-1/",
    "title": "Introduction to Topic Models",
    "description": "An overview of dimensionality reduction via topics.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-20",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nTopic modeling is a type of dimensionality reduction method that is\nespecially useful for high-dimensional count matrices. For example, it\ncan be applied to,\n\n\nText data analysis, where each row is a document and each column is a\nword. The \\(ij^{th}\\) entry contains the count\nof the \\(j^{th}\\) word in the \\(i^{th}\\)\ndocument.\n\n\nGene expression analysis, where each row is a biological sample and each\ncolumn is a gene. The \\(ij^{th}\\) entry measures the amount\nof gene \\(j\\) expressed in sample \\(i\\).\n\n\n\n\nFor clarity, we will refer to samples as documents and features as\nwords. However, keep in mind that these methods can be used more\ngenerally – we will see a biological application three lectures from\nnow.\n\n\n\n\nThese models are useful to know about because they provide a compromise\nbetween clustering and PCA.\n\n\nIn clustering, each document would have to be assigned to a single\ntopic. In contrast, topic models allow each document to partially belong\nto several topics simultaneously. In this sense, they are more suitable\nwhen data do not belong to distinct, clearly-defined clusters.\n\n\nPCA is also appropriate when the data vary continuously, but it does not\nprovide any notion of clusters. In contrast, topic models estimate \\(K\\)\ntopics, which are analogous to a cluster centroids (though documents are\ntypically a mix of several centroids).\n\n\n\n\nWithout going into mathematical detail, topic models perform\ndimensionality reduction by supposing,\n\n\nEach document is a mixture of topics.\n\n\nEach topic is a mixture of words.\n\n\n\n\n\n\nFigure 1: An overview of the topic modeling process. Topics are\ndistributions over words, and the word counts of new documents are\ndetermined by their degree of membership over a set of underlying\ntopics. In an ordinary clustering model, the bars for the memberships\nwould have to be either pure purple or orange. Here, each document is a\nmixture.\n\n\n\n\n\nTo illustrate the first point, consider modeling a collection of\nnewspaper articles. A set of articles might belong primarily to the\n“politics” topic, and others to the “business” topic. Articles that\ndescribe a monetary policy in the federal reserve might belong partially\nto both the “politics” and the “business” topic.\n\n\n\n\nFor the second point, consider the difference in words that would appear\nin politics and business articles. Articles about politics might\nfrequently include words like “congress” and “law,” but only rarely\nwords like “stock” and “trade.”\n\n\n\n\nGeometrically, LDA can be represented by the following picture. The\ncorners of the\nsimplex1\nrepresent different words (in reality, there would be \\(V\\)\ndifferent corners to this simplex, one for each word). A topic is a\npoint on this simplex. The closer the topic is to one of the corners,\nthe more frequently that word appears in the topic.\n\n\n\n\n\n\nFigure 2: A geometric interpretation of LDA, from the original paper by\nBlei, Ng, and Jordan.\n\n\n\n\nA document is a mixture of topics, with more words coming from the\ntopics that it is close to. More precisely, a document that is very\nclose to a particular topic has a word distribution just like that\ntopic. A document that is intermediate between two topics has a word\ndistribution that mixes between both topics. Note that this is different\nfrom a clustering model, where all documents would lie at exactly one of\nthe corners of the topic simplex. Finally, note that the topics form\ntheir own simplex, since each document can be described as a mixture of\ntopics, with mixture weights summing up to 1.\n\n\n\n\n\n\nA simplex is the geometric object describing the set of probability\nvectors over \\(V\\) elements. For example, if \\(V = 3\\),\nthen \\(\\left(0.1,\n0, 0.9\\right)\\) and \\(\\left(0.2, 0.3, 0.5\\right)\\) belong\nto the simplex, but not \\(\\left(0.3, 0.1, 9\\right)\\), since it\nsums to a number larger than\n1.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T17:05:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week11-2/",
    "title": "Fitting Topic Models",
    "description": "Data preparation and model fitting code for topics.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-19",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gutenbergr\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\nlibrary(\"tidytext\")\nlibrary(\"topicmodels\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\n\nThere are several packages in R that can be used to fit topic models. We\nwill use LDA as implemented in the topicmodels package,\nwhich expects input to be structured as a\nDocumentTermMatrix, a special type of matrix that stores\nthe counts of words (columns) across documents (rows). In practice, most\nof the effort required to fit a topic model goes into transforming the\nraw data into a suitable DocumentTermMatrix.\n\n\n\n\nTo illustrate this process, let’s consider the “Great Library Heist”\nexample from the reading. We imagine that a thief has taken four books —\nGreat Expectations, Twenty Thousand Leagues Under The Sea, War of the\nWorlds, and Pride & Prejudice — and torn all the chapters out. We\nare left with pieces of isolated pieces of text and have to determine\nfrom which book they are from. The block below downloads all the books\ninto an R object.\n\n\n\n\ntitles <- c(\"Twenty Thousand Leagues under the Sea\", \n            \"The War of the Worlds\",\n            \"Pride and Prejudice\", \n            \"Great Expectations\")\nbooks <- gutenberg_works(title %in% titles) %>%\n  gutenberg_download(meta_fields = \"title\")\nbooks\n\n# A tibble: 53,718 × 3\n   gutenberg_id text                    title                \n          <int> <chr>                   <chr>                \n 1           36 \"cover \"                The War of the Worlds\n 2           36 \"\"                      The War of the Worlds\n 3           36 \"\"                      The War of the Worlds\n 4           36 \"\"                      The War of the Worlds\n 5           36 \"\"                      The War of the Worlds\n 6           36 \"The War of the Worlds\" The War of the Worlds\n 7           36 \"\"                      The War of the Worlds\n 8           36 \"by H. G. Wells\"        The War of the Worlds\n 9           36 \"\"                      The War of the Worlds\n10           36 \"\"                      The War of the Worlds\n# ℹ 53,708 more rows\n\n\n\nSince we imagine that the word distributions are not equal across the\nbooks, topic modeling is a reasonable approach for discovering the books\nassociated with each chapter. Note that, in principle, other clustering\nand dimensionality reduction procedures could also work.\n\n\n\n\nFirst, let’s simulate the process of tearing the chapters out. We split\nthe raw texts anytime the word “Chapter” appears. We will keep track of\nthe book names for each chapter, but this information is not passed into\nthe topic modeling algorithm.\n\n\n\n\nby_chapter <- books %>%\n  group_by(title) %>%\n  mutate(\n    chapter = cumsum(str_detect(text, regex(\"chapter\", ignore_case = TRUE)))\n  ) %>%\n  group_by(title, chapter) %>%\n  mutate(n = n()) %>%\n  filter(n > 5) %>%\n  ungroup() %>%\n  unite(document, title, chapter)\n\n\n\nAs it is, the text data are long character strings, giving actual text\nfrom the novels. To fit LDA, we only need counts of each word within\neach chapter – the algorithm throws away information related to word\norder. To derive word counts, we first split the raw text into separate\nwords using the unest_tokens function in the tidytext\npackage. Then, we can count the number of times each word appeared in\neach document using count, a shortcut for the usual\ngroup_by and summarize(n = n()) pattern.\n\n\n\nword_counts <- by_chapter %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words) %>%\n  count(document, word) # shortcut for group_by(document, word) %>% summarise(n = n())\n\nword_counts\n\n# A tibble: 101,271 × 3\n   document               word             n\n   <chr>                  <chr>        <int>\n 1 Great Expectations_0   1867             1\n 2 Great Expectations_0   charles          1\n 3 Great Expectations_0   contents         1\n 4 Great Expectations_0   dickens          1\n 5 Great Expectations_0   edition          1\n 6 Great Expectations_0   expectations     1\n 7 Great Expectations_0   illustration     1\n 8 Great Expectations_100 age              1\n 9 Great Expectations_100 arose            1\n10 Great Expectations_100 barnard’s        1\n# ℹ 101,261 more rows\n\n\nThese words counts are still not in a format compatible with conversion\nto a DocumentTermMatrix. The issue is that the\nDocumentTermMatrix expects words to be arranged along\ncolumns, but currently they are stored across rows. The line below\nconverts the original “long” word counts into a “wide”\nDocumentTermMatrix in one step. Across these 4 books, we\nhave 65 chapters and a vocabulary of size 18325.\n\n\n\nchapters_dtm <- word_counts %>%\n  cast_dtm(document, word, n)\nchapters_dtm\n\n<<DocumentTermMatrix (documents: 195, terms: 18740)>>\nNon-/sparse entries: 101271/3553029\nSparsity           : 97%\nMaximal term length: 19\nWeighting          : term frequency (tf)\n\n\nOnce the data are in this format, we can use the LDA\nfunction to fit a topic model. We choose \\(K = 4\\) topics because we expect\nthat each topic will match a book. Different hyperparameters can be set\nusing the control argument.\n\n\n\nchapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))\nchapters_lda\n\nA LDA_VEM topic model with 4 topics.\n\n\nThere are two types of outputs produced by the LDA model: the topic word\ndistributions (for each topic, which words are common?) and the\ndocument-topic memberships (from which topics does a document come\nfrom?). For visualization, it will be easiest to extract these\nparameters using the tidy function, specifying whether we\nwant the topics (beta) or memberships (gamma).\n\n\n\ntopics <- tidy(chapters_lda, matrix = \"beta\")\nmemberships <- tidy(chapters_lda, matrix = \"gamma\")\n\n\n\nThis tidy approach is preferable to extracting the parameters directly\nfrom the fitted model (e.g., using chapters_lda@gamma) because it ensures the\noutput is a tidy data.frame, rather than a matrix. Tidy data.frames are\neasier to visualize using ggplot2.\n\n\n\n# highest weight words per topic\ntopics %>%\n  arrange(topic, -beta)\n\n# A tibble: 74,960 × 3\n   topic term          beta\n   <int> <chr>        <dbl>\n 1     1 captain    0.00990\n 2     1 _nautilus_ 0.00817\n 3     1 sea        0.00587\n 4     1 nemo       0.00562\n 5     1 ned        0.00514\n 6     1 water      0.00504\n 7     1 conseil    0.00440\n 8     1 time       0.00418\n 9     1 land       0.00389\n10     1 day        0.00343\n# ℹ 74,950 more rows\n\n# topic memberships per document\nmemberships %>%\n  arrange(document, topic)\n\n# A tibble: 780 × 3\n   document               topic     gamma\n   <chr>                  <int>     <dbl>\n 1 Great Expectations_0       1 0.515    \n 2 Great Expectations_0       2 0.479    \n 3 Great Expectations_0       3 0.00316  \n 4 Great Expectations_0       4 0.00316  \n 5 Great Expectations_100     1 0.000476 \n 6 Great Expectations_100     2 0.000476 \n 7 Great Expectations_100     3 0.999    \n 8 Great Expectations_100     4 0.000476 \n 9 Great Expectations_101     1 0.0000158\n10 Great Expectations_101     2 0.0000158\n# ℹ 770 more rows\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T17:05:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week11-3/",
    "title": "Visualizing Topic Models",
    "description": "Once we've fit a topic model, how should we inspect it?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-18",
    "categories": [],
    "contents": "\nReading 1 and 2, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(superheat)\nlibrary(tidytext)\nlibrary(topicmodels)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nIn the last set of notes, we fit a topic model to the “Great Library Heist”\ndataset, but we did not visualize or interpret the results. We’ll work on that\nhere. The code below reads in the tidy topic and membership data.frames from\nbefore.\n\n\nmemberships <- read_csv(\"https://uwmadison.box.com/shared/static/c5k5iinwo9au44fb3lc00vq6isbi72c5.csv\")\ntopics <- read_csv(\"https://uwmadison.box.com/shared/static/uh34hhc1wnp072zcryisvgr3z0yh25ad.csv\")\n\n\nVisualizing Topics\nA topic is a probability distribution across a collection of words. If the\nvocabulary isn’t too large, two appropriate visualization strategies are,\nFaceted barplot: Each facet corresponds to a topic. The height of each bar\ncorresponds to a given word’s probability within the topic. The sum of heights\nacross all bars is 1.\nHeatmap: Each row is a topic and each column is a word. The color of the\nheatmap cells gives the probability of the word within the given topic.\n\nWe can construct a faceted barplot using the tidied beta matrix. We’ve\nfiltered to only words with a probability of at least \\(0.0003\\) in at least one\ntopic, but there are still more words than we could begin to inspect.\nNonetheless, it seems that there are words that have relatively high probability\nin one topic, but not others.\n\n\nggplot(topics %>% filter(beta > 3e-4), aes(term, beta)) +\n  geom_col() +\n  facet_grid(topic ~ .) +\n  theme(axis.text.x = element_blank())\n\n\n\nFigure 1: A faceted barplot view of the original topic distributions, with only very limited filtering.\n\n\n\nFor the heatmap, we need to pivot the topics, so that words appear along\ncolumns. From there, we can use superheatmap. The advantage of the heatmap is\nthat it takes up less space, and while it obscures comparisons between word\nprobabilities1 the main\ndifferences of interest are between low and high probability words.\n\n\ntopics %>%\n  filter(beta > 3e-4) %>%\n  pivot_wider(names_from = \"term\", values_from = \"beta\", values_fill = 0) %>%\n  select(-1) %>%\n  superheat(\n    pretty.order.cols = TRUE,\n    legend = FALSE\n  )\n\n\n\nFigure 2: An equivalent heatmap view of the above faceted barplot.\n\n\n\nNeither approach is very satisfactory since there are too many words for us\nto effectively label. A workaround is to restrict attention to a subset of\n“interesting” words. For example, we could filter to,\nTop words overall: We can consider only words whose probabilities are\nabove some threshold. This is the approach used in the visualizations above,\nthough the threshold is very low (there are still too many words to add\nlabels).\nTop words per topic: We can sort the words within each topic in order from\nhighest to lowest probability, and then keep only the \\(S\\) largest.\nMost discriminative words: Some words have high probability just because\nthey are common. They have high probability within each topic but aren’t\nactually interesting as far as characterizing the topics is concerned.\nInstead, we can focus on words that are common in some topics but rare in\nothers.\n\nWe can obtain the most probable words using the slice_max function, after\nfirst grouping by topic. Then, we use the same reorder_within function from\nthe PCA lectures to reorder words within each topic. The resulting plot is much\nmore interpretable. Judging from the words that are common in each topic’s\ndistribution, we can guess that the topics approximately correspond to: 1 ->\nGreat Expectations, 2 -> 20,000 Leagues Under the Sea, 3 -> Pride & Prejudice, 4\n-> War of the Worlds.\n\n\ntop_terms <- topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>%\n  mutate(term = reorder_within(term, beta, topic))\n\nggplot(top_terms, aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_reordered()\n\n\n\nFigure 3: The top words associated with the four fitted topics from the Great Library Heist example.\n\n\n\nTo visualize discriminative words, we first compute a discrimination measure\nfor each word and filter to those with the top score. The filtered results can\nbe used in either faceted barplots or heatmaps. Specifically, to find the words\nthat discriminate between topics \\(k\\) and \\(l\\), we compute\n\\[\\begin{align*}\nD\\left(k, l\\right) := \\beta_{kw}\\log\\left(\\frac{\\beta_{kw}}{\\beta_{lw}}\\right) + \\left(\\beta_{lw} - \\beta_{kw}\\right)\n\\end{align*}\\]\nfor each word \\(w\\). By maximizing over all pairs \\(k, l\\), we can determine whether\nthe word is discriminative between any pair of topics. This might seem like a\nmysterious formula, but it is just a function that is large when topic \\(k\\) has\nmuch larger probability than topic \\(l\\) (see the figure).\n\n\n\n\n\np <- seq(0.01, .99, length.out = 50)\ndf <- expand.grid(p, p) %>%\n  mutate(D = kl_div(Var1, Var2))\n\nggplot(df, aes(Var2, Var1)) +\n  geom_tile(aes(col = D, fill = D)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed() +\n  scale_color_distiller(direction = 1) +\n  scale_fill_distiller(direction = 1) +\n  labs(\n    y = expression(beta[kw]),\n    x = expression(beta[lw])\n  )\n\n\n\nFigure 4: An illustration of the formula used for computing a word’s discrimination between topics. The value of D is large when topic k has much larger probability than topic l.\n\n\n\nAn example heatmap of discriminative words is shown below. This backs up our\ninterpretation from the figure above. It also has the advantage that it removes\ncommon words (e.g., hand, people, and time appeared in the plot above) and\nhighlights rarer words that are specific to individual topics (e.g., names of\ncharacters that appear in only one of the books).\n\n\ndiscriminative_terms <- topics %>%\n  group_by(term) %>%\n  mutate(D = discrepancy(beta)) %>%\n  ungroup() %>%\n  slice_max(D, n = 200) %>%\n  mutate(term = fct_reorder(term, -D))\n\ndiscriminative_terms %>%\n  pivot_wider(names_from = \"topic\", values_from = \"beta\") %>%\n  column_to_rownames(\"term\") %>%\n  select(-D) %>%\n  superheat(\n    pretty.order.rows = TRUE,\n    left.label.size = 1.5,\n    left.label.text.size = 3,\n    bottom.label.size = 0.05,\n    legend = FALSE\n  )\n\n\n\nFigure 5: A heatmap of the terms that are most discriminative across the four topics.\n\n\n\nVisualizing Memberships\nBesides the topics, it is useful to study the topic proportions for each\nchapter. One compact approach is to use a boxplot. The result below suggest that\neach chapter is very definitely assigned to one of the four topics, except for\nchapters from Great Expectations. Therefore, while the model had the flexibility\nto learn more complex mixtures, it decided that a clustering structure made the\nmost sense for Pride & Prejudice, War of the Worlds, and 20,000 Leagues Under\nthe Sea.\n\n\nmemberships <- memberships %>%\n  mutate(\n    book = str_extract(document, \"[^_]+\"),\n    topic = factor(topic)\n  )\n\nggplot(memberships, aes(topic, gamma)) +\n  geom_boxplot() +\n  facet_wrap(~book)\n\n\n\nFigure 6: A boxplot of the document memberships. It seems that most documents are definitively assigned to one of the four topics.\n\n\n\nThe boxplot considers the collection of documents in aggregate. If we want\nto avoid aggregation and visualize individual documents, we can use a heatmap or\njittered scatterplot. These approaches are useful because heatmap cells and\nindividual points can be drawn relatively small — anything requiring more space\nwould become unwieldy as the number of documents grows. For example, the plot\nbelow shows that chapter 119 of Great Expectations has unusually high membership\nin Topic 2 and low membership in topic 3.\n\n\nggplot(memberships, aes(topic, gamma, col = book)) +\n  geom_point(position = position_jitter(h = 0.05, w = 0.3)) +\n  geom_text_repel(aes(label = document), size = 3) +\n  facet_wrap(~ book) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\nFigure 7: A jittered scatterplot of the topic memberships associated with each document.\n\n\n\nAlternatively, we can use a “structure” plot. This is a type of stacked\nbarplot where the colors of each bar corresponds to a topic. We’ve sorted the\ndocuments using the result of a hierarchical clustering on their proportion\nvectors – this is like how superheatmap orders rows using a dendrogram when\nusing pretty.order.rows. The takeaways here are similar to those in the\nscatterplot above.\n\n\ngamma <- memberships %>%\n  pivot_wider(names_from = topic, values_from = gamma)\n\n\n\n\nhclust_result <- hclust(dist(gamma[, 3:6]))\ndocument_order <- gamma$document[hclust_result$order]\nmemberships <- memberships %>%\n  mutate(document = factor(document, levels = document_order))\n\nggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +\n  geom_col(position = position_stack()) +\n  facet_grid(book ~ ., scales = \"free\", space = \"free\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(axis.text.y = element_blank())\n\n\n\nFigure 8: A structure plot view of each chapter’s topic memberships.\n\n\n\n\nColor is in general harder to compare than bar height.↩︎\n",
    "preview": "posts/2024-12-27-week11-3/week11-3_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2025-08-19T17:06:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week11-4/",
    "title": "Topic Modeling Case Study",
    "description": "An application to a gene expression dataset.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-17",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(superheat)\nlibrary(tidytext)\nlibrary(topicmodels)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nWe have used text data analysis to motivate and illustrate the use of topic\nmodels. However, these models can be used whenever we have high-dimensional\ncount data1. To illustrate this broad applicability, this\nlecture will consider an example from gene expression analysis.\nThe dataset we consider comes from the GTEX\nconsortium. A variety of tissue\nsamples have been subject to RNA-seq analysis, which measures how much of each\ntype of gene is expressed within each sample. Intuitively, we relate,\nDocuments → Tissue samples\nWords → Genes\nWord Counts → Gene expression levels\n\n\n\nx <- read_csv(\"https://uwmadison.box.com/shared/static/fd437om519i5mrnur14xy6dq3ls0yqt2.csv\")\nx\n\n# A tibble: 2,000,000 × 6\n   sample                 gene  tissue tissue_detail Description value\n   <chr>                  <chr> <chr>  <chr>         <chr>       <dbl>\n 1 GTEX-NFK9-0926-SM-2HM… ENSG… Heart  Heart - Left… FGR           368\n 2 GTEX-OXRO-0011-R10A-S… ENSG… Brain  Brain - Fron… FGR           593\n 3 GTEX-QLQ7-0526-SM-2I5… ENSG… Heart  Heart - Left… FGR           773\n 4 GTEX-POMQ-0326-SM-2I5… ENSG… Heart  Heart - Left… FGR           330\n 5 GTEX-QESD-0526-SM-2I5… ENSG… Heart  Heart - Left… FGR           357\n 6 GTEX-OHPN-0011-R4A-SM… ENSG… Brain  Brain - Amyg… FGR           571\n 7 GTEX-OHPK-0326-SM-2HM… ENSG… Heart  Heart - Left… FGR           391\n 8 GTEX-OIZG-1126-SM-2HM… ENSG… Heart  Heart - Left… FGR           425\n 9 GTEX-O5YW-0326-SM-2I5… ENSG… Heart  Heart - Left… FGR           172\n10 GTEX-REY6-1026-SM-2TF… ENSG… Heart  Heart - Left… FGR           875\n# ℹ 1,999,990 more rows\n\nThe goal here is to find sets of genes that tend to be expressed together,\nbecause these co-expression patterns might be indications of shared biological\nprocesses. Unlike clustering, which assumes that each sample is described by one\ngene expression profile, a topic model will be able to model each tissue sample\nas a mixture of profiles (i.e., a mixture of underlying biological processes).\nAs a first step in our analysis, we need to prepare a DocumentTermMatrix\nfor use by the topicmodels package. Since the data were in tidy format, we can\nuse the cast_dtm function to spreaed genes across columns. From there, we can\nfit an LDA model. However, we’ve commented out the code (it takes a while to\nrun) and instead just download the results that we’ve hosted on Box.\n\n\nx_dtm <- cast_dtm(x, sample, gene, value)\n#fit <- LDA(x_dtm, k = 10, control = list(seed = 479))\n#save(fit, file = \"lda_gtex.rda\")\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/ifgo6fbvm8bdlshzegb5ty8xif5istn8.rda\", f)\nfit <- get(load(f))\n\n\nLet’s extract the tidy topic and memberships data. For the memberships, we\nwill also join in the tissue from which each biological sample belonged.\n\n\ntissue_info <- x %>%\n  select(sample, starts_with(\"tissue\")) %>%\n  unique()\n\ntopics <- tidy(fit, matrix = \"beta\") %>%\n  mutate(topic = factor(topic))\nmemberships <- tidy(fit, matrix = \"gamma\") %>%\n  mutate(topic = factor(topic)) %>%\n  left_join(tissue_info, by = c(\"document\" = \"sample\"))\n\n\nWe can now visualize the topics. Let’s consider the genes with the highest\ndiscrimination between topics, using the same discrimination score as in the\nprevious notes. Each row in the heatmap below is a gene, and each column is a\ntopic. The intensity of color represents the gene’s probability within the\ncorresponding topic. Since only discriminative genes are shown, it’s not\nsurprising that most genes are only active within a subset of topics.\n\n\n\n\n\ndiscriminative_genes <- topics %>%\n  group_by(term) %>%\n  mutate(D = discrepancy(beta)) %>%\n  ungroup() %>%\n  slice_max(D, n = 400) %>%\n  mutate(term = fct_reorder(term, -D))\n\ndiscriminative_genes %>%\n  pivot_wider(names_from = topic, values_from = beta) %>%\n  column_to_rownames(\"term\") %>%\n  superheat(\n    pretty.order.rows = TRUE,\n    left.label.size = 1.5,\n    left.label.text.size = 3,\n    bottom.label.size = 0.05,\n    legend = FALSE\n  )\n\n\n\nFigure 1: A heatmap of the most discriminative genes across the 10 estimated topics.\n\n\n\nNow, let’s see what tissues are related to which topics. We can use a\nstructure plot. Before making the plot, we prepare the data appropriately.\nFirst, there are some tissues with very few samples, so we will filter those\ntissues away. Second, we will reorder the samples so that those samples with\nsimilar topic profiles are placed next to one another. This is accomplished by\nrunning a hierarchical clustering on the topic membership vectors and extracting\nthe order of the resulting dendrogram leaves.\n\n\nkeep_tissues <- memberships %>%\n  count(tissue) %>%\n  filter(n > 70) %>%\n  pull(tissue)\n\nhclust_result <- hclust(dist(fit@gamma))\ndocument_order <- fit@documents[hclust_result$order]\nmemberships <- memberships %>%\n  filter(tissue %in% keep_tissues) %>%\n  mutate(document = factor(document, levels = document_order))\n\n\nNext, we can generate the structure plot. The first three lines are the key\nlines: they create a stacked barchart for each sample and then facet across\ntissues. The remaining lines simply refine the appearance of the plot.\n\n\nggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +\n  geom_col(position = position_stack()) +\n  facet_grid(tissue ~ ., scales = \"free\", space = \"free\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_color_brewer(palette = \"Set3\", guide = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(x = \"Topic Membership\", y = \"Sample\", fill = \"Topic\") +\n  theme(\n    panel.spacing = unit(0.5, \"lines\"),\n    strip.switch.pad.grid = unit(0, \"cm\"),\n    strip.text.y = element_text(size = 8, angle = 0),\n    axis.text.y = element_blank(),\n  )\n\n\n\nFigure 2: A structure plot, showing the topic memberships across all tissue samples in the dataset.\n\n\n\nFrom this plot, we can see clearly that different tissues express different\ncombinations of topics. For example, pancreas tissue typically expresses genes\nwith high probability in topics 3 and 8. Further, within tissues, there can be\ndifferences in the types of genes expressed – certain blood cells are almost\nentirely summarized by topic 1, but most require some mixture of topics 1 and 6.\nFinally, we see that there are some topics that are common across several\ntissues. For example, topic 4 is key component of thyroid, skin, muscle, lung,\nand some brain tissue.\n\nIn fact, topic models are an example of a larger family of models,\ncalled mixed-membership models. All of these models generalize clustering, and\ndifferent variants can be applied to other data types, like continuous,\ncategorical, and network data.↩︎\n",
    "preview": "posts/2024-12-27-week11-4/week11-4_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2025-08-19T17:06:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week12-1/",
    "title": "Partial Dependence Profiles I",
    "description": "An introduction to partial dependence profiles.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-16",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(caret)\nlibrary(tidyverse)\nlibrary(DALEX)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nAs more complex models become more common in practice, visualization has\nemerged as a key way for (a) summarizing their essential structure and (b)\nmotivating further modeling refinements.\nIn modern machine learning, it’s common to use a function \\(f\\) to approximate\nthe relationship between a \\(D\\)-dimensional input \\(\\mathbf{x}\\) and a univariate\nresponse \\(y\\). We are given a sample of \\(n\\) pairs \\(\\left(\\mathbf{x}_{i},\ny_{i}\\right)\\) with which to learn this relationship, and we hope that the\nfunction we learn will generalize to future observations.\nSome further notation: We will write \\(x_{j}\\) for the \\(j^{th}\\) coordinate of\n\\(\\mathbf{x}\\). We will write \\(\\mathbf{x}^{j\\vert = z}\\) to denote the observation\n\\(\\mathbf{x}\\) with the \\(j^{th}\\) coordinate set to \\(z\\).\n\n\n\nFigure 1: Illustration of the \\(\\mathbf{x}^{j \\vert = z}\\) operation. The \\(j^{th}\\) coordinate (1 in this case) for a selected observation is set equal to \\(z\\).\n\n\n\nLinear models are simple enough that they don’t require any follow-up visual\ninspection. Since they assume \\(f\\left(\\mathbf{x}\\right) =\n\\hat{\\beta}^{T}\\mathbf{x}\\), they are completely described by the vector of\ncoefficients \\(\\hat{\\beta}\\). We can exactly describe what happens to \\(f\\) when we\nincrease \\(x_{j}\\) by one unit: we just increase the prediction by\n\\(\\hat{\\beta}_{j}\\).\nMore complex models — think random forests or neural networks — don’t have\nthis property. While these models often have superior performance, it’s hard to\nsay how changes in particular input features will affect the prediction.\nPartial dependence plots try to address this problem. They provide a\ndescription for how changing the \\(j^{th}\\) input feature affects the predictions\nmade by complex models.\nTo motivate the definition, consider the toy example below. The surface is\nthe fitted function \\(f\\left(\\mathbf{x}\\right)\\), mapping a two dimensional input\n\\(\\mathbf{x}\\) to a real-valued response. How would you summarize the relationship\nbetween \\(x_{1}\\) and \\(y\\)? The main problem is that the shape of the relationship\ndepends on which value of \\(x_{2}\\) we start at.\n\n\n\nFigure 2: An example of why it is difficult to summarize the relationship between an input variable and a fitted surface for nonlinear models.\n\n\n\nOne idea is to consider the values of \\(x_{2}\\) that were observed in our\ndataset. Then, we can evaluate our model over a range of values \\(x_{1}\\) after\nfixing those values of \\(x_{2}\\). These curves are called Ceteris Paribus\nprofiles1.\nThe same principle holds in higher dimensions. We can fix \\(D - 1\\) coordinates\nof an observation and then evaluate what happens to a sample’s predictions when\nwe vary coordinate \\(j\\). Mathematically, this is expressed by \\(h_{x}^{f,\nj}\\left(z\\right) := f\\left(\\mathbf{x}^{j\\vert= z}\\right)\\).\n\n\n\nFigure 3: Visual intuition behind the CP profile. Varying the \\(j^{th}\\) coordinate for an observation traces out a curve in the prediction surface.\n\n\n\nFor example, let’s consider how CP can be used to understand a model fitted\nto the Titanic dataset. This is a dataset that was used to understand what\ncharacteristics survivors of the Titanic disaster had in common. It’s not\nobvious in advance which characteristics of passengers made them more likely to\nsurvive, so a model is fitted to predict survival.\n\n\ndata(titanic)\ntitanic <- select(titanic, -country) %>%\n  na.omit()\n\nx <- select(titanic, -survived)\nhyper <- data.frame(n.trees = 100, interaction.depth = 8, shrinkage = 0.1, n.minobsinnode = 10)\nfit <- train(x = x, y = titanic$survived, method = \"gbm\", tuneGrid = hyper, verbose = F)\n\n\nNext, we can compute the CP profiles. We are showing the relationship\nbetween age and survival, though any subset of variables could be requested. The\nbold curve is a Partial Dependence (PD) profile, which we will discuss below.\nEach of the other curves corresponds to a passenger, though only a subsample is\nshown. The curves are obtained by fixing all the characteristics of the\npassanger except for age, and then seeing what happens to the prediction when\nthe age variable is increased or decreased.\n\n\nexplanation <- explain(model = fit, data = x, y = titanic$survived)\n\nPreparation of a new explainer is initiated\n  -> model label       :  train  (  default  )\n  -> data              :  2179  rows  7  cols \n  -> target variable   :  2179  values \n  -> predict function  :  yhat.train  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package caret , ver. 7.0.1 , task classification (  default  ) \n  -> model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -> model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -> model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -> model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -> predicted values  :  numerical, min =  0.01206743 , mean =  0.3240771 , max =  0.9908791  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \n\nprofile <- model_profile(explainer = explanation)\nplot(profile, geom = \"profiles\", variables = \"age\") +\n  theme479\n\n\n\nFigure 4: CP and PDP profiles for age, for a GBM fitted to the Titanic dataset.\n\n\n\nIt seems that children had the highest probability2 of survival. The relationship is\nfar from linear, with those between 40 and 60 all having about the same\nprobabilities. Notice that the profiles are vertically offset from one passenger\nto another. This is because, aside from age, each passenger had characteristics\nthat made them more or less likely to survive.\nWe used the DALEX package to produce these curves. The explain function\ntakes the fitted model and original dataset as input. It returns an object with\nmany kinds of model summaries. To extract the CP profiles from these summaries,\nwe use model_profile. The output of this function has been designed so that\ncalling plot with geom = \"profiles\" will show the CP profiles.\nThe PD profile is computed by averaging across all the CP profiles. It is a\nmore concise alternative to CP profiles, showing one curve per features, rather\nthan one curve per sample.\n\n\nplot(profile, geom = \"aggregates\") +\n  theme479\n\n\n\nNot only are the PD plots simpler to read than the full collection of CP\nprofiles — by performing this aggregation step, subtle patterns may become more\nsalient, for the same reason that an average carries more information than any\nsubset of observations.\n\nCeteris Paribus means « All Else Held Equal. »↩︎\nTechnically, these are\nall predicted probabilities from the model.↩︎\n",
    "preview": "https://uwmadison.box.com/shared/static/mpe45nor6xm4gt1idhedayw9ik754c2k.png",
    "last_modified": "2025-08-19T17:25:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week12-2/",
    "title": "Partial Dependence Profiles II",
    "description": "Discovering richer structure in partial dependence profiles.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-15",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(DALEX)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nPartial dependence (PD) plots help answer the question, « How is feature \\(j\\)\nused by my model \\(f\\)? » Slight variations on PD plots are useful for some\nrelated followup questions,\nHas my model \\(f\\) learned interactions between features \\(j\\) and \\(j^{\\prime}\\)?\nHow do the models \\(f\\) and \\(f^{\\prime}\\) differ in the way that they use feature \\(j\\)?\n\nThe variants, called Grouped and Contrastive PD plots, reduce the original CP\nprofiles less aggressively than PD plots, but without becoming overwhelmingly\ncomplicated.\nInteractions\nWe say that there is an interaction between variables \\(j\\) and \\(j^{\\prime}\\) if\nthe relationship between \\(x_{j}\\) and \\(y\\) is modulated by variable \\(j^{\\prime}\\).\nFor example, in the figure below, the slope of cross-sections across \\(j\\) depends\non \\(j^{\\prime}\\).\nUsing the language of CP profiles, the figure above means that the shape of\nthe CP profile in \\(j\\) depends on the particular setting of \\(j^{\\prime}\\). This\nmotivates the use of Grouped PD profiles — we compute several PD profiles in\n\\(j\\), restricting attention to CP profiles whose value \\(x_{j^{\\prime}}\\) lies\nwithin a prespecified range.\nTo illustrate, we revisit the CP profiles for age from the Titanic dataset.\nBelow, the profiles are grouped according to the class of the ticket holder. The\nresult shows that the relationship between age and survival was not the same\nacross all passengers. For all classes, there was a decrease in survival\nprobability for adults, but the dropoff was most severe for crew members.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/nau695mppsoxx0f6bns1ieo7kh1bje0j.rda\", f)\nfit <- get(load(f))\n\ndata(titanic)\ntitanic <- titanic %>%\n  select(-country) %>%\n  na.omit()\nx <- select(titanic, -survived)\n\nexplanation <- explain(model = fit, data = x, y = titanic$survived)\n\nPreparation of a new explainer is initiated\n  -> model label       :  train  (  default  )\n  -> data              :  2179  rows  7  cols \n  -> target variable   :  2179  values \n  -> predict function  :  yhat.train  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package caret , ver. 7.0.1 , task classification (  default  ) \n  -> model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -> model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -> model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -> model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -> predicted values  :  numerical, min =  0.007271972 , mean =  0.3239071 , max =  0.9885397  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \n\nprofiles <- model_profile(explainer = explanation, groups = \"class\")\nplot(profiles, geom = \"profiles\", variables = \"age\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\nFigure 1: Grouping the CP profiles by ticket class reveals an interaction effect with age in the Titanic dataset.\n\n\n\nWhat should we do if there are many input variables and we don’t have a\npriori knowledge about which variables \\(j^{\\prime}\\) might be interacting with\n\\(j\\)? One idea is to try to discover relevant interactions by clustering the\noriginal set of CP profiles.\nIn more detail, we can compute the CP profiles for all the samples, and then\nsee whether there are subsets of profiles that all look similar. If we find\nfeatures \\(j^{\\prime}\\) that characterize these groupings, then we have found\nfeatures that interact with \\(j\\) (with respect to the fitted model \\(f\\)). The plot\nbelow shows the same profiles as above, but clustering directly. It seems to\nrecover the interaction between age and class, even though we have not\nexplicitly provided this grouping variable.\n\n\nprofiles <- model_profile(explainer = explanation, variables = \"age\", k = 3)\nplot(profiles, geom = \"profiles\", variables = \"age\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\nFigure 2: Discovered groupings in the CP profiles for age reveals an interaction effect.\n\n\n\nModel Comparison\nThe comparison of different models’ PD profiles can be used to,\nValidate a simple model\nGuide the design of new features, and\nCharacterizing overfitting\nPD profiles that are used to compare different models are sometimes called\n« Contrastive » PD profiles.\n\nTo validate a simple model, we can compare its PD profiles with those of a\nmore sophisticated model. We will illustrate this by fitting linear and random\nforest models to a dataset of apartment prices1. Given various properties of\nan apartment, the goal is to determine its price. The code below fits the two\nmodels and extracts their CP and PD profiles.\n\n\ndata(apartments)\nx <- select(apartments, -m2.price)\nprofiles_lm <- train(x, apartments$m2.price, method = \"lm\") %>%\n  explain(x, apartments$m2.price, label = \"LM\") %>%\n  model_profile()\n\nPreparation of a new explainer is initiated\n  -> model label       :  LM \n  -> data              :  1000  rows  5  cols \n  -> target variable   :  1000  values \n  -> predict function  :  yhat.train  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package caret , ver. 7.0.1 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  1781.848 , mean =  3487.019 , max =  6176.032  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -247.4728 , mean =  -1.07525e-12 , max =  469.0023  \n  A new explainer has been created!  \n\nprofiles_rf <- train(x, apartments$m2.price, method = \"rf\", tuneGrid = data.frame(mtry = 10)) %>%\n  explain(x, apartments$m2.price, label = \"RF\") %>%\n  model_profile()\n\nPreparation of a new explainer is initiated\n  -> model label       :  RF \n  -> data              :  1000  rows  5  cols \n  -> target variable   :  1000  values \n  -> predict function  :  yhat.train  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package caret , ver. 7.0.1 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  1663.523 , mean =  3486.655 , max =  6386.398  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -227.9678 , mean =  0.3644315 , max =  274.7448  \n  A new explainer has been created!  \n\nThe PD profile below shows that the random forest learns linear\nrelationships with price for both the surface and floor variables. If all the\neffects were like this, then we would have a good reason for preferring the\nlinear model.\n\n\nplot(profiles_lm, profiles_rf, variables = c(\"surface\", \"floor\")) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\nFigure 3: A contrastive PD display suggests that the floor and surface features are linearly related with apartment price.\n\n\n\n11.. When making the comparison between a simple and a complex model, certain\ndiscrepancies might become apparent. For example, important nonlinearities or\ninteractions might be visible from the PD profiles of the complex model. This\ninformation can guide the design of new features in the simpler model, so that\nit can continue to be used. This is exactly the case in the apartments dataset\nabove – there is a strong nonlinear relationship for the construction year\nvariables. This suggests that, if a linear model is still desired, then a new\nfeature should be defined that identifies whether the apartment was built\nbetween 1935 and 1990.\n\n\nplot(profiles_lm, profiles_rf, variables = \"construction.year\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\nFigure 4: The random forest learns a nonlinear relationship between construction year and apartment price. This suggests designing new features to include in the linear model.\n\n\n\nSuppose you have found that a model is overfitting (e.g., by finding that\nit’s training error is much lower than its test error). One way to address this\noverfitting is to compare the PD profiles between the simple and complex models.\nIf the profiles are very different for one of the features, then that feature\nmay be the source of overfitting.\n\nIt is a simulated dataset, but\ndesigned to reflect properties of a real dataset.↩︎\n",
    "preview": "posts/2024-12-27-week12-2/week12-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-08-19T17:25:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week12-3/",
    "title": "Visualization for Model Building",
    "description": "The relationship between exploratory analysis and model development.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-14",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(rstan)\nlibrary(tidyverse)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\n\nExploratory data analysis and model building complement each other well.\nIn practical problems, visualization can guide us towards more plausible\nmodels.\n\n\n\n\nWe rarely know the exact form of a model in advance, but usually have a\nfew reasonable candidates. Exploratory analysis can rule out some\ncandidates and suggest new, previously unanticipated, relationships.\n\n\n\n\nWe will illustrate these ideas using an example. A researcher is\ninterested in monitoring the level of PM2.5, a type of small air\nparticlute that can be bad for public health. High quality data are\navailable from weather stations scattered around the world, but their\ndata only apply locally. On the other hand, low quality data, available\nfrom satellites, are available everywhere. A model is desired that uses\nthe weather station measurements to calibrate the satellite data. If it\nworks well, it could be used to monitor PM2.5 levels at global scale.\n\n\n\n\nf <- tempfile()\ndownload.file(\"https://github.com/jgabry/bayes-vis-paper/blob/master/bayes-vis.RData?raw=true\", f)\nGM <- get(load(f))\nGM@data <- GM@data %>% \n  mutate(\n    log_pm25 = log(pm25), \n    log_sat = log(sat_2014)\n  )\n\n\n\nThe simplest model simply fits \\(\\text{station} = a + b \\times\n\\text{satellite}\\) at locations where they are both\navailable. This model was used in practice by the Global Burden of\nDisease project until 2016.\n\n\n\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = super_region_name), size = 0.8, alpha = 0.7) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"WHO Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nFigure 1: The relationship between satellite and ground station\nestimates of PM2.5.\n\n\n\n\nHowever, when we plot these two variables against one another, we notice\nthat there is still quite a bit of heterogeneity. The residuals are\nlarge — what features might be correlated with these residuals, which if\nincluded, would improve the model fit?\n\nThe error \\(\\epsilon_{i}\\) in a model \\(y_i =\nf\\left(x_{i}\\right) +\n\\epsilon_{i}\\) represents out our ignorance of the myriad\nof unmeasured factors that determine the relationship between \\(x\\) and\n\\(y\\).\n\n\nFor example, desert sand is known to increase PM2.5, but it is not\nvisible from space. The residuals are probably correlated with whether\nthe model is in a desert area (we underpredict PM2.5 in deserts), and so\nwould be improved if we included a term with this feature.\n\n\n\nOne hypothesis is that country region is an important factor. Below, we\nfit regression lines separately for different country super-regions, as\nspecified by the WHO. The fact that the slopes are not the same in each\nregion means that we should modify our model to have a different slope\nin each\nregion1.\n\n\n\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = super_region_name), size = 0.4, alpha = 0.7) +\n  geom_smooth(aes(col = super_region_name), method = \"lm\", se = F, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"WHO Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nFigure 2: The relationship between these variables is not the same\nacross regions.\n\n\n\n\nThe WHO categorizations are somewhat arbitrary. Maybe there are better\ncountry groupings, tailored specifically to the PM2.5 problem? One idea\nis to cluster the ground stations based on PM2.5 level and use these\nclusters as a different region grouping.\n\n\n\naverage <- GM@data %>% \n  group_by(iso3) %>% \n  summarise(pm25 = mean(pm25))\n\nclust <- dist(average) %>%\n  hclust() %>%\n  cutree(k = 6)\n\nGM@data$cluster_region <- map_chr(GM@data$iso3, ~ clust[which(average$iso3 == .)])\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = cluster_region), size = 0.4, alpha = 0.7) +\n  geom_smooth(aes(col = cluster_region), method = \"lm\", se = F, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"Cluster Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nFigure 3: We can define clusters of regions on our own, using a\nhierarchical clustering.\n\n\n\n\nWe now have a « network » of models. We’re going to want more refined\ntools for distinguishing between them. This is the subject of the next\ntwo lectures.\n\n\n\n\n\n\nViewed differently, this is like adding an interaction between the\nsatellite measurements and WHO\nregion.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T17:25:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week12-4/",
    "title": "Prior and Posterior Predictives",
    "description": "Simulating data to evaluate model quality.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-13",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(rstan)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\n\nFrom the previous notes, we see that an exploratory analysis can\nmotivate few plausible models for a dataset. How should we go about\nchoosing between them?\n\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/2pzgdu7gyobhl5tezo63tns7by1aiy6d.rda\", f)\nGM <- get(load(f))\n\n\n\nIn some of your other classes, you might have seen people use\ncross-validation / test set error. While this is useful (and relatively\nautomatic), it can be a black box. An alternative which often brings\nmores insight into the structure of the problem is to use prior and\nposterior predictive distributions for (visual) model comparison.\n\n\nPrior predictive distributions\n\n\n\nThe prior predictive distribution can be used to decide whether certain\nmodel families are reasonable candidates for a problem, before formally\nincorporating the evidence coming from the data.\n\n\n\n\nThe idea is that if we can write down a generative model, then we can\nsimulate different datasets with it, even before estimating the model\nparameters. This is often the case in Bayesian models, where we can (a)\nsample parameters from the prior, and (b) simulate data from the model\nwith those parameters.\n\n\n\n\nIf the simulated datasets are plausible, then the overall model class is\na reasonable one. If they are not, then the model class should be\nmodified. Either the prior or the likelihood might need revision.\n\n\n\n\nFor example, in the example below, we simulate datasets using both a\nvague and an informative prior. Vague priors are often recommended\nbecause they are « more objective » in some sense. However, in this\ncase, we see that the simulated datasets are not even remotely\nplausible.\n\n\n\n\n# function to simulate from vague prior\nprior1 <- function(Nsim) {\n  tau0 <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  tau1 <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  sigma <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  beta0i <- rnorm(7, 0, tau0)\n  beta1i <- rnorm(7, 0, tau1)\n  beta0 <- rnorm(1, 0, 100)\n  beta1 <- rnorm(1, 0, 100)\n  \n  epsilon <- rnorm(Nsim, 0, sigma)\n  data.frame(\n    log_pm25 = GM$log_pm25,\n    region = GM$super_region_name,\n    sim = beta0 + beta0i[GM$super_region] + (beta1 + beta1i[GM$super_region]) * GM$log_sat + epsilon\n  )\n}\n\n\n\n\nprior1_data <- map_dfr(1:12, ~ prior1(Nsim = nrow(GM@data)), .id = \"replicate\")\nggplot(prior1_data, aes(x = log_pm25, y = sim)) + \n  geom_abline(slope = 1) +\n  geom_point(aes(col = region), alpha = 0.1, size = 0.4) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\n\nFigure 1: Prior predictive samples from the vague prior are on a\ncompletely implausible scale.\n\n\n\n\nThe block below instead simulates from a subjective, informative prior.\nThe resulting samples are much more plausible, lying in a comparable\nrange to the true data. However, note, the samples from the prior\npredictive do not need to look exactly like the observed data — if they\ndid, there would be no need to fit model parameters! Instead, they\nshould look like plausible datasets that might have been observed.\n\n\n\n# function to simulate from informative prior\nprior2 <- function(Nsim) {\n  tau0 <- abs(rnorm(1, 0, 1))\n  tau1 <- abs(rnorm(1, 0, 1))\n  sigma <- abs(rnorm(1, 0, 1))\n  beta0i <- rnorm(7, 0, tau0)\n  beta1i <- rnorm(7, 0, tau1)\n  beta0 <- rnorm(1, 0, 1)\n  beta1 <- rnorm(1, 1, 1)\n  \n  epsilon <- rnorm(Nsim, 0, sigma)\n  data.frame(\n    log_pm25 = GM$log_pm25,\n    region = GM$super_region_name,\n    sim = beta0 + beta0i[GM$super_region] + (beta1 + beta1i[GM$super_region]) * GM$log_sat + epsilon\n  )\n}\n\nprior2_data <- map_dfr(1:12, ~ prior2(Nsim = nrow(GM@data)), .id = \"replicate\")\nggplot(prior2_data, aes(x = log_pm25, y = sim)) + \n  geom_abline(slope = 1) +\n  geom_point(aes(col = region), alpha = 0.1, size = 0.4) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\n\nFigure 2: Prior predictive samples from the weakly informative prior\nseem more plausible, though they do not (and should not) exactly fit the\ntrue data.\n\n\n\n\nPhilosophically, this prior predictive analysis is based on the idea\nthat, though probability is subjective, evidence can be used to update\nour beliefs. The idea of the prior predictive is to visually encode\nsubjective beliefs about the problem under study before gathering new\nevidence.\n\n\nPosterior predictive distributions\n\n\n\nOnce the prior predictive is calibrated, we can fit the model. To\nevaluate it’s quality, we can use the posterior predictive.\n\n\n\n\nThe posterior predictive is just like the prior predictive, except that\nit samples model parameters from the data-informed posterior, rather\nthan the data-ignorant prior.\n\n\n\n\nFormally, it is the distribution of new datasets when drawing parameters\nfrom the posterior. The simulation mechanism is (a) draw model\nparameters from the posterior and (b) simulate a dataset using\nparameters from (a).\n\n\n\n\nThe code below fits the three models. We are using the\nrstan package to fit three Bayesian models. The first model\nlm is a Bayesian linear regression, assuming the same slope\nacross all regions. The two other models assume different slopes for\ndifferent\nregions1,\nbut use the WHO and cluster-based region definitions, respectively. You\ndo not need to worry about how the rstan code, which is\nsourced into the stan_model function, is written. It is\nenough to be able to fit these two types of models as if they are\nbuilt-in function in R.\n\n\n\n\n# Define the input datasets for the lm, region-based, and cluster-based models\ndatasets <- list(\n  \"lm\" = with(GM@data, list(N = length(log_pm25), y = log_pm25, x = log_sat)),\n  \"regions\" = with(\n    GM@data, \n    list(N = length(log_pm25), y = log_pm25, x = log_sat, group = super_region, R = n_distinct(super_region))\n  ),\n  \"clusters\" = with(\n    GM@data, \n    list(N = length(log_pm25), y = log_pm25, x = log_sat, group = as.integer(cluster_region), R = n_distinct(cluster_region))\n  )\n)\n\n# Define the two types of Bayesian models\nmodel_def <- list(\n  \"lm\" = stan_model(\"https://uwmadison.box.com/shared/static/hoq1whain301bj6gj670itxabnnhvcy7.stan\"),\n  \"hier\" = stan_model(\"https://uwmadison.box.com/shared/static/lvouz9jj4rbkmrx5osj2dtrhj2ycdll8.stan\")\n)\n\n\n\n\n# Fit the models above to the three datasets of interest\ncontrols <- list(max_treedepth = 15, adapt_delta = 0.99)\nmodels <- list(\n  \"lm\" = sampling(model_def$lm, data = datasets$lm, chains = 1, control = controls),\n  \"regions\" = sampling(model_def$hier, data = datasets$regions, chains = 1, control = controls),\n  \"clusters\" = sampling(model_def$hier, data = datasets$clusters, chains = 1, control = controls)\n)\n\n\n\nThe code above takes a little while to run (about 10 minutes for the\nlast two models). To save some time, you can download the fitted models\nfrom the link below. The models object is a list whose\nelements are fitted STAN models for the three model definitions above.\nThe fitted model objects include posterior samples for the region slopes\nas well as simulated ground station PM2.5 data, based on those posterior\nslopes.\n\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/x7dotair443mhx34yzie3m3lrsvhk19a.rda\", f)\nmodels <- get(load(f))\n\n\n\nThe block below simulates station PM2.5 data from the fitted posterior\nof the cluster-based model. Note that, compared to the prior predictive,\nthe posterior predictive is much more closely related to the true\nunderlying dataset.\n\n\n\n# extract 12 samples and reshape it to \"long\" format\nposterior_samples <- as.matrix(models$clusters, pars = \"y_sim\")[950:961, ] %>%\n  t() %>%\n  as_tibble() %>%\n  bind_cols(GM@data) %>%\n  pivot_longer(V1:V12, names_to = \"replicate\", values_to = \"log_pm25_sim\")\n\nggplot(posterior_samples, aes(log_pm25, log_pm25_sim)) +\n  geom_abline(slope = 1) +\n  geom_point(aes(col = cluster_region), size = 0.4, alpha = 0.1) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\n\nFigure 3: Samples from the posterior predictive in the cluster-based\nmodel.\n\n\n\n\nWe can verify that features of the real dataset are accurately captured\nby features of the posterior predictive. One subtlety is that there is a\ndanger of overfitting features in the posterior predictive. It is best\nto choose features of the data that are not directly modeled (e.g., if\nyou use slope in the model estimation, then don’t evaluate the posterior\npredictive using the slope, since by definition this will be\nwell-captured). In the block below, we compute the skewness for each\nsimulated station dataset from the three different models. These\nskewnesses are plotted as histograms, with the true dataset’s skewness\nindicated by a vertical line. It seems that the model that uses\nclustering to define regions is able to simulate datasets with skewness\nsimilar to that in the real dataset.\n\n\n\napply_stat <- function(x, f) {\n  z <- as.matrix(x, pars = \"y_sim\")\n  tibble(\n    \"replicate\" = seq_len(nrow(z)),\n    \"statistic\" = apply(z, 1, f)\n  )\n}\n\nskew <- function(x) {\n  xdev <- x - mean(x)\n  n <- length(x)\n  r <- sum(xdev^3) / sum(xdev^2)^1.5\n  r * sqrt(n) * (1 - 1/n)^1.5\n}\n\nposteriors <- map_dfr(models, ~ apply_stat(., skew), .id = \"model\")\ntruth <- skew(GM@data$log_pm25)\nggplot(posteriors, aes(statistic)) +\n  geom_histogram(aes(fill = model), binwidth = 0.01) +\n  geom_vline(xintercept = truth, col = \"red\") +\n  scale_fill_brewer(palette = \"Set3\")\n\n\n\n\nFigure 4: Posterior simulated skewness according to the three different\nmodels.\n\n\n\n\n\n\n\n\nBayesian regression models that allow different slopes for different\ngroups are called hierarchical\nmodels.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T17:25:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week12-5/",
    "title": "Pointwise Diagnostics",
    "description": "Evaluating the fit at particular observations in Bayesian models.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-12",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidyverse)\nlibrary(loo)\nlibrary(ggrepel)\nlibrary(rstan)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\n\n\n\n\nAll the model visualization strategies we’ve looked at in the last few\nlectures have been dataset-wide. That is, we looked at properties of the\ndataset as a whole, and whether the model made sense globally, across\nthe whole dataset. Individual observations might warrant special\nattention, though.\n\n\n\n\nThe block below loads in the fitted models from the previous set of\nnotes.\n\n\n\n\ndownloader <- function(link) {\n  f <- tempfile()\n  download.file(link, f)\n  get(load(f))\n}\n\nmodels <- downloader(\"https://uwmadison.box.com/shared/static/x7dotair443mhx34yzie3m3lrsvhk19a.rda\")\nGM <- downloader(\"https://uwmadison.box.com/shared/static/2pzgdu7gyobhl5tezo63tns7by1aiy6d.rda\")\n\n\n\nA first diagnostic to consider is the leave-one-out predictive\ndistribution. This is the probability \\(p\\left(y_{i} \\vert y_{-i}\\right)\\)\nof sample \\(i\\) after having fitted a model to\nall samples except \\(i\\). Ideally, most observations in\nthe dataset to have high predictive probability.\n\nNote that this can be used for model comparison. Some models might have\nbetter per-sample leave-one-out predictive probabilities for almost all\nobservations.\n\n\nThis is similar to a leave-one-out residual.\n\n\n\nIf we use rstan to fit a Bayesian model, then these leave-one-out\nprobabilities can be estimated using the loo function in\nthe loo package. The code below computes these\nprobabilities for each model, storing the difference in predictive\nprobabilities for models two and three in the diff23\nvariable.\n\n\n\nelpd <- map(models, ~ loo(., save_psis = TRUE)$pointwise[, \"elpd_loo\"])\nelpd_diffs <- GM@data %>%\n  mutate(\n    ID = row_number(),\n    diff23 = elpd[[3]] - elpd[[2]]\n  )\n\noutliers <- elpd_diffs %>%\n  filter(abs(diff23) > 6)\n\n\n\nWe plot the difference between these predictive probabilities below. The\ninterpretation is that Ulaanbataar has much higher leave-one-out\nprobability under the cluster-based model, perhaps because that model is\nable to group the countries with large deserts together with one\nanother. On the other hand, Santo Domingo is better modeled by model 2,\nsince it has higher leave-one-out probability in that model.\n\n\n\nggplot(elpd_diffs, aes(ID, diff23)) +\n  geom_point(\n    aes(col = super_region_name),\n    size = 0.9, alpha = 0.8\n    ) +\n  geom_text_repel(\n    data = outliers,\n    aes(label = City_locality),\n    size = 3 \n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    y = \"Influence (Model 2 vs. 3)\",\n    col = \"WHO Region\"\n  )\n\n\n\n\nFigure 1: The difference in leave one out predictive probabilities for\neach sample, according to the WHO-region and cluster based hierarchical\nmodels.\n\n\n\n\nAnother diagnostic is to consider the influence of an observation.\nFormally, the influence is a measure of how much the posterior\npredictive distribution changes when we leave one sample out. The idea\nis to measure the difference between the posterior predictives using a\nform of KL divergence, and note down the observations that lead to a\nvery large difference in divergence.\n\n\n\n\n\nFigure 2: Visual intuition about the influence of observations. If the\nposterior predictive distributions shift substantially when an\nobservation is included or removed, then it is an influential\nobservation.\n\n\n\n\nWhen using rstan, the influence measure can be computed by the\npsis function. The pareto_k diagnostic\nsummarizes how much the posterior predictive shifts when an observation\nis or isn’t included. For example, in the figure below, observation 2674\n(Ulaanbaatar again) is highly influential.\n\n\n\nloglik <- map(models, ~ as.matrix(., pars = \"log_lik\"))\nkdata <- GM@data %>%\n  mutate(\n    k_hat = psis(loglik[[2]])$diagnostics$pareto_k,\n    Index = row_number()\n  )\noutliers <- kdata %>%\n  filter(k_hat > 0.25)\n\nggplot(kdata, aes(x = Index, y = k_hat)) + \n  geom_point(aes(col = super_region_name), size = 0.5, alpha = 0.9) + \n  scale_color_brewer(palette = \"Set2\") +\n  geom_text_repel(data = outliers, aes(label = Index)) +\n  labs(y = \"k-hat\")\n\n\n\n\nFigure 3: The influence of each sample on the final posterior\ndistribution.\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T17:25:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week13-1/",
    "title": "Introduction to Feature Learning",
    "description": "An introduction to compositional feature learning.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-11",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(keras)\n\n\n\n\nIn classical machine learning, we assume that the features most relevant\nto prediction are already available. E.g., when we want to predict home\nprice, we already have features about square feet and neighborhood\nincome, which are clearly relevant to the prediction task.\n\n\n\n\nIn many modern problems though, we have only access to data where the\nmost relevant features have not been directly encoded.\n\n\nIn image classification, we only have raw pixel values. We may want to\npredict whether a pedestrian is in an image taken from a self-driving\ncar, but we have only the pixels to work with. It would be useful to\nhave an algorithm come up with labeled boxes like those in the examples\nbelow.\n\n\nFor sentiment analysis, we want to identify whether a piece of text is a\npositive or negative sentiment. However, we only have access to the raw\nsequence of words, without any other context. Examples from the IMDB\ndataset are shown below.\n\n\nIn both of these examples, this information could be encoded manually,\nbut it would a substantial of effort, and the manual approach could not\nbe used in applications that are generating data constantly. In a way,\nthe goal of these algorithms is to distill the raw data down into a\nsuccinct set of descriptors that can be used for more classical machine\nlearning or decision making.\n\n\n\n\n\n\nFigure 1: An example of the types of labels that would be useful to\nhave, starting from just the raw image.\n\n\n\n\nExample reviews from the IMDB dataset:\n\n  positive,\"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"\"has got all the polari ....\"\n  positive,\"I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be ...\"\n  negative,\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to ...\"\n  positive,\"Petter Mattei's \"\"Love in the Time of Money\"\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a ...\"\n\n\nIn these problems, the relevant features only arise as complex\ninteractions between the raw data elements.\n\n\nTo recognize a pedestrian, we need many adjacent pixels to have a\nparticular configuration of values, leading to combinations of edges and\nshapes, which when viewed together, become recognizable as a person.\n\n\nTo recognize a positive or negative sentiment, we need to recognize\ninteractions between words. “The movie was good until” clearly has bad\nsentiment, but you cannot tell that from the isolated word counts alone.\n\n\n\n\nThe main idea of deep learning is to learn these more complex features\none layer at a time. For image data, the first layer recognizes\ninteractions between individual pixels. Specifically, individual\nfeatures are designed to “activate” when particular pixel interactions\nare present. The second layer learns to recognize interactions between\nfeatures in the first layer, and so on, until the learned features\ncorrespond to more “high-level” concepts, like sidewalk or pedestrian.\n\n\n\n\nBelow is a toy example of how an image is processed into feature\nactivations along a sequence of layers. Each pixel within the feature\nmaps correspond to a patch of pixels in the original image – those later\nin the network have a larger field of view than those early on. A pixel\nin a feature map has a large value if any of the image features that it\nis sensitive to are present within its field of vision.\n\n\n\n\n\n\nFigure 2: A toy diagram of feature maps from the model loaded below.\nEarly layers have fewer, but larger feature maps, while later layers\nhave many, but small ones. The later layers typically contain\nhigher-level concepts used in the final predictions.\n\n\n\n\n\nAt the end of the feature extraction process, all the features are\npassed into a final linear or logistic regression module that completes\nthe regression or classification task, respectively.\n\n\n\n\nIt is common to refer to each feature map as a neuron. Different neurons\nactivate when different patterns are present in the original, underlying\nimage.\n\n\n\n\n\n\nFigure 3: An illustration of the different spatial contexts of feature\nmaps at different layers. An element of a feature map has a large value\n(orange in the picture) if the feature that it is sensitive to is\npresent in its spatial context. Higher-level feature maps are smaller,\nbut each pixel within it has a larger spatial context.\n\n\n\n\nBelow, we load a model to illustrate the concept of multilayer networks.\nThis model has 11 layers followed by a final logistic regression layer.\nThere are many types of layers. Each type of layer contributes in a\ndifferent way to the feature learning goal, and learning how design and\ncompose these different types of layers is one of the central concerns\nof deep learning. The Output Shape column describes the number and shape\nof feature maps associated with each layer. For example, the first layer\nhas 32 feature maps, each of size \\(148\n\\times 148\\). Deeper parts of the network have more\nlayers, but each is smaller. We will see how to load and inspect these\nfeatures in the next lecture.\n\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\nmodel\n\nModel: \"sequential_1\"\n______________________________________________________________________\n Layer (type)                  Output Shape                Param #    \n======================================================================\n conv2d_7 (Conv2D)             (None, 148, 148, 32)        896        \n max_pooling2d_7 (MaxPooling2  (None, 74, 74, 32)          0          \n D)                                                                   \n conv2d_6 (Conv2D)             (None, 72, 72, 64)          18496      \n max_pooling2d_6 (MaxPooling2  (None, 36, 36, 64)          0          \n D)                                                                   \n conv2d_5 (Conv2D)             (None, 34, 34, 128)         73856      \n max_pooling2d_5 (MaxPooling2  (None, 17, 17, 128)         0          \n D)                                                                   \n conv2d_4 (Conv2D)             (None, 15, 15, 128)         147584     \n max_pooling2d_4 (MaxPooling2  (None, 7, 7, 128)           0          \n D)                                                                   \n flatten_1 (Flatten)           (None, 6272)                0          \n dropout (Dropout)             (None, 6272)                0          \n dense_3 (Dense)               (None, 512)                 3211776    \n dense_2 (Dense)               (None, 1)                   513        \n======================================================================\nTotal params: 3453121 (13.17 MB)\nTrainable params: 3453121 (13.17 MB)\nNon-trainable params: 0 (0.00 Byte)\n______________________________________________________________________\n\n\n\nWhile we will only consider image data in this course, the idea of\nlearning complex features by composing a few types of layers is a\ngeneral one. For example, in sentiment analysis, the first layer learns\nfeatures that activate when specific combinations of words are present\nin close proximity to one another. The next layer learns interactions\nbetween phrases, and later layers are responsive to more sophisticated\ngrammar.\n\n\n\n\nDeep learning is often called a black box because these intermediate\nfeatures are often complex and not directly interpretable according to\nhuman concepts. The problem is further complicated by the fact that\nfeatures are “distributed” in the sense that a single human concept can\nbe encoded by a configuration of multiple features. Conversely, the same\nmodel feature can encode several human concepts.\n\n\n\n\nFor this reason, a literature has grown around the question of\ninterpreting neural networks. The field relies on visualization and\ninteraction to attempt to understand the learned representations, with\nthe goal of increasing the safety and scientific usability of deep\nlearning models. While our class will not discuss how to design or\ndevelop deep learning models, we will get a taste of the\ninterpretability literature in the next few lectures.\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T17:34:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week13-2/",
    "title": "Visualizing Learned Features",
    "description": "A first look at activations in a deep learning model.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-10",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(\"dplyr\")\nlibrary(\"keras\")\nlibrary(\"purrr\")\nlibrary(\"RColorBrewer\")\n\n\n\n\nIn the last lecture, we discussed the conceptual foundations of feature\nlearning. In this lecture, we’ll see how to extract and visualize\nfeatures learned by a computer vision model.\n\n\n\n\nWe will inspect a model that was\ntrained1\nto distinguish between photos of cats and dogs. We’ve included a\nsubsample of the training dataset below – the full dataset can be\ndownloaded\nhere. From the\nprintout, you can see that we have saved 20 images, each of size \\(150 \\times\n150\\) pixels, and with three color channels (red, green,\nand blue).\n\n\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/o7t3nt77iv3twizyv7yuwqnca16f9nwi.rda\", f)\nimages <- get(load(f))\ndim(images) # 20 sample images\n\n[1]  20 150 150   3\n\n\nLet’s take a look at a few examples from the training dataset. We’ve\nrandomly sampled 10 dogs and 10 cats. The command par\nallows us to plot many images side by side (in this case, in a \\(4 \\times\n5\\) grid).\n\n\n\npar(mfrow = c(4, 5), mai = rep(0.00, 4))\nout <- images %>%\n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\n\nFigure 1: A sample of 20 random images from the dog vs. cat training\ndataset.\n\n\n\n\n\nThe array_tree function above splits the 4D array into a\ncollection of 3D slices. Each of these 3D slices corresponds to one\nimage — the three channels correspond to red, green, and blue colors,\nrespectively. The next map line plots each of the resulting\n3D arrays\n\n\n\n\nNext, let’s consider what types of features the model has learned, in\norder to distinguish between cats and dogs. Our approach will be to\ncompute activations on a few images and visualize them as 2D feature\nmaps. These visualizations will help us see whether there are systematic\npatterns in what leads to an activation for a particular neuron.\n\n\n\n\nTo accomplish this, we will create an R object to retrieve all the\nintermediate feature activations associated with an input image. Every\ntime we call this object on a new image, it will return the activations\nfor features at all layers.\n\n\n\n\n# download model\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\n\nlayer_outputs <- map(model$layers, ~ .$output)\nactivation_model <- keras_model(inputs = model$input, outputs = layer_outputs)\nfeatures <- predict(activation_model, images)\n\n\n\nEach element of features corresponds to a different layer.\nWithin a single layer, the 3D array provides the activations of each\nfeature across different spatial windows. For example, for the first\nlayer, there are 32 features with activations spread across a 148 x 148\ngrid, each grid element with its own spatial context.\n\n\n\ndim(features[[1]])\n\n[1]  20 148 148  32\n\n\nThe block below visualizes the first feature map in the first layer. We\nplot the associated input image next to it. This feature seems to be a\nhorizontal edge detector – it activates whenever there are transitions\nfrom dark to light areas when moving vertically. For example, when the\nwhite leash goes over the shadow in the background, this feature has\nsome of its highest activations.\n\n\n\nplot_feature <- function(feature) {\n  rotate <- function(x) t(apply(x, 2, rev))\n  image(rotate(feature), axes = FALSE, asp = 1, col = brewer.pal(4, \"Blues\"))\n}\n\nix <- 3\npar(mfrow = c(1, 2), mai = rep(0.00, 4))\nplot(as.raster(images[ix,,, ], max = 255))\nplot_feature(features[[1]][ix,,, 1])\n\n\n\n\nFigure 2: An image and its activations for the first neuron in layer 1.\n\n\n\n\nLet’s visualize a few more of these features. We see more vertical and\nhorizontal edge detectors — features with high values at sharp changes\nin color in the underlying images. This is consistent with our earlier\nclaim that the first layer of a network learns to recognize pixel-level\ninteractions.\n\n\n\npar(mfrow = c(6, 7), mai = rep(0.00, 4))\nout <- features[[2]][ix,,,] %>%\n  array_branch(margin = 3) %>%\n  map(~ plot_feature(.))\n\n\n\n\nFigure 3: Activations for a collection of neurons at layer 2, for the\nsame image as given above.\n\n\n\n\nNext, we visualize features at a higher level in the network. At this\npoint, each activation corresponds to a larger spatial context in the\noriginal image, so there are fewer activations per feature. There are\nmore feature maps total, but each is smaller. It’s not so clear what\nthese feature maps correspond to, but there do seem to be a few that are\nclearly activated within the dog, and others that are sensitive to the\nfoliage in the background.\n\n\n\npar(mfrow = c(6, 7), mai = rep(0.00, 4))\nout <- features[[6]][ix,,,1:40] %>%\n  array_branch(margin = 3) %>%\n  map(~ plot_feature(.))\n\n\n\n\nFigure 4: Activations for a collection of neurons at layer 6.\n\n\n\n\nWhile we had some interpretations for these higher-level features, it’s\nhard to know definitively, since we are only considering a single image.\nIn the next set of notes, we will examine the same neuron across many\ndataset examples, and this will give us more confidence in our\ninterpretations of individual neurons.\n\n\n\n\n\n\nFor those who are curious, the training code is\nhere).\nOf course, you will not be responsible for understanding this\nstep.↩︎\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T17:34:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week13-3/",
    "title": "Collections of Features",
    "description": "Analyzing feature activations across datasets",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-09",
    "categories": [],
    "contents": "\nReading (1, 2), Recording, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(keras)\nlibrary(magrittr)\nlibrary(pdist)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(superheat)\nlibrary(tidymodels)\nset.seed(479)\n\n\nThe previous notes gave us a look into the features learned by a deep\nlearning model. However, we could only look at one feature within one layer at a\ntime. We also only studied an individual image. If we want to better understand\nthe representations learned by a network, we will need ways of analyzing\ncollections of features taken from throughout the network, across entire\ndatasets.\nThis seems like an impossible task, but it turns out that, in real-world\nmodels, the learned features tend to be highly correlated. Certain patterns of\nactivation tend to recur across similar images. This kind of structure makes it\npossible to use clustering and dimensionality-reduction to begin to make sense\nof the learned representations of individual networks.\nTo illustrate this idea, we will download the same model from before along\nwith a larger subsample of images used in training.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/dxibamcr0bcmnj7xazqxnod8wtew70m2.rda\", f)\nimages <- get(load(f))\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\n\n\nThe code block below save features from layers 6 and 8 from this model, for\nall the images we downloaded. The code for extracting features is the same as\nfrom the previous lecture, except instead of extracting features from all\nlayers, we’re only considering these later ones. The reason we’ve focused on\nthese deeper layers is that (1) they are smaller, so we will consume less memory\non our computers, and (2) they correspond to the higher-level concepts which are\nmore difficult to understand directly, unlike the simple edge detectors in the\ninitial layers.\n\n\nl <- c(model$layers[[6]]$output, model$layers[[8]]$output)\nactivation_model <- keras_model(inputs = model$input, outputs = l)\nfeatures <- predict(activation_model, images)\n\n32/32 - 2s - 2s/epoch - 57ms/step\n\nideally, we could work with a matrix of samples by features. the \\(ij^{th}\\)\nelement would be the activation of feature \\(j\\) on observation \\(i\\).\nthis is unfortunately not immediately available. as we saw before, each\nfeature map is actually a small array across spatial contexts, not a single\nnumber. there is no single way to aggregate across a feature map, and it is\ncommon to see people use the maximum, average, norm, or variance of a feature\nmap as a summary for how strongly that feature activates on a given image. we\nwill take the mean of activations.\n\n\nfeature_means <- function(h) {\n  apply(h, c(1, 4), mean) %>%\n    as_tibble()\n}\n\nh <- map_dfc(features, feature_means) %>%\n  set_colnames(str_c(\"feature_\", 1:ncol(.))) %>%\n  mutate(id = row_number())\n\n\nGiven this array, we can ask questions like, which neurons are most activated\nfor a particular image? or, which images induce the largest activations for a\nparticular neuron? In the block below, we find the 20 images that activate the\nmost for the third feature map in layer 6. This neuron seems to have learned to\nrecognize grass. Perhaps unsurprisingly, all the images are of dogs.\n\n\ntop_ims <- h %>%\n  slice_max(feature_3, n = 20) %>%\n  pull(id)\n\npar(mfrow = c(5, 4), mai = rep(0.00, 4))\nout <- images[top_ims,,,] %>% \n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\nFigure 1: The 20 images in the training dataset with the highest activations for neuron 3 in layer 6. This neuron seems to be sensitive to the presence of grass in an image (which happens to be correlated with whether a dog is present).\n\n\n\nThis particular example should serve as a kind of warning. While it’s easy to\nimbue models with human-like characteristics, they often arrive at the answers\nthey need in unexpected ways. We asked the model to distinguish between cats and\ndogs, but it is using whether the image has grass in it as a predictor. While\nfor this dataset this may be accurate, I would expect this model to fail on an\nimage of a cat in a grassy field.\nInstead of only investigating one neuron, we can consider all the images and\nneurons simultaneously. The code below makes a heatmap of the average feature\nactivations from before. Each row is an image and each column is a feature from\neither layers 6 or 8. A similar example is given in the reading, where\ncoordinated views reveal that certain patterns of neuron activation encode the\nlifts of the pen or specific curve shapes in a handwriting generation network.\n\n\nsuperheat(\n  h %>% select(-id),\n  pretty.order.rows = TRUE,\n  pretty.order.cols = TRUE,\n  legend = FALSE\n)\n\n\n\nFigure 2: A heatmap of feature map activations for layers 6 and 8, across the entire dataset. Each row is an image, and each column is a neuron. There is limited clustering structure, but there are substantial differences in how strongly different neurons activate on average.\n\n\n\nWe can also apply clustering and dimensionality reduction ideas to understand\nthe collection of mean feature activations. For example, below we run \\(K\\)-means\nacross images, with the hope of finding images that are viewed similarly\naccording to the model. The 20 images closest to the centroid of cluster 3 are\nprinted below. It seems like the model has learned to cluster all the orange\nimages together1. This\nis a little surprising, considering that there are both cats and dogs that are\norange, so this isn’t a particularly discriminating feature. It also suggests a\nway to improve the model – we could train it on recolorized input images, so\nthat it is forced to discover features that are unrelated to color2.\n\n\nsub <- function(x) {\n  select(x, starts_with(\"feature\"))\n}\n\ncluster_result <- kmeans(sub(h), centers = 25, nstart = 20)\ncentroids <- tidy(cluster_result)\nD <- pdist(sub(centroids), sub(h)) %>%\n  as.matrix()\n\npar(mfrow = c(5, 4), mai = rep(0.00, 4))\nnear_centroid <- order(D[3, ])[1:20]\nout <- images[near_centroid,,, ] %>%\n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\nFigure 3: The 20 images closest to the centroid of cluster 3 in the feature activation space. This cluster seems to include images with many orange pixels.\n\n\n\nIn general, we can treat the activations generated by a deep learning model\nas themselves the object of data analysis. This can help us determine whether\nthe kinds of features that we want it to (high-level concepts, rather than just\ncolors or textures). It can also highlight instances where the model learns\nfeatures associated with concepts that we would rather it be invariant to (e.g.,\nchanges in season, variations in lighting).\n\nSometimes, it’s just looking at the color of the floor!↩︎\nIn the deep\nlearning community, this would be called using color augmentation to enforce\ninvariance.↩︎\n",
    "preview": "posts/2024-12-27-week13-3/week13-3_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2025-08-19T17:35:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week13-4/",
    "title": "Optimizing Feature Maps",
    "description": "Interpreting neurons by finding optimal inputs",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-08",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(\"dplyr\")\nlibrary(\"purrr\")\nlibrary(\"keras\")\nlibrary(\"tensorflow\")\n\n\n\n\nSo far, we’ve visualized neural networks by analyzing the activations of\nlearned features across observed samples. A complementary approach is to\nask instead — is there a hypothetical image that would maximize the\nactivation of a particular neuron? If we can construct such an image,\nthen we might have a better sense of the types of image concepts to\nwhich a neuron is highly sensitive.\n\n\n\n\nWe will illustrate these ideas on a network that has been trained on\nImagenet. This is a large image dataset with many (thousands of) class\nlabels, and it is often used to evaluate image classification\nalgorithms. The network is loaded below.\n\n\n\n\nmodel <- application_vgg16(weights = \"imagenet\", include_top = FALSE)\n\n\n\nThe main idea is to setup an optimization problem that searches through\nimage space for an image that maximizes the activation for a particular\nneuron. The function below computes the average activation of a one of\nthe feature maps. The goal is to find an image that maximizes this value\nfor a given feature.\n\n\n\nmean_activation <- function(image, layer, ix=1) {\n  h <- layer(image)\n  k_mean(h[,,, ix])\n}\n\n\n\nTo implement this, we can compute the gradient of a neuron’s average\nactivation with respect to input image pixel values. This is a measure\nof how much the activation would change when individual pixel values are\nperturbed. The function below moves an input image in the direction of\nsteepest ascent for the mean_activation function above.\n\n\n\ngradient_step <- function(image, layer, ix=1, lr=1e-3) {\n  with(tf$GradientTape() %as% tape, {\n    tape$watch(image)\n    objective <- mean_activation(image, layer, ix)\n  })\n  grad <- tape$gradient(objective, image)\n  image <- image + lr * grad\n}\n\n\n\n\n\n\nFigure 1: Starting from a random image, we can take a gradient step in\nthe image space to increase a given neuron’s mean activation.\n\n\n\n\nOnce these gradients can be computed, it’s possible to perform gradient\nascent to solve the activation maximization problem. This ascent is\nencoded by the function below. We initialize with a random uniform image\nand then take n_iter gradient steps in the direction that\nmaximizes the activation of feature ix.\n\n\n\nrandom_image <- function() {\n  tf$random$uniform(map(c(1, 150, 150, 3), as.integer))\n}\n\ngradient_ascent <- function(layer, ix = 1, n_iter = 100, lr = 10) {\n  im_seq <- array(0, dim = c(n_iter, 150, 150, 3))\n  image <- random_image()\n  for (i in seq_len(n_iter)) {\n    image <- gradient_step(image, layer, ix, lr)\n    im_seq[i,,,] <- as.array(image[1,,,])\n  }\n  \n  im_seq\n}\n\n\n\n\n\n\nFigure 2: Taking many gradient steps leads us towards an image that\noptimizes a neuron’s activation.\n\n\n\n\nBelow, we visualize the images that optimize the activations for a few\nneurons in layer 3. These neurons seem to be most responsive particular\ncolors and edge orientations.\n\n\n\nsquash <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\npar(mfrow = c(5, 8), mai = rep(0.00, 4))\nactivation_model <- keras_model(inputs = model$input, outputs = model$layers[[3]]$output)\nfor (i in seq_len(40)) {\n  im_seq <- gradient_ascent(activation_model, ix = i)\n  plot(as.raster(squash(im_seq[100,,,])))\n}\n\n\n\n\nFigure 3: The hypothetical images that maximize the activations for 40\ndifferent neurons. These neurons seem to pull out features related to\ncolor and edge orientations.\n\n\n\n\n\nWe can think of these features as analogous to a collection of basis\nfunctions. At the first layer, the network is representing each image as\na combination of basis images, related to particular color or edge\npatterns.\n\n\n\n\nWe can compare these activation maximizing inputs with those associated\nwith later layers. It seems that the basis images at this level are more\nintricate, reflecting textures and common objects across this dataset.\nFor example, the polka dot pattern may be strongly activated by cat\neyes.\n\n\n\n\npar(mfrow = c(5, 8), mai = rep(0.00, 4))\nactivation_model <- keras_model(inputs = model$input, outputs = model$layers[[8]]$output)\nfor (i in seq_len(40)) {\n  im_seq <- gradient_ascent(activation_model, ix = i)\n  plot(as.raster(squash(im_seq[100,,,])))\n}\n\n\n\n\nFigure 4: The results of the corresponding optimization for 40 neurons\nin layer 8.\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-19T17:35:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week14-1/",
    "title": "Final Takeaways",
    "description": "Some major themes from STAT 436, in a nutshell.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-07",
    "categories": [],
    "contents": "\nRecording, Rmarkdown\nWe’ve covered many practical visualization strategies in this course.\nHowever, I hope a few overarching themes have come across as well. In these\nnotes, I will try articulating a few of these themes, and they are also the\nsubject of the readings reviewed in the next few notes.\nFirst, creating a visualization is in many ways like writing an essay. A\ngreat visualization cannot simply be summoned on demand. Instead, visualizations\ngo through drafts, as the ideal graphical forms and questions of interest become\nclearer.\nVisualization is not simply for consumption by external stakeholders. In data\nscience, visualization can be used to support the process of exploration and\ndiscovery, before any takeaways have yet been found.\nVisualization can be used throughout the data science process, from data\nquality assessment to model prediction analysis. In fact, I would be wary of any\ndata science workflow that did not make use of visualization at multiple\nintermediate steps.\nIt pays dividends to think carefully about data format. A design can be easy\nto implement when the data are in tidy, but not wide, format, and vice versa.\nStructured, nontabular data – think time series, spatial formats, networks,\ntext, and images – are everywhere, and specific visualization idioms are\navailable for each.\nSimilarly, high-dimensional data are commonplace, but a small catalog of\ntechniques, like faceting, dynamic linking, PCA, and UMAP, are able to take us\nquite far.\nFinally, data visualization can be enriching. Visual thinking has helped me\nnavigate and appreciate complexity in many contexts. Through this course, I have\nshared techniques that I’ve found useful over the years, and I hope you find\nthem handy as you go off and solve real-world data science problems.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-08-13T16:42:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week14-2/",
    "title": "Design Process Case Study",
    "description": "Tracing the refinement of questions and design.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-06",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nHow does a visualization expert go about creating a data visualization?\nPerhaps the most useful lesson from this reading is that good visualizations\ndon’t materialize out of thin air – there is always a creative process\ninvolved, steps where it’s unclear what the final result will be, even for a\ndata visualization genius like Shirley Wu. We’re lucky that she has documented\nthis process for us, so that we might be able to take away a few lessons for our\nown reflection.\nHer visualization, “655 Frustrations of Data\nVisualization”, is based on an online\ndata visualization survey. It had 45 questions (“How many years have you been\ndoing data visualization? What percent of your day is focused on data prep work?\n…). There are 981 responses, probably mostly submitted by the survey\ninitiator’s internet following.\n\n\n\nFigure 1: A few entries from the data visualization survey. The full data are publicly available here.\n\n\n\nQuestion Formulation\nAt the start of the project – before writing code – there is the problem of\nchoosing a guiding question. Her initial question was “Why might people leave\nthe field?” This was a timely question, because, after a few years of high\nactivity and visibility in industry, data visualization seemed to be cooling\ndown. However, this question was not directly answerable with the data at hand,\nso instead, she focused on the proxy question, “Do you want to spend more time\nor less time visualizing data in the future?”\nThis is an important lesson: there is often a distinction between what we\nreally want to know and what the data can tell us.\nWe will revisit this theme of asking sharper questions in the next reading.\nData analysis should be driven by curiosity about the world, not simply the\ndata that happen to be conveniently accessible.\n\nTo see what within the data are relevant to this guiding question, she then\nconducted an exploratory analysis1, studying the marginal\ndistributions of all the available questions. For example, visualizing the\n“percentage of time” questions, it became clear that most people worked on a mix\nof multiple data-related tasks in their work – the particular mix might help\nunderstand whether people want to stay in the field.\n\n\n\nFigure 2: Example exploratory displays of the ‘percentage of time’ questions on the data science survey.\n\n\n\nThe exploratory analysis also revealed that there are some\nquestions\nthat would not be useful for answering the guiding question. For example, many\nof the qualitative responses were not useful2. It’s easy to feel responsible for\nvisualizing all the data that are available, but this is not necessary. It’s far\nmore important to focus on the guiding question(s).\nInitial Design\nThe initial design answered whether there is a relationship between (a) the\nsurvey respondent wanting to do more data visualization in the future, and (b)\nthe current fraction of time spent on design. This was visually encoded using a\nstacked barchart. However, the display was not that informative, because most\nrespondents wanted to do more data visualization in the future3.\n\n\n\nFigure 3: The initial design used a bar chart to see whether experience was related to interest in further work in data visualization.\n\n\n\nIn this situation, it seemed like perhaps additional context would help.\nHowever, the resulting faceted barchart was difficult to make sense of, again\nbecause any relationships between variables were weak or nonexistent.\n\n\n\nFigure 4: The faceted barchart did not add much information relevant to the guiding question.\n\n\n\nRedesigns\nAt this point, two significant changes to the design were made, one to the\nquestion, and one to the design. The question was reframed from “do you want to\ncontinue working in data visualization” to “do you experience any frustrations\nwith data visualization.” This question is still related to the guiding\nquestion, but shows much more variation across respondents. For the design, the\nencoding changed from a stacked barchart to a beeswarm plot. Unlike the\nbarchart, which aggregates responses into bins, the beeswarm makes it possible\nto see every single respondent.\n\n\n\nFigure 5: A redesigned plot. The left and right panels separate respondents with and without frustrations, vertical position encodes job role, and color gives number of years of experience.\n\n\n\nA few more refinements were made. Instead of placing those with and without\nfrustrations far apart on the page, they were rearranged to share the same\n\\(x\\)-axis4. Also, instead of coloring circles by\nyears of experience, color was used to represent the percentage of the day spent\non data visualization. Again, these changes reflect sharpening of both design\nand questions.\nIn the final version of the static display, a boxplot was introduced to\nsummarize the most salient characteristics of each beeswarm. Then, instead of\njust plotting the points in two parallel regions, they were made to “rise” and\n“fall” off the boxplots, depending on whether the respondents experienced\nfrustrations. This kind of visual metaphor takes the visualization to another\nlevel; it becomes more than functional, it becomes evocative.\n\n\n\nFigure 6: The final version of the static visualization.\n\n\n\nInterpretation\nOnly at this point is interactivity introduced into the visualization. The\ninteractivity is simple – views transition into one another depending on\nselected questions – but provides an effective alternative to simply faceting\nall pairs of questions.\n\n\n\nFigure 7: The visualization above with interactivity added in.\n\n\n\nFinally, this interactive visualization is used for an extensive\nexploration. This is often when the effectiveness of a visualization can be\nevaluated. Ultimately, visualization should help inform our body of beliefs,\nguiding the actions we take (either in the short or long-term). If it’s hard to\ndraw these sorts of inferences, then a visualization is not particularly\nfunctional.\n\n\n\nFigure 8: A screenshot of notes from the designer’s exploration of the resulting visualization.\n\n\n\nTo guide the reader, this investigative work was then incorporated into the\nvisualization. These additional details allow the visualization to stand alone,\nit becomes a self-explanatory intellectual artifact.\nConclusion\nWrapping up, the final visualization is clearly the culmination of\nsubstantial intellectual labor over the course of weeks (if not months). The\nresult is both beautiful and informative. This is an ideal to strive for – the\ncrafting of data visualizations that can guide discovery and change.\nOne final note. It’s often useful to study the development of projects that\nyou find interesting. Sometimes, authors share their code on github, or earlier\nversions are available through technical reports or recorded talks. This\nadditional context can shed light on the overall inspiration and intention of\nthe project, and especially when starting out, imitation can be an effective\nstrategy for learning.\n\nUsing vega-lite!↩︎\nAre there really 100x more pie\ncharts than scatterplots in industry??↩︎\nPerhaps an\ninstance of selection bias.↩︎\nThe less our eyes have to travel across the page to make a comparison,\nthe more efficient the visualization.↩︎\n",
    "preview": "https://uploads-ssl.webflow.com/5fa9d88da1855554db37b1e1/5fc61cc4356e0d312965c372_split_beeswarm.png",
    "last_modified": "2025-08-13T16:42:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week14-3/",
    "title": "Asking Better Questions",
    "description": "What is the purpose of data analysis?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-05",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nIn these notes, we will review “Tukey, Design Thinking, and Better Questions,”\na blog post by Roger Peng. In the post, he discusses the importance of\nformulating good questions in data analysis. While the process of refining\nquestions is central to any successful, real-world analysis, it is rarely\ndiscussed in formal statistics textbooks, which usually restrict themselves to\ndevelopments in theory, methodology, or computation.\nThe post itself is based on a line of thinking from the statistician John\nTukey’s 1962 paper, “The Future of Data Analysis.” Perhaps the most famous\nquote from this paper is,\n\nFar better an approximate answer to the right question, which is often vague,\nthan an exact answer to the wrong question, which can always be made precise.\n\nand throughout the article, Tukey argues that data analysts should not (1) be\ncalled upon to provide stamps of authority and (2) be distracted by the allure\nof proving optimal algorithms in settings that are likely not that realistic.\nThis of course begs the question – what should data analysts be doing? Tukey\nnever directly discusses this, but Peng’s thesis is that the purpose of data\nanalysis is to spark the formulation of better questions. This idea is captured\nin the diagram below. The idea is that algorithms (visual, predictive, and\ninferential) can help us strengthen evidence that certain patterns exist.\nHowever, if those patterns answer irrelevant questions, then there is little\npoint in the analysis. With so many algorithms available, it’s easy to spend\nsubstantial effort gathering evidence to answer low quality questions. Instead,\na good data scientist should focus on improving the quality of the questions\nasked, even if that means that the answers will be less definitive.\n\n\n\nFigure 1: Peng’s summary of misplaced priorities in the data science process.\n\n\n\nWhat does it mean to ask better questions? Peng argues that better questions\nare “sharper,” providing more discriminating information and guiding research to\nmore promising directions. In my view, a sharper question is one that helps\ninform decisions that have to be made (all data aside), either by adding to our\nbody of beliefs or bringing new uncertainties into focus.\nThe implication is that data analysis is not about finding “the answer.”\nInstead, it should be about clarifying what can and cannot be answered based on\nthe data. But more, it can help clarify what questions we really want answered\n– in Peng’s words, “we can learn more about ourselves by looking at the data.”\nThe final example in the reading is that the residuals in an analysis are\nthemselves data worth examining. They inform whether a model (and the associated\nway of conceptualizing the problem) is really appropriate, or whether a\nreformulation would be beneficial.\nAt the end, the main idea is that data analysis equips us to ask better\nquestions.\n\n\n\n",
    "preview": "https://github.com/krisrs1128/stat436_f24/blob/main/docs/figure/question_evidence.png?raw=true",
    "last_modified": "2025-08-13T16:42:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week14-4/",
    "title": "A History of Data Visualization up to 1900",
    "description": "A look at the origins of the field.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-11-04",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nThere could probably be an entire class taught on the history of data\nvisualization. The reason it is worth covering in an otherwise\npractically-oriented class is that,\nHistorical study can illuminate the core intellectual foundation on which the\nentire discipline of data visualization is built.\nThe limits, biases, and trends of our current era become clear when\nconsidering it within its full historical context.\nIt’s often possible to draw inspiration from past masters.\nIt can be humbling to realize that now-commonplace ideas had to be discovered.\nIf it took until 1833 until the first scatterplot was published, then what ideas\nhave we yet to find?\n\nThe reading divides up the history of data visualization into 8 epochs. In\nthese notes, we will consider the 5 epochs before 1900.\nBefore 1600: Early Maps and Diagrams\nIt might come as a surprise, but visualization has been around since the\ninvention of writing. The Egyptians made maps, and there are examples multiple\ntime series plots from the 10th century.\n\n\n\nFigure 1: Possibly the earliest multiple time series visualization, showing the movement of various planets, from In Somnium Scripionus.\n\n\n\nHowever, most visualizations were focused on physical geographical or\nastronomical quantities, and even these quantities were only imprecisely\nmeasured. Without more formal data gathering instruments, there could not be\nmuch data visualization.\n1600 - 1700: Measurement and Theory\nThe situation begins to change around 1600, at the dawn of the scientific\nrevolution. All of a sudden, precise measurement of physical space become\npossible. Also, important new mathematical ideas were introduced, like\nprobability, calculus, and reasoning about functions. This created the right\nenvironment for the design of some of the first truly sophisticated data\nvisualizations, like\nChristopher Scheiner’s 1630 visualization of sunspots over\ntime (a first instance of faceting),\n\n\n\nFigure 2: Scheiner’s visualization of sunspots, which played a role in his dialogues with Galileo.\n\n\n\nEdmond Halley’s plot of barometric pressure against altitude (an first\ninstance of one feature being plotted against another), and\n\n\n\nFigure 3: One of the first bivariate plots, relating barometric pressure to altitude. Note that absence of the true observed data.\n\n\n\nEdmond Halley’s plot of wind speed over the ocean (a first visualization of a\nvector field).\n\n\n\nFigure 4: A plot of trade winds, appearing in An Historical Account of the Trade Winds, and Monsoons, Observable in the Seas between and near the Tropicks, with an Attempt to Assign the Phisical Cause of the Said Wind.\n\n\n\nDuring this period, the scientific value of visual thinking was more or less\nestablished. However, there were still only relatively few graphical forms\navailable, and the focus continued to remain on visualizing physical quantities,\nrather than more general social, economic, biological, or ecological data.\n1700 - 1800\nDuring this period, many new graphical forms were invented, including\ntimelines, cartograms, functional interpolations, line graphs, bar charts, and\npie charts. Further, forms from earlier, like maps and function plots, became\nmore firmly established.\nThis was also when three-color printing was invented. Before this point,\ncolor could not be used as an encoding channel.\nIn this century, governments also began large-scale data collection of social\nand economic statistics1. One of\nthe most prolific inventors of data visualizations, William Playfair2, was used\nvisualization to study a variety of economic problems. In addition to inventing\nline, bar, and pie charts, he experimented with original ways of composing\nmultiple graphs to suggest the relationships between variables.\n\n\n\nFigure 5: One of William Playfair’s data visualizations, juxtaposing the price of wheat with growth in wages.\n\n\n\n1800 - 1850\nThis was a period of maturation for the field of data visualization. By this\npoint, visualization had become standard in scientific publications. Advances in\nprinting technology also made it also became easier to mass produce\nvisualizations.\nIt was also around this time that the scope of problems studied through\nvisualization expanded far beyond display of purely physical (geographical and\nastronomical) applications. Two areas in particular flourished, applications to\nsocial science and to engineering.\nVisualization in the social sciences began to emerged in response to\ngovernment sponsored collection of social statistics – data about crime,\nbirths, and deaths, among other topics. This wasn’t a purely intellectual\nexercise: understanding demographic trends was important for countries that\nwere often at war with another.\n\n\n\nFigure 6: A visualization of property crime statistics, by Andre-Michel Guerry (1829).\n\n\n\nIn engineering, the idea that visualization could serve as a computational\naid become more and more common. For example, rather the chart below, by Charles\nJoseph Minard, displays the cost of transporting goods across different\nstretches of a canal. Vertical breaks correspond to cities along the canal, and\nthe area of the square between cities encodes the cost of transportation between\nthose cities. While the information could be stored in a table, it becomes\neasier to perform mental computations (and make guesstimates) using the display.\n\n\n\nFigure 7: Minard’s 1844 visualization of the transport costs across the Canal du Centre in France.\n\n\n\n1850 - 1900: The Golden Age\nIt might be counterintuitive that there was a golden age of visualization a\ncentury before the first computers were invented. However, a look at the\nvisualizations from this period demonstrate that this was a period where\nvisualizations inspired scientific discoveries, informed commercial decisions,\nand guided social reform.\nFor example, in public health, Florence Nightingale invented new\nvisualizations to demonstrate the impact of sanitary practices in\nhospital-induced infections and death. Similarly, it was a\nvisualization that guided John\nSnow to the source of the 1855 cholera epidemic.\n\n\n\nFigure 8: Florence Nightingale’s visualization of hospital mortality statistics from the Crimean War, used to support a campaign for sanitary reforms.\n\n\n\nSome of the graphical innovations include,\n3D function plots. The plot below, by Luigi Perozzo, shows population size\nbroken down into age groups and traced over time.\n\n\n\nFigure 9: Luigi Perrozo’s 1879 3D visualizations of population over time.\n\n\n\nFlow diagrams. Charles-Joseph Minard, who we met before with the canal\nvisualization, was a master of these displays. One in particular is widely\nconsidered a masterpiece, it shows the size of Napoleon’s army during it’s\nRussian Campaign.\n\n\n\nFigure 10: Minard’s flow display of the size of Napoleon’s army during the Russia Campaign.\n\n\n\nMultivariate visualization. Francis Galton made some of the first efforts to\nvisualize more than 3 variables at a time. His Meteorographica, published in\n1863, contained over 600 visualizations of weather data that had been collected\nfor decades, but never visualized. One plot, shown below, led to the discovery\nof anticyclones.\n\n\n\nFigure 11: Galton’s display of weather patterns. Low pressure (black) areas tend to have clockwise wind patterns, while high pressure (red) tends to have anticlockwise wind patterns.\n\n\n\nThis was also an age of state-sponsored atlases. More than sponsoring the\ncollection of data, governments assembled teams to visualize the results for\nofficial publication. From 1879 to 1897, the French Ministry of Public Works\npublished the Albums de Statistique Graphique, which under the guidance of\nÉmile Cheysson, developed some of the most imaginative and ambitious\nvisualizations of the era.\n\n\n\nFigure 12: A visualization of the flow of passengers and goods through railways from Paris. Each square shows the breakdown to cities further away, and color encodes the railines.\n\n\n\n\nThe word “statistics” comes from the same root as\n“state”, since it originally focused on data collected for governance.↩︎\nHe has\nbeen described as an “engineer, political economist and scoundrel.”↩︎\n",
    "preview": "https://uwmadison.box.com/shared/static/c79fbmfc3mff9ota1tzxtvv4gp7e3pul.jpeg",
    "last_modified": "2025-08-13T16:42:39-05:00",
    "input_file": {}
  }
]
