[
  {
    "objectID": "documents.html",
    "href": "documents.html",
    "title": "STAT 436 Notes",
    "section": "",
    "text": "Introduction to ggplot2\n\n\nA discussion of ggplot2 terminology, and an example of iteratively refining a simple scatterplot.\n\n\n\n\n\n\n\n\n\n\n\nA Vocabulary of Marks\n\n\nExamples of encodings and sequential refinement of a plot.\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Dimensionality Reduction\n\n\nExamples of high-dimensional data.\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Components Analysis I\n\n\nLinear dimensionality reduction using PCA.\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Components Analysis II\n\n\nVisualizing and interpreting PCA.\n\n\n\n\n\n\n\n\n\n\n\nUniform Manifold Approximation and Projection\n\n\nAn overview of the UMAP algorithm.\n\n\n\n\n\n\n\n\n\n\n\nPCA and UMAP Examples\n\n\nMore examples of dimensionality reduction using PCA and UMAP.\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Topic Models\n\n\nAn overview of dimensionality reduction via topics.\n\n\n\n\n\n\n\n\n\n\n\nFitting Topic Models\n\n\nData preparation and model fitting code for topics.\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Topic Models\n\n\nOnce we’ve fit a topic model, how should we inspect it?\n\n\n\n\n\n\n\n\n\n\n\nTopic Modeling Case Study\n\n\nAn application to a gene expression dataset.\n\n\n\n\n\n\n\n\n\n\n\nPartial Dependence Profiles I\n\n\nAn introduction to partial dependence profiles.\n\n\n\n\n\n\n\n\n\n\n\nPartial Dependence Profiles II\n\n\nDiscovering richer structure in partial dependence profiles.\n\n\n\n\n\n\n\n\n\n\n\nVisualization for Model Building\n\n\nThe relationship between exploratory analysis and model development.\n\n\n\n\n\n\n\n\n\n\n\nPrior and Posterior Predictives\n\n\nSimulating data to evaluate model quality.\n\n\n\n\n\n\n\n\n\n\n\nPointwise Diagnostics\n\n\nEvaluating the fit at particular observations in Bayesian models.\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Feature Learning\n\n\nAn introduction to compositional feature learning.\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Learned Features\n\n\nA first look at activations in a deep learning model.\n\n\n\n\n\n\n\n\n\n\n\nCollections of Features\n\n\nAnalyzing feature activations across datasets\n\n\n\n\n\n\n\n\n\n\n\nOptimizing Feature Maps\n\n\nInterpreting neurons by finding optimal inputs\n\n\n\n\n\n\n\n\n\n\n\nFinal Takeaways\n\n\nSome major themes from STAT 436, in a nutshell.\n\n\n\n\n\n\n\n\n\n\n\nDesign Process Case Study\n\n\nTracing the refinement of questions and design.\n\n\n\n\n\n\n\n\n\n\n\nAsking Better Questions\n\n\nWhat is the purpose of data analysis?\n\n\n\n\n\n\n\n\n\n\n\nA History of Data Visualization up to 1900\n\n\nA look at the origins of the field.\n\n\n\n\n\n\n\n\n\n\n\nTidy Data\n\n\nThe definition of tidy data, and why it’s often helpful for visualization._\n\n\n\n\n\n\n\n\n\n\n\nPivoting\n\n\nTools for reshaping data into tidy format.\n\n\n\n\n\n\n\n\n\n\n\nDeriving Variables\n\n\nUsing separate, mutate, and summarise to derive new variables for downstream visualization.\n\n\n\n\n\n\n\n\n\n\n\nTidy Data Example\n\n\nAn extended example of tidying a real-world dataset.\n\n\n\n\n\n\n\n\n\n\n\nRidge Plots\n\n\nAn extended example of faceting with data summaries.\n\n\n\n\n\n\n\n\n\n\n\nCompound Figures\n\n\nShowing different variables across subpanels.\n\n\n\n\n\n\n\n\n\n\n\nPatchwork\n\n\nImplementing compound figures in R\n\n\n\n\n\n\n\n\n\n\n\nElements of a Shiny App\n\n\nVocabulary used by the R Shiny Library, and a few example apps.\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Reactivity\n\n\nViewing shiny code execution as a graph.\n\n\n\n\n\n\n\n\n\n\n\nIMDB Shiny Application\n\n\nUsing Shiny to explore a movies dataset\n\n\n\n\n\n\n\n\n\n\n\nGraphical Queries - Click Events\n\n\nAn introduction to click events in Shiny\n\n\n\n\n\n\n\n\n\n\n\nGraphical Queries - Brush Events\n\n\nAn introduction to brush events in Shiny.\n\n\n\n\n\n\n\n\n\n\n\nLinked Brushing\n\n\nMore examples defining brush queries using Shiny and ggplot2.\n\n\n\n\n\n\n\n\n\n\n\nLinking using Crosstalk\n\n\nLinking in web-based visualizations.\n\n\n\n\n\n\n\n\n\n\n\ntsibble Objects\n\n\nA data structure for managing time series data.\n\n\n\n\n\n\n\n\n\n\n\nTime Series Patterns\n\n\nVocabulary for describing visual structure in time series.\n\n\n\n\n\n\n\n\n\n\n\nSeasonal Plots\n\n\nApproaches for visualizing seasonality.\n\n\n\n\n\n\n\n\n\n\n\nCross and Auto-Correlation\n\n\nSummaries of relationships between and within time series.\n\n\n\n\n\n\n\n\n\n\n\nCollections of Time Series\n\n\nNavigating across related time series.\n\n\n\n\n\n\n\n\n\n\n\nVector Data\n\n\nManipulating and visualizing spatial vector data.\n\n\n\n\n\n\n\n\n\n\n\nRaster Data\n\n\nStoring spatially gridded information in rasters.\n\n\n\n\n\n\n\n\n\n\n\nCoordinate Reference Systems\n\n\nThe projection problem, and how to check your CRS.\n\n\n\n\n\n\n\n\n\n\n\nGeospatial Interaction\n\n\nIdioms for interacting with geographic data.\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Networks and Trees\n\n\nTypical tasks and example network datasets.\n\n\n\n\n\n\n\n\n\n\n\nNode - Link Diagrams\n\n\nThe most common network visualization strategy.\n\n\n\n\n\n\n\n\n\n\n\nAdjacency Matrix Views\n\n\nA scalable network visualization strategy.\n\n\n\n\n\n\n\n\n\n\n\nEnclosure\n\n\nVisualization of hierarchical structure using containment.\n\n\n\n\n\n\n\n\n\n\n\nK-means\n\n\nAn introduction to clustering and how to manage its output.\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Clustering\n\n\nClustering data at multiple scales using trees.\n\n\n\n\n\n\n\n\n\n\n\nHeatmaps\n\n\nVisualizing table values, ordered by clustering results.\n\n\n\n\n\n\n\n\n\n\n\nSilhouette Statistics\n\n\nDiagnostics for the quality of a clustering.\n\n\n\n\n\n\n\n\n\n\n\nCluster Stability\n\n\nHow reliable are the results of a clustering?\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Formats\n\n\nAn overview of common formats, with illustrative examples.\n\n\n\n\n\n\n\n\n\n\n\nFaceting\n\n\nUsing small multiples to create information dense plots.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/6-2.html",
    "href": "content/6-2.html",
    "title": "Time Series Patterns",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(tsibbledata)\nlibrary(fpp2)\ntheme_set(theme_minimal())\n\n\nThere are a few structures that are worth keeping an eye out for whenever you plot a single time series. We’ll review the main vocabulary in these notes.\nVocabulary\n\nTrend: A long-run increase or decrease in a time series.\nSeasonal: A pattern that recurs over a fixed and known period.\nCyclic: A rising and falling pattern that does not occur over a fixed or known period.\n\n\nA few series with different combinations of these patterns are shown below.\n\nggplot(as_tsibble(qauselec)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Australian Quarterly Electricity Production\")\n\n\n\n\n\n\n\nggplot(as_tsibble(hsales)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Housing Sales\")\n\n\n\n\n\n\n\nggplot(as_tsibble(ustreas)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"US Treasury Bill Contracts\")\n\n\n\n\n\n\n\nggplot(as_tsibble(diff(goog))) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Google Stock Prices\")\n\n\n\n\n\n\n\n\n\nA series can display combinations of these patterns at once. Further, the same data can exhibit different patterns depending on the scale at which it is viewed. For example, though a dataset might seem seasonal at short time scales, a long-term trend might appear after zooming out. This is visible in the electricity production plot above.\nFinally, it’s worth keeping in mind that real-world structure can be much more complicated than any of these patterns. For example, the plot below shows the number of passengers on flights from Melbourne to Sydney between 1987 and 1992. You can see a period when no flights were made and a trial in 1992 where economy seats were switched to business seats.\n\n\nmelbourne_sydney &lt;- ansett %&gt;%\n  filter(Airports == \"MEL-SYD\") # challenge: facet by Airports, instead of filtering\n\nggplot(melbourne_sydney) +\n  geom_line(aes(x = Week, y = Passengers, col = Class))"
  },
  {
    "objectID": "content/6-3.html",
    "href": "content/6-3.html",
    "title": "Seasonal Plots",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(feasts)\nlibrary(fpp2)\nlibrary(tsibbledata)\ntheme_set(theme_minimal())\n\n\nIf our data have seasonal structure, it’s natural to compare individual periods against one another. In contrast to plotting the data in one long time series, seasonal plots reduce the amount of distance our eyes have to travel in order to compare two periods in a seasonal pattern. This also reduces the burden on our memory.\nIn R, we can use the gg_season function to overlay all the seasons onto one another. The plot below shows antidiabetes drug sales over time. This view makes it clear that there is a spike in sales every January.\n\n\ncols &lt;- scales::viridis_pal()(10)\ngg_season(as_tsibble(a10), pal = cols)\n\n\n\n\n\n\n\n\n\nIf the time series exhibit seasonal structure at multiple scales, then we can view them all using the period argument.\n\n\ngg_season(vic_elec, Demand, period = \"day\", pal = cols)\n\n\n\n\n\n\n\ngg_season(vic_elec, Demand, period = \"week\", pal = cols)"
  },
  {
    "objectID": "content/4-3.html",
    "href": "content/4-3.html",
    "title": "IMDB Shiny Application",
    "section": "",
    "text": "Recording, Code\n\nSo far, all of our Shiny applications have been based on toy simulated data. In this set of notes, we’ll use Shiny to explore a real dataset, illustrating the general development workflow in the process. Before diving into code, let’s consider the role of interactivity in data analysis.\nA major difference between doing visualization on paper and on computers is that visualization on computers can make use of interactivity. An interactive visualization is one that changes in response to user cues. This allows a display to update in a way that provides a visual comparison that was not available in a previous view. In this way, interactive visualization allows users to answer a sequence of questions.\nSelection, both of observations and of attributes, is fundamental to interactive visualization. This is because it precedes other interactive operations: you can select a subset of observations to filter down to or attributes to coordinate across multiple displays (we consider both types of interactivity in later lectures).\nThe code below selects movies to highlight based on Genre. We use a selectInput to create the dropdown menu. A reactive expression creates a new column (selected) in the movies dataset specifiying whether the current movie is selected. The reactive graph structure means that the ggplot2 figure is recreated each time the selection is changed, and the selected column is used to shade in the points. This process of changing the visual encoding of graphical marks depending on user selections is called “conditional encoding.”\n::: {.cell}\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(lubridate)\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %&gt;%\n  mutate(\n    date = as_date(Release_Date, format = \"%b %d %Y\"),\n    year = year(date),\n    Major_Genre = fct_explicit_na(Major_Genre),\n    MPAA_Rating = fct_explicit_na(MPAA_Rating),\n  )\n\ngenres &lt;- pull(movies, Major_Genre) %&gt;%\n  unique() %&gt;%\n  na.omit()\n\n### functions used in app\nscatterplot &lt;- function(df) {\n  ggplot(df) +\n    geom_point(\n      aes(Rotten_Tomatoes_Rating, IMDB_Rating, size = selected, alpha = selected)\n    ) +\n    scale_size(limits = c(0, 1), range = c(.5, 2), guide = \"none\") +\n    scale_alpha(limits = c(0, 1), range = c(.1, 1), guide = \"none\")\n}\n\n### definition of app\nui &lt;- fluidPage(\n  titlePanel(\"IMDB Analysis\"),\n  selectInput(\"genres\", \"Genre\", genres),\n  plotOutput(\"ratings_scatter\")\n)\n\nserver &lt;- function(input, output) {\n  movies_subset &lt;- reactive({\n    movies %&gt;%\n      mutate(selected = 1 * (Major_Genre %in% input$genres))\n  })\n\n  output$ratings_scatter &lt;- renderPlot({\n    scatterplot(movies_subset())\n  })\n}\n\napp &lt;- shinyApp(ui, server)\n:::\n\n::: {.cell} ::: {.cell-output-display}  ::: :::\nWe can extend this further. Let’s allow the user to filter by year and MPAA rating. Notice that there are some years in the future! We also find that there are systematic differences in IMDB and Rotten Tomatoes ratings as a function of these variables.\n::: {.cell}\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(lubridate)\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %&gt;%\n  mutate(\n    date = as_date(Release_Date, format = \"%b %d %Y\"),\n    year = year(date),\n    Major_Genre = fct_explicit_na(Major_Genre),\n    MPAA_Rating = fct_explicit_na(MPAA_Rating),\n  )\n\ngenres &lt;- pull(movies, Major_Genre) %&gt;%\n  unique() %&gt;%\n  na.omit()\nratings &lt;- pull(movies, MPAA_Rating) %&gt;%\n  unique() %&gt;%\n  na.omit()\n\n### functions used in app\nscatterplot &lt;- function(df) {\n  ggplot(df) +\n    geom_point(\n      aes(Rotten_Tomatoes_Rating, IMDB_Rating, size = selected, alpha = selected)\n    ) +\n    scale_size(limits = c(0, 1), range = c(.5, 2), guide = \"none\") +\n    scale_alpha(limits = c(0, 1), range = c(.1, 1), guide = \"none\")\n}\n\n### definition of app\nui &lt;- fluidPage(\n  titlePanel(\"IMDB Analysis\"),\n  selectInput(\"genres\", \"Genre\", genres, multiple = TRUE),\n  checkboxGroupInput(\"mpaa\", \"MPAA Rating\", ratings, ratings),\n  sliderInput(\"year\", \"Year\", min = min(movies$year), max = max(movies$year), c(1928, 2020), sep = \"\"),\n  plotOutput(\"ratings_scatter\")\n)\n\nserver &lt;- function(input, output) {\n  movies_subset &lt;- reactive({\n    movies %&gt;%\n      mutate(selected = 1 * (\n        (Major_Genre %in% input$genres) &\n        (MPAA_Rating %in% input$mpaa) &\n        (year &gt;= input$year[1]) &\n        (year &lt;= input$year[2])\n      ))\n  })\n\n  output$ratings_scatter &lt;- renderPlot({\n    scatterplot(movies_subset())\n  })\n}\n\napp &lt;- shinyApp(ui, server)\n:::\n\n::: {.cell} ::: {.cell-output-display}  ::: :::\nWe’ll include a final version of this plot which additionally shows the movie name when points are hovered. To accomplish this, we can no longer use ggplot2 on its own – it has to be linked with a plotting library that renders web-based visualizations (not just static image files). This is what the ggplotly() call does in the updated version of the app. The mouseover text is added through the tooltip argument.\n::: {.cell}\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(plotly)\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %&gt;%\n  mutate(\n    date = as_date(Release_Date, format = \"%b %d %Y\"),\n    year = year(date),\n    Major_Genre = fct_explicit_na(Major_Genre),\n    MPAA_Rating = fct_explicit_na(MPAA_Rating),\n  )\n\ngenres &lt;- pull(movies, Major_Genre) %&gt;%\n  unique() %&gt;%\n  na.omit()\nratings &lt;- pull(movies, MPAA_Rating) %&gt;%\n  unique() %&gt;%\n  na.omit()\n\n### functions used in app\nscatterplot &lt;- function(df) {\n  p &lt;- ggplot(mapping = aes(Rotten_Tomatoes_Rating, IMDB_Rating)) +\n    geom_point(data = df %&gt;% filter(selected),  aes(text = Title), size = 2, alpha = 1) +\n    geom_point(data = df %&gt;% filter(!selected),  size = .5, alpha = .1)\n  ggplotly(p, tooltip = \"Title\") %&gt;%\n    style(hoveron = \"fill\")\n}\n\n### definition of app\nui &lt;- fluidPage(\n  titlePanel(\"IMDB Analysis\"),\n  selectInput(\"genres\", \"Genre\", genres),\n  checkboxGroupInput(\"mpaa\", \"MPAA Rating\", ratings, ratings),\n  sliderInput(\"year\", \"Year\", min = min(movies$year), max = max(movies$year), c(1928, 2020), sep = \"\"),\n  plotlyOutput(\"ratings_scatter\")\n)\n\nserver &lt;- function(input, output) {\n  movies_subset &lt;- reactive({\n    movies %&gt;%\n      mutate(selected = (\n        (Major_Genre %in% input$genres) &\n        (MPAA_Rating %in% input$mpaa) &\n        (year &gt;= input$year[1]) &\n        (year &lt;= input$year[2])\n      ))\n  })\n\n  output$ratings_scatter &lt;- renderPlotly({\n    scatterplot(movies_subset())\n  })\n}\n\napp &lt;- shinyApp(ui, server)\n:::\n\nThese visualizations are an instance of the more general idea of using filtering to reduce complexity in data. Filtering is an especially powerful technique in the interactive paradigm, where it is possible to easily reverse (or compare) filtering choices.\nConceptually, what we are doing falls under the name of “Dynamic Querying,” which refers more generally to updating a visualization based on user queries. There are several ways to think about these dynamic queries,\n\nInterpretation 1: Dynamic queries create the visual analog of a database interaction. Rather than using a programming-based interface to filter elements or select attributes, we can design interactive visual equivalents.\nInterpretation 2: Dynamic queries allow rapid evaluation of conditional probabilities. The visualization above was designed to answer: What is the joint distribution of movie ratings, conditional on being a drama?"
  },
  {
    "objectID": "content/4-2.html",
    "href": "content/4-2.html",
    "title": "Introduction to Reactivity",
    "section": "",
    "text": "Recording, Code\n\nThese notes will explore the idea of reactivity in more depth. Recall that reactivity refers to the fact that Shiny app code is not run from top to bottom, like an ordinary R script. Instead, it runs reactively, depending on inputs that the user has provided. This can make writing Shiny code a bit unintuitive at first, but there are a few higher-level concepts that can help when writing reactive code.\nThe most important of these concepts is that reactive code can be viewed as a graph. The ui and server define an explicit dependency structure for how components depend on one another. The input$’s within render* functions in the server specify how UI inputs affect server computations. The IDs within the *Output elements in the ui specify which of the rendered output$’s in the server should be used to populate the visible interface.\nFor example, our first “Hello” app has the following (simple) reactivity graph. Note that I’ve drawn input and output nodes differently, to emphasize the flow of computation. I’ve also copied the code from the original app for reference.\n::: {.cell} ::: {.cell-output-display}  ::: :::\n::: {.cell}\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello!\"),\n  textInput(\"name\", \"Enter your name\"),\n  textOutput(\"printed_name\")\n)\n\nserver &lt;- function(input, output) {\n  output$printed_name &lt;- renderText({\n    paste0(\"Welcome to shiny, \", input$name, \"!\")\n  })\n}\n\napp &lt;- shinyApp(ui, server)\n:::\n\nEven though the graph is simple, note that the outputs will be recomputed each time that the input is changed. For more general graphs, all downstream nodes will be re-executed whenever an upstream source is changed (typically by a user input, though it’s possible to trigger changes automatically).\nReactive expressions provide a special kind of node that live between inputs and outputs. They depend on inputs, and they feed into outputs, but they are never made directly visible to the user. This is why we’ve drawn them as a kind of special intermediate node. Below, I’ve drawn the graph for our random normal plotter, with the reactive samples() expression.\n::: {.cell} ::: {.cell-output-display}  ::: :::\n::: {.cell}\nlibrary(shiny)\nlibrary(tidyverse)\n\n### Functions within app components\ngenerate_data &lt;- function(n, mean, sigma) {\n  data.frame(values = rnorm(n, mean, sigma))\n}\n\nhistogram_fun &lt;- function(df) {\n  ggplot(df) +\n    geom_histogram(aes(values), bins = 100) +\n    xlim(-10, 10)\n}\n\n### Defines the app\nui &lt;- fluidPage(\n  titlePanel(\"Random Normals\"),\n  numericInput(\"mean\", \"Enter the mean\", 0),\n  sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n  sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n  plotOutput(\"histogram\"),\n  dataTableOutput(\"dt\")\n)\n\nserver &lt;- function(input, output) {\n  samples &lt;- reactive({\n    generate_data(input$n, input$mean, input$sigma)\n  })\n  output$histogram &lt;- renderPlot(histogram_fun(samples()))\n  output$dt &lt;- renderDataTable(samples())\n}\n\napp &lt;- shinyApp(ui, server)\n:::\n\nA useful perspective is to think of reactive expressions as simplifying the overall reactivity graph. Specifically, by adding a reactive node, it’s possible to trim away many edges. For example, our initial implementation of the random normal plotter (which didn’t use the reactive expression) has a much more complicated graph, since many inputs feed directly into outputs.\n::: {.cell} ::: {.cell-output-display}  ::: :::\nLet’s see these principles in action for a similar, but more complex app. The app below can be used for power analysis. It simulates two groups of samples, both from normal distributions, but with different (user specified) means. We’ve used a reactive expression to generate the samples, so that both the histogram and hypothesis test result outputs can refer to the same intermediate simulated data.\n::: {.cell}\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(broom)\n\n### Functions within app components\ngenerate_data &lt;- function(n, mean1, mean2, sigma) {\n  data.frame(\n    values = c(rnorm(n, mean1, sigma), rnorm(n, mean2, sigma)),\n    group = rep(c(\"A\", \"B\"), each = n)\n  )\n}\n\nhistogram_fun &lt;- function(df) {\n  ggplot(df) +\n    geom_histogram(\n      aes(values, fill = group), \n      bins = 100, position = \"identity\",\n      alpha = 0.8\n    ) +\n    xlim(-10, 10)\n}\n\ntest_fun &lt;- function(df) {\n  t.test(values ~ group, data = df) %&gt;%\n    tidy() %&gt;%\n    select(p.value, conf.low, conf.high)\n}\n\n### Defines the app\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"mean1\", \"Mean (Group 1)\", 0, min = -10.0, max = 10.0, step = 0.1),\n      sliderInput(\"mean2\", \"Mean (Group 2)\", 0, min = -10, max = 10, step = 0.1),\n      sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n      sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n    ),\n    mainPanel(\n      plotOutput(\"histogram\"),\n      dataTableOutput(\"test_result\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  samples &lt;- reactive({\n    generate_data(input$n, input$mean1, input$mean2, input$sigma)\n  })\n  output$histogram &lt;- renderPlot(histogram_fun(generate_data(input$n, input$mean1, input$mean2, input$sigma)))\n  output$test_result &lt;- renderDataTable(test_fun(generate_data(input$n, input$mean1, input$mean2, input$sigma)))\n}\n\napp &lt;- shinyApp(ui, server)\n:::\n\nOther than that, the only difference is that I’ve saved output from the t.test using test_result. Notice the use of the broom package, which helps format the test output into a data.frame.\nSo far, all of our reactive code has lived within the render* or reactive() sets of functions. However, there is a another kind that is often useful, especially in more advanced applications: observers. An observer is a computation that is done every time certain inputs are changed, but which don’t affect downstream UI outputs through a render* function. For example, below, we’ve added a block (under observeEvent) that prints to the console every time either of the means are changed. I realize it is a bit of a mystery why these functions would ever be useful, but we will see them in more realistic contexts next week.\n::: {.cell}\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(broom)\n\n### Functions within app components\ngenerate_data &lt;- function(n, mean1, mean2, sigma) {\n  data.frame(\n    values = c(rnorm(n, mean1, sigma), rnorm(n, mean2, sigma)),\n    group = rep(c(\"A\", \"B\"), each = n)\n  )\n}\n\nhistogram_fun &lt;- function(df) {\n  ggplot(df) +\n    geom_histogram(\n      aes(values, fill = group), \n      bins = 100, position = \"identity\",\n      alpha = 0.8\n    ) +\n    xlim(-10, 10)\n}\n\ntest_fun &lt;- function(df) {\n  t.test(values ~ group, data = df) %&gt;%\n    tidy() %&gt;%\n    select(p.value, conf.low, conf.high)\n}\n\n### Defines the app\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"mean1\", \"Mean (Group 1)\", 0, min = -10.0, max = 10.0, step = 0.1),\n      sliderInput(\"mean2\", \"Mean (Group 2)\", 0, min = -10, max = 10, step = 0.1),\n      sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n      sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n    ),\n    mainPanel(\n      plotOutput(\"histogram\"),\n      dataTableOutput(\"test_result\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  samples &lt;- reactive({\n    generate_data(input$n, input$mean1, input$mean2, input$sigma)\n  })\n  output$histogram &lt;- renderPlot(histogram_fun(samples()))\n  output$test_result &lt;- renderDataTable(test_fun(samples()))\n  observeEvent(input$mean1 | input$mean2, {\n    message(\"group 1 mean is now: \", input$mean1)\n    message(\"group 2 mean is now: \", input$mean2)\n  })\n}\n\napp &lt;- shinyApp(ui, server)\n:::"
  },
  {
    "objectID": "content/2-1.html",
    "href": "content/2-1.html",
    "title": "Tidy Data",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\nA dataset is called tidy if rows correspond to distinct observations and columns correspond to distinct variables.\n\n\n\nFor visualization, it is important that data be in tidy format. This is because (a) each visual mark will be associated with a row of the dataset and (b) properties of the visual marks will determined by values within the columns. A plot that is easy to create when the data are in tidy format might be very hard to create otherwise.\nThe tidy data might seem like an idea so natural that it’s not worth teaching (let alone formalizing). However, exceptions are encountered frequently, and it’s important that you be able to spot them. Further, there are now many utilities for “tidying” data, and they are worth becoming familiar with.\nHere is an example of a tidy dataset.\n\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nIt is easy to visualize the tidy dataset.\n\nggplot(table1, aes(x = year, y = cases, col = country)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\n\nBelow are three non-tidy versions of the same dataset. They are representative of more general classes of problems that may arise,\n\nA variable might be implicitly stored within column names, rather than explicitly stored in its own column. Here, the years are stored as column names. It’s not really possible to create the plot above using the data in this format.\n\n\n\ntable4a # cases\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b # population\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n\nThe same observation may appear in multiple rows, where each instance of the row is associated with a different variable. Here, the observations are the country by year combinations.\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\nA single column actually stores multiple variables. Here, rate is being used to store both the population and case count variables.\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nThe trouble is that this variable has to be stored as a character; otherwise, we lose access to the original population and case variable. But, this makes the plot useless.\n\nggplot(table3, aes(x = year, y = rate)) +\n  geom_point() +\n  geom_line(aes(group = country))\n\n\n\n\n\n\n\n\nThe next few lectures provide tools for addressing these three problems.\n\nA few caveats are in order. It’s easy to become a tidy-data purist, and lose sight of the bigger data-analytic picture. To prevent that, first, remember that what is or is not tidy may be context dependent. Maybe you want to treat each week as an observation, rather than each day. Second, know that there are sometimes computational reasons to prefer non-tidy data. For example, “long” data often require more memory, since column names that were originally stored once now have to be copied onto each row. Certain statistical models are also sometimes best framed as matrix operations on non-tidy datasets."
  },
  {
    "objectID": "content/2-3.html",
    "href": "content/2-3.html",
    "title": "Deriving Variables",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\n\n\nIt’s easiest to define visual encodings when the variables we want to encode are contained in their own columns. After sketching out a visualization of interest, we may find that these variables are not explicitly represented among the columns of the raw dataset. In this case, we may need to derive them based on what is available. The dplyr and tidyr packages provide functions for deriving new variables, which we review in these notes.\nSometimes a single column is used to implicitly store several variables. To make the data tidy, separate can be used to split that single column into several columns, each of which corresponds to exactly one variable.\nThe block below separates our earlier table3, which stored rate as a fraction in a character column. The original table was,\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nand the separated version is,\n\ntable3 %&gt;% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) # try without convert, and compare the data types of the columns\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nNote that this function has an inverse, called unite, which can merge several columns into one. This is sometimes useful, but not as often as separate, since it isn’t needed to tidy a dataset.\nSeparating a single column into several is a special case of a more general operation, mutate, which defines new columns as functions of existing ones. We have used this is in previous lectures, but now we can philosophically justify it: the variables we want to encode need to be defined in advance.\nFor example, we may want to create a column rate that includes cases over population,\n\n\ntable3 %&gt;% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) %&gt;%\n  mutate(rate = cases / year)\n\n# A tibble: 6 × 5\n  country      year  cases population    rate\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;   &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071   0.373\n2 Afghanistan  2000   2666   20595360   1.33 \n3 Brazil       1999  37737  172006362  18.9  \n4 Brazil       2000  80488  174504898  40.2  \n5 China        1999 212258 1272915272 106.   \n6 China        2000 213766 1280428583 107.   \n\n\n\nSometimes, the variables of interest are functions of several rows. For example, perhaps we want to visualize averages of a variable across age groups. In this case, we can derive a summary across groups of rows using the group_by-followed-by-summarise pattern.\nFor example, perhaps we want the average rate over both years.\n\n\ntable3 %&gt;% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) %&gt;%\n  mutate(rate = cases / year) %&gt;%\n  group_by(country) %&gt;%\n  summarise(avg_rate = mean(rate))\n\n# A tibble: 3 × 2\n  country     avg_rate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Afghanistan    0.853\n2 Brazil        29.6  \n3 China        107."
  },
  {
    "objectID": "content/5-3.html",
    "href": "content/5-3.html",
    "title": "Linked Brushing",
    "section": "",
    "text": "Code, Recording"
  },
  {
    "objectID": "content/5-3.html#footnotes",
    "href": "content/5-3.html#footnotes",
    "title": "Linked Brushing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that we restrict brush motion to the \\(x\\)-direction. This is because the \\(x\\) direction alone encodes year information, which we want to select.↩︎"
  },
  {
    "objectID": "content/5-2.html",
    "href": "content/5-2.html",
    "title": "Graphical Queries - Brush Events",
    "section": "",
    "text": "Code, Recording"
  },
  {
    "objectID": "content/5-2.html#footnotes",
    "href": "content/5-2.html#footnotes",
    "title": "Graphical Queries - Brush Events",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, the code only executes when the mouse lifts off the brush selection. Some visualizations will be able to call the updating code every time the mouse is moved with the brush selected. This creates a smoother experience.↩︎"
  },
  {
    "objectID": "content/7-2.html",
    "href": "content/7-2.html",
    "title": "Vector Data",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(ceramic)\nlibrary(knitr)\nlibrary(sf)\nlibrary(spData)\nlibrary(tidyverse)\nlibrary(tmap)\ntheme_set(theme_minimal())\n\n\nAs mentioned previously, vector data are used to store geometric spatial data. Specifically, there are 7 types of geometric information that are commonly used, as given in the figure below.\n\n\ninclude_graphics(\"https://krisrs1128.github.io/stat479/posts/2021-03-02-week7-2/sf-classes.png\")\n\n\n\n\n\n\n\n\n\nWe can construct these geometric objects from scratch. For example, starting from the defining coordinates, we can use st_point to create a point object,\n\n\n# make a point\np &lt;- st_point(c(5, 2))\nplot(p)\n\n\n\n\n\n\n\n\nst_linestring to create a linestring,\n\n# make a line\nlinestring_matrix &lt;- rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))\np &lt;- st_linestring(linestring_matrix)\nplot(p)\n\n\n\n\n\n\n\n\nand st_polygon to create a polygon.\n\n# make a polygon\npolygon_list &lt;- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\np &lt;- st_polygon(polygon_list)\nplot(p)\n\n\n\n\n\n\n\n\n\nDifferent geometries can be combined into a geometry collection, using sfc.\n\n\npoint1 &lt;- st_point(c(5, 2))\npoint2 &lt;- st_point(c(1, 3))\npoints_sfc &lt;- st_sfc(point1, point2)\nplot(points_sfc)\n\n\n\n\n\n\n\n\n\nReal-world vector datasets are more than just these geometries — they also associate each geometry with some additional information about each feature. We can add this information to the geometries above by associating each element with a row of a data.frame. This merging is accomplished by st_sf, using geometry to associate a raw st_geom each row of a data.frame.\n\n\nlnd_point &lt;- st_point(c(0.1, 51.5))                \nlnd_geom &lt;- st_sfc(lnd_point, crs = 4326)         \nlnd_attrib = data.frame(                        \n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n  )\n\nlnd_sf = st_sf(lnd_attrib, geometry = lnd_geom)\n\n\nVisualization\n\nVector data can be directly plotted using base R. For example, suppose we want to plot the boundaries of India, within it’s local context. We can use the world dataset, provided by the spData package. Each row of the world object contains both the boundary of a country (in the geom column) and information about its location and population characteristics.\n\n\ndata(world)\nhead(world)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 11\n  iso_a2 name_long  continent region_un subregion type  area_km2     pop lifeExp\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 FJ     Fiji       Oceania   Oceania   Melanesia Sove…   1.93e4  8.86e5    70.0\n2 TZ     Tanzania   Africa    Africa    Eastern … Sove…   9.33e5  5.22e7    64.2\n3 EH     Western S… Africa    Africa    Northern… Inde…   9.63e4 NA         NA  \n4 CA     Canada     North Am… Americas  Northern… Sove…   1.00e7  3.55e7    82.0\n5 US     United St… North Am… Americas  Northern… Coun…   9.51e6  3.19e8    78.8\n6 KZ     Kazakhstan Asia      Asia      Central … Sove…   2.73e6  1.73e7    71.6\n# ℹ 2 more variables: gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt;\n\n\nThis makes the plot, using dplyr to filter down to just the row containing the India geometry.\n\nindia_geom &lt;- world %&gt;%\n  filter(name_long == \"India\") %&gt;%\n  st_geometry()\n\nplot(india_geom)\n\n\n\n\n\n\n\n\n\nUsing base R, we can also layer on several vector objects, using add = TRUE.\n\n\nworld_asia &lt;- world %&gt;%\n  filter(continent == \"Asia\")\n\nplot(india_geom, expandBB = c(0, 0.2, 0.1, 1), col = \"gray\", lwd = 3)\nplot(st_union(world_asia), add = TRUE)\n\n\n\n\n\n\n\n\n\nWe can also use tm_polygons in tmap. To change the coordinates of the viewing box, we can set the bbox (bounding box) argument.\n\n\nbbox &lt;- c(60, 5, 110, 40)\ntm_shape(world_asia, bbox = bbox) +\n  tm_polygons(col = \"white\") +\n  tm_shape(india_geom) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\nWe can also encode data that’s contained in the vector dataset.\n\n\ntm_shape(world_asia, bbox = bbox) +\n  tm_polygons(col = \"lifeExp\") +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\nEven in this more complex setup, where we work with background images and vector data rather than standard data.frames, we can still apply the kinds of visual encoding ideas that we are familiar with. For example, we can still color code or facet by fields in the vector dataset. To illustrate, we revisit the bus route data from the last lecture and distinguish between buses operated by the cities of Madison vs. Monona. Before plotting, we fetch the underlying data.\n\n\nSys.setenv(MAPBOX_API_KEY=\"pk.eyJ1Ijoia3Jpc3JzMTEyOCIsImEiOiJjbDYzdjJzczQya3JzM2Jtb2E0NWU1a3B3In0.Mk4-pmKi_klg3EKfTw-JbQ\")\nbasemap &lt;- cc_location(loc= c(-89.401230, 43.073051), buffer = 15e3)\nbus &lt;- read_sf(\"https://uwmadison.box.com/shared/static/5neu1mpuh8esmb1q3j9celu73jy1rj2i.geojson\") |&gt;\n  mutate(\n    operator = ifelse(is.na(operator), \"Unknown\", operator),\n    operator_color = case_when(\n      operator == \"City of Madison\" ~ \"#1f77b4\",\n      TRUE ~ \"#2ca02c\"\n    )\n  )\n\ntm_shape(basemap) +\n  tm_rgb() +\n  tm_shape(bus) +\n  tm_lines(col = \"#bc7ab3\", lwd = 1)\n\n\n\n\n\n\n\n\nNote that operator is the field containing information about which city is operating the buses. We can color code the routes by this attribute.\n\ntm_shape(basemap) +\n  tm_rgb() +\n  tm_shape(bus) +\n  tm_lines(col = \"operator\", lwd = 1) +\n  tm_layout(legend.bg.color = \"white\")"
  },
  {
    "objectID": "content/5-1.html",
    "href": "content/5-1.html",
    "title": "Graphical Queries - Click Events",
    "section": "",
    "text": "Code, Recording\n\nSome of the most sophisticated interactive data visualizations are based on the idea that user queries can themselves be defined visually. For example, to select a date range, we could directly interact with a time series plot, rather than relying on a slider input. Or, instead of a long dropdown menu of items, a user could select items by clicking on bars in a bar plot. There are many variations of this idea, but they all leverage graphical (rather than textual) displays to define queries. The advantage of this approach is that it increases information density – the selection inputs themselves encode data.\nTo implement this in Shiny, we first need a way of registering user interactions on plots themselves. We will consider two types of plot interaction mechanisms: clicks and brushes. These can be specified by adding click or brush events to plotOutput objects.\nThis creates a UI with a single plot on which we will be able to track user clicks,\n::: {.cell}\nui &lt;- fluidPage(\n  plotOutput(\"plot\", click = \"plot_click\")\n)\n:::\nHere, plot_click is an ID that can be used as input$plot_click in the server. We could name it however we want, but we need to be consistent across the UI and server (just like ordinary, non-graphical inputs).\nBefore, we just needed to place the input$id items within render and reactive server components, and the associated outputs would automatically know to redraw each time the value of any input was changed. Clicks are treated slightly differently. We have to both (a) recognize when a click event has occurred and (b) extract relevant information about what the click was referring to.\nFor (a), we generally use observeEvent,\n::: {.cell}\nobserveEvent(\n  input$plot_click,\n  ... things to do when the plot is clicked ...\n)\n:::\nThis piece of code will be run anytime the plot is clicked.\nFor (b), we can use the nearPoints helper function. Suppose the plot was made using the data.frame x. Then\n::: {.cell}\nnearPoints(x, input$click)\n:::\nwill return the samples in x that are close to the clicked location. We will often use a variant of this code that doesn’t just return the closeby samples – it returns all samples, along with their distance from the clicked location,\n::: {.cell}\nnearPoints(x, input$click, allRows = TRUE, addDist = TRUE)\n:::\nWe are almost ready to build a visualization whose outputs respond to graphical queries. Suppose we want a scatterplot where point sizes update according to their distance from the user’s click. Everytime the plot is clicked, we need to update the set of distances between samples and the clicked point. We then need to rerender the plot to reflect the new distances. This logic is captured by the block below,\n::: {.cell}\nserver &lt;- function(input, output) {\n  dist &lt;- reactiveVal(rep(1, nrow(x)))\n  observeEvent(\n    input$plot_click,\n    dist(reset_dist(x, input$plot_click))\n  )\n\n  output$plot &lt;- renderPlot({\n    scatter(x, dist())\n  })\n}\n:::\nThe code above uses one new concept, the reactiveVal on the first line of the function. It is a variable that doesn’t directly depend on any inputs, which can become a source node for downstream reactive and render nodes in the reactive graph. Anytime the variable’s value is changed, all downstream nodes will be recomputed. A very common pattern is use an observeEvent to update a reactiveVal every time a graphical query is performed. Any plots that depend on this value will then be updated. For example,\n::: {.cell}\nval &lt;- reactiveVal(initial_val) # initialize the reactive value\n\nobserveEvent(\n  ...some input event...\n  ...do some computation...\n  val(new_value) # update val to new_val\n)\n\n# runs each time the reactiveVal changes\nrenderPlot({\n  val() # get the current value of the reactive value\n})\n:::\nSo, revisiting the dist in the earlier code block, we see that it is initialized as a vector of 1’s whose length is equal to the number of rows of x. Everytime the plot is clicked, we update the value of dist according to the function reset_dist. Finally, the changed value of dist triggers a rerun of renderPlot. Let’s look at the full application in action. It makes a scatterplot using the cars dataset and resizes points every time the plot is clicked.\n::: {.cell}\nlibrary(tidyverse)\nlibrary(shiny)\n\n# wrapper to get the distances from points to clicks\nreset_dist &lt;- function(x, click) {\n  nearPoints(x, click, allRows = TRUE, addDist = TRUE)$dist_\n}\n\n# scatterplot plot with point size dependent on click location\nscatter &lt;- function(x, dists) {\n  x %&gt;%\n    mutate(dist = dists) %&gt;%\n    ggplot() +\n    geom_point(aes(mpg, hp, size = dist)) +\n    scale_size(range = c(6, 1))\n}\n\nui &lt;- fluidPage(\n  plotOutput(\"plot\", click = \"plot_click\")\n)\n\nserver &lt;- function(input, output) {\n  dist &lt;- reactiveVal(rep(1, nrow(mtcars)))\n  observeEvent(\n    input$plot_click,\n    dist(reset_dist(mtcars, input$plot_click))\n  )\n\n  output$plot &lt;- renderPlot(scatter(mtcars, dist()))\n}\n\nshinyApp(ui, server)\n:::\n\nThe reset_dist function uses nearPoints to compute the distance between each sample and the plot, each time the plot is clicked. The associated reactive value dist gets changed, which triggers scatterplot to run, and it is encoded using size in the downstream ggplot2 figure.\nWe can make the plot more interesting by outputting a table showing the original dataset. Using the same dist() call, we can sort the table by distance each time the plot is clicked.\n::: {.cell}\nlibrary(tidyverse)\nlibrary(shiny)\nmtcars &lt;- add_rownames(mtcars)\n\nreset_dist &lt;- function(x, click) {\n  nearPoints(x, click, allRows = TRUE, addDist = TRUE)$dist_\n}\n\nscatter &lt;- function(x, dists) {\n  x %&gt;%\n    mutate(dist = dists) %&gt;%\n    ggplot() +\n    geom_point(aes(mpg, hp, size = dist)) +\n    scale_size(range = c(6, 1))\n}\n\nui &lt;- fluidPage(\n  plotOutput(\"plot\", click = \"plot_click\"),\n  dataTableOutput(\"table\")\n)\n\nserver &lt;- function(input, output) {\n  dist &lt;- reactiveVal(rep(1, nrow(mtcars)))\n  observeEvent(\n    input$plot_click,\n    dist(reset_dist(mtcars, input$plot_click))\n  )\n\n  output$plot &lt;- renderPlot(scatter(mtcars, dist()))\n  output$table &lt;- renderDataTable({\n    mtcars %&gt;%\n      mutate(dist = dist()) %&gt;%\n      arrange(dist)\n  })\n}\n\nshinyApp(ui, server)\n:::"
  },
  {
    "objectID": "content/3-3.html",
    "href": "content/3-3.html",
    "title": "Compound Figures",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nReading, Recording, Rmarkdown\n\nFaceting is useful whenever we want different rows of the data to appear in different panels. What if we want to compare different columns, or work with several datasets? A more general alternative is to use compound plots. The idea is to construct plots separately and then combine them only at the very end.\nThe main advantage of compound plots is that individual panels can be tailored to specific visual comparisons, but relationships across panels can also be studied. For example, the plot below shows change in the total number and composition of undergraduate majors over the last few decades. In principle, the same information could be communicated using a stacked area plot (geom_area). However, comparing the percentages for 1970 and 2015 is much more straightforward using a line plot, and we can still see changes in the overall number of degrees using the area plot.\n::: {.cell} ::: {.cell-output-display}  ::: :::\nFor reference, here is a non-compound display of the same information.\n::: {.cell} ::: {.cell-output-display}  ::: :::\nThere are a few considerations that can substantially improve the quality of a compound plot,\n\nConsistent visual encodings for shared variables\nClear, but unobtrusive annotation\nProper alignment in figure baselines\n\nWe will discuss each point separately.\nThe figures below are compound plots of a dataset of athlete physiology. They are very similar, but the second is better because it enforces a more strict consistency in encodings across panels. Specifically, the male / female variable is (1) encoded using the same color scheme across all panels and (2) ordered so that female repeatedly appears on the right of male.\n::: {.cell} ::: {.cell-output-display}  ::: :::\nThe improved, visually consistent approach is given below.\n::: {.cell} ::: {.cell-output-display}  ::: :::\nEffective annotation can be used to refer to different subpanels of the data without drawing too much attention to itself. Labels should be visible but subtle – not too large, similar fonts as the figures, and logically ordered ((a) on top left). A nice heuristic is to think of these annotations like page numbers. They are useful for making references, but aren’t something that is actively read.\n::: {.cell} ::: {.cell-output-display}  ::: :::\nFor alignment, we will want figure baselines / borders to be consistent. Misalignment can be distracting. This is primarily a problem when compound plots are made from manually. If we follow the programmatic approaches discussed in the next lecture, we won’t have this issue.\n::: {.cell} ::: {.cell-output-display}  ::: :::"
  },
  {
    "objectID": "content/3-2.html",
    "href": "content/3-2.html",
    "title": "Ridge Plots",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nRidge plots are a special case of small multiples that are particularly suited to displaying multiple parallel time series or densities.\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(ggridges)\ntheme_set(theme_bw())\n\nAn example ridge plot for the gapminder dataset is shown below. The effectiveness of this plot comes from the fact that multiple densities can be displayed in close proximity to one another, making it possible to facet across many variables in a space-efficient way.\n\ndata(gapminder)\ngapminder &lt;- gapminder %&gt;%\n  mutate(\n    dollars_per_day = (gdp / population) / 365,\n    group = case_when(\n      region %in% c(\"Western Europe\", \"Northern Europe\",\"Southern Europe\",  \"Northern America\",  \"Australia and New Zealand\") ~ \"West\",\n      region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n      region %in% c(\"Caribbean\", \"Central America\",  \"South America\") ~ \"Latin America\",\n      continent == \"Africa\" &  region != \"Northern Africa\" ~ \"Sub-Saharan\", \n      TRUE ~ \"Others\"\n    ),\n    group = factor(group, levels = c(\"Others\", \"Latin America\",  \"East Asia\", \"Sub-Saharan\", \"West\")),\n    west = ifelse(group == \"West\", \"West\", \"Developing\")\n  )\n\n\npast_year &lt;- 1970\ngapminder_subset &lt;- gapminder %&gt;%\n  filter(year == past_year, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +  \n  geom_density_ridges() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nYou might wonder, why not plot the raw data? We do this using a geom_point below. The result isn’t as satisfying, because, while the shape of the ridges popped out immediately in the original plot, we have to invest a bit more effort to understand the regions of higher density in the raw data plot. Also, when there are many samples, the entire range might appeared covered in marks when in fact there are some intervals with higher density than others.\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_point(shape = \"|\", size = 5) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nA nice compromise is to include both the raw positions and the smoothed densities.\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_density_ridges(\n    jittered_points = TRUE, \n    position = position_points_jitter(height = 0),\n    point_shape = '|', point_size = 3\n  ) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nTo exercise our knowledge of both faceting and ridge plots, let’s study a particular question in depth: How did the gap between rich and poor countries change between 1970 and 2010?\nA first reasonable plot is to facet year and development status. While the rich countries have become slightly richer, there is a larger shift in the incomes for the poorer countries.\n\npast_year &lt;- 1970\npresent_year &lt;- 2010\nyears &lt;- c(past_year, present_year)\n\ngapminder_subset &lt;- gapminder %&gt;%\n  filter(year %in% years, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day)) +\n  geom_histogram(binwidth = 0.25) +\n  scale_x_log10() +\n  facet_grid(year ~ west)\n\n\n\n\n\n\n\n\nWe can express this message more compactly by overlaying densities, but the attempt below is misleading, because it gives the same total area to both groups of countries. This doesn’t make sense, considering there are more than 4 times as many developing vs. western countries (87 vs. 21).\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n\nWe can adjust the areas of the curves by directly referencing the ..count variable, which is implicitly computed by ggplot2 while it’s computing these densities.\n\nggplot(gapminder_subset, aes(dollars_per_day, y = ..count.., fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n\nCan we attribute the changes to specific regions? Let’s apply our knowledge of ridge plots.\n\nggplot(gapminder_subset, aes(dollars_per_day, y = group, fill = as.factor(year))) +\n  geom_density_ridges(alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nAlternatively, we can overlay densities from the different regions. Both plots suggest that much of the increase in incomes can be attributed to countries in Latin America and East Asia.\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = group, y = ..count..)) +\n  geom_density(alpha = 0.7, bw = .12, position = \"stack\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_x_log10(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0, 0.15, 0)) +\n  facet_grid(year ~ .)"
  },
  {
    "objectID": "content/7-4.html",
    "href": "content/7-4.html",
    "title": "Coordinate Reference Systems",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(sf)\nlibrary(spData)\ntheme_set(theme_minimal())\n\n\nAn important subtlety of geographic data visualization is that all our maps are in 2D, but earth is 3D. The process of associated points on the earth with 2D coordinates is called « projection. » All projections introduce some level of distortion, and there is no universal, ideal projection.\nHere are a few examples of famous global projections. There are also many projections designed to give optimal representations locally within a particular geographic area.\nThis means that there is no universal standard for how to represent coordinates on the earth, and it’s common to find different projections in practice. For example, the block below shows how North America gets projected according to two different CRSs.\n::: {.cell}\n# some test projections, don't worry about this syntax\nmiller &lt;- \"+proj=mill +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\nlambert &lt;- \"+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n\nnorth_america &lt;- world %&gt;%\n  filter(continent == \"North America\")\n\ntm_shape(st_transform(north_america, miller)) +\n  tm_polygons()\n::: {.cell-output-display}  :::\ntm_shape(st_transform(north_america, lambert)) +\n  tm_polygons()\n::: {.cell-output-display}  ::: :::\nBoth vector and raster spatial data will be associated with CRS’s. In either sf or raster objects, they can be accessed using the crs function.\nA common source of bugs is to use two different projections for the same analysis. In this class, we will always use the EPSG:4326 projection, which is what is used in most online maps. But in your own projects, you should always check that the projections are consistent across data sources. If you find an inconsistency, it will be important to « reproject » the data into the same CRS."
  },
  {
    "objectID": "content/3-1.html",
    "href": "content/3-1.html",
    "title": "Faceting",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nIt might seem like we’re limited with the total number of variables we can display at a time. While there are many types of encodings we could in theory use, only a few them are very effective, and they can interfere with one another.\nNot all is lost, though! A very useful idea for visualizing high-dimensional data is the idea of small multiples. It turns out that our eyes are pretty good at making sense of many small plots, as long as there is some shared structure across the plots.\n\nLet’s see these ideas in action. These are libraries we need.\n\nlibrary(tidyverse)\nlibrary(dslabs)\ntheme_set(theme_bw())\n\nIn ggplot2, we can implement this idea using the facet_wrap and facet_grid commands. We specify the column in the data.frame along which we want to generate comparable small multiples.\n\nyears &lt;- c(1962, 1980, 1990, 2000, 2012)\ncontinents &lt;- c(\"Europe\", \"Asia\")\ngapminder_subset &lt;- gapminder %&gt;%\n  filter(year %in% years, continent %in% continents)\n\nggplot(\n    gapminder_subset, \n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(. ~ year) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nIn facet grid, you specify whether you want the plot to be repeated across rows or columns, depending on whether you put the variable before or after the tilde.\n\nggplot(\n    gapminder_subset, \n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(year ~ .) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nYou can also facet by more than one variable at a time, specifying which variables should go in rows and which should go in columns again using the tilde.\n\nggplot(\n    gapminder %&gt;% filter(year %in% years),\n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(year ~ continent) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nSometimes, you just want to see the display repeated over groups, but you don’t really need them to all appear in the same row or column. In this case, you can use facet_wrap.\n\nggplot(\n    gapminder %&gt;% filter(year %in% years),\n    aes(x = fertility, y = life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_wrap(~ year)\n\n\n\n\n\n\n\n\nJust to illustrate, faceting makes sense for datasets other than scatterplots. This example also shows that faceting will apply to multiple geom layers at once.\nThe dataset shows the abundances of five different bacteria across three different subjects over time, as they were subjected to antibiotics. The data were the basis for this study.\n\nantibiotic &lt;- read_csv(\"https://uwmadison.box.com/shared/static/5jmd9pku62291ek20lioevsw1c588ahx.csv\")\nhead(antibiotic)\n\n# A tibble: 6 × 7\n  species  sample value ind    time svalue antibiotic     \n  &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          \n1 Unc05qi6 D1         0 D         1   NA   Antibiotic-free\n2 Unc05qi6 D2         0 D         2   NA   Antibiotic-free\n3 Unc05qi6 D3         0 D         3    0   Antibiotic-free\n4 Unc05qi6 D4         0 D         4    0   Antibiotic-free\n5 Unc05qi6 D5         0 D         5    0   Antibiotic-free\n6 Unc05qi6 D6         0 D         6    0.2 Antibiotic-free\n\n\nI have also separately computed running averages for each of the variables – this is in the svalue column. We’ll discuss ways to do this during the week on time series visualization.\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\n\n\n\n\nIt seems like some of the species are much more abundant than others. In this situation, it might make sense to rescale the \\(y\\)-axis. Though, this is always a risky decision – people might easily misinterpret the plot and conclude that the different species all have the same abundances. Nonetheless, it can’t hurt to try, using the scale argument to facet_grid.\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind, scale = \"free_y\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\n\n\n\n\nUnlike the years example, the facets don’t automatically come with their own natural order. We can define an order based on the average value of the responses over the course of the survey, and then change the factor levels of the Species column to reorder the panels.\n\nspecies_order &lt;- antibiotic %&gt;%\n  group_by(species) %&gt;%\n  summarise(avg_value = mean(value)) %&gt;%\n  arrange(desc(avg_value)) %&gt;%\n  pull(species)\n\nantibiotic &lt;- antibiotic %&gt;%\n  mutate(species = factor(species, levels = species_order))\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind, scale = \"free_y\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))"
  },
  {
    "objectID": "content/11-1.html",
    "href": "content/11-1.html",
    "title": "Introduction to Topic Models",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nAn overview of the topic modeling process. Topics are distributions over words and the word counts of new documents are determined by their degree of membership over a set of underlying topics. In an ordinary clustering model the bars for the memberships would have to be either pure purple or orange. Here each document is a mixture.\nA geometric interpretation of LDA from the original paper by Blei Ng and Jordan."
  },
  {
    "objectID": "content/11-1.html#footnotes",
    "href": "content/11-1.html#footnotes",
    "title": "Introduction to Topic Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA simplex is the geometric object describing the set of probability vectors over \\(V\\) elements. For example, if \\(V = 3\\), then \\(\\left(0.1,\n0, 0.9\\right)\\) and \\(\\left(0.2, 0.3, 0.5\\right)\\) belong to the simplex, but not \\(\\left(0.3, 0.1, 9\\right)\\), since it sums to a number larger than 1.↩︎"
  },
  {
    "objectID": "content/11-2.html",
    "href": "content/11-2.html",
    "title": "Fitting Topic Models",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gutenbergr\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\nlibrary(\"tidytext\")\nlibrary(\"topicmodels\")\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nThere are several packages in R that can be used to fit topic models. We will use LDA as implemented in the topicmodels package, which expects input to be structured as a DocumentTermMatrix, a special type of matrix that stores the counts of words (columns) across documents (rows). In practice, most of the effort required to fit a topic model goes into transforming the raw data into a suitable DocumentTermMatrix.\nTo illustrate this process, let’s consider the “Great Library Heist” example from the reading. We imagine that a thief has taken four books — Great Expectations, Twenty Thousand Leagues Under The Sea, War of the Worlds, and Pride & Prejudice — and torn all the chapters out. We are left with pieces of isolated pieces of text and have to determine from which book they are from. The block below downloads all the books into an R object.\n\n\ntitles &lt;- c(\"Twenty Thousand Leagues under the Sea\", \n            \"The War of the Worlds\",\n            \"Pride and Prejudice\", \n            \"Great Expectations\")\nbooks &lt;- gutenberg_works(title %in% titles) %&gt;%\n  gutenberg_download(meta_fields = \"title\")\nbooks\n\n# A tibble: 53,718 × 3\n   gutenberg_id text                    title                \n          &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt;                \n 1           36 \"cover \"                The War of the Worlds\n 2           36 \"\"                      The War of the Worlds\n 3           36 \"\"                      The War of the Worlds\n 4           36 \"\"                      The War of the Worlds\n 5           36 \"\"                      The War of the Worlds\n 6           36 \"The War of the Worlds\" The War of the Worlds\n 7           36 \"\"                      The War of the Worlds\n 8           36 \"by H. G. Wells\"        The War of the Worlds\n 9           36 \"\"                      The War of the Worlds\n10           36 \"\"                      The War of the Worlds\n# ℹ 53,708 more rows\n\n\n\nSince we imagine that the word distributions are not equal across the books, topic modeling is a reasonable approach for discovering the books associated with each chapter. Note that, in principle, other clustering and dimensionality reduction procedures could also work.\nFirst, let’s simulate the process of tearing the chapters out. We split the raw texts anytime the word “Chapter” appears. We will keep track of the book names for each chapter, but this information is not passed into the topic modeling algorithm.\n\n\nby_chapter &lt;- books %&gt;%\n  group_by(title) %&gt;%\n  mutate(\n    chapter = cumsum(str_detect(text, regex(\"chapter\", ignore_case = TRUE)))\n  ) %&gt;%\n  group_by(title, chapter) %&gt;%\n  mutate(n = n()) %&gt;%\n  filter(n &gt; 5) %&gt;%\n  ungroup() %&gt;%\n  unite(document, title, chapter)\n\n\nAs it is, the text data are long character strings, giving actual text from the novels. To fit LDA, we only need counts of each word within each chapter – the algorithm throws away information related to word order. To derive word counts, we first split the raw text into separate words using the unest_tokens function in the tidytext package. Then, we can count the number of times each word appeared in each document using count, a shortcut for the usual group_by and summarize(n = n()) pattern.\n\n\nword_counts &lt;- by_chapter %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(document, word) # shortcut for group_by(document, word) %&gt;% summarise(n = n())\n\nword_counts\n\n# A tibble: 101,271 × 3\n   document               word             n\n   &lt;chr&gt;                  &lt;chr&gt;        &lt;int&gt;\n 1 Great Expectations_0   1867             1\n 2 Great Expectations_0   charles          1\n 3 Great Expectations_0   contents         1\n 4 Great Expectations_0   dickens          1\n 5 Great Expectations_0   edition          1\n 6 Great Expectations_0   expectations     1\n 7 Great Expectations_0   illustration     1\n 8 Great Expectations_100 age              1\n 9 Great Expectations_100 arose            1\n10 Great Expectations_100 barnard’s        1\n# ℹ 101,261 more rows\n\n\n\nThese words counts are still not in a format compatible with conversion to a DocumentTermMatrix. The issue is that the DocumentTermMatrix expects words to be arranged along columns, but currently they are stored across rows. The line below converts the original “long” word counts into a “wide” DocumentTermMatrix in one step. Across these 4 books, we have 65 chapters and a vocabulary of size 18325.\n\n\nchapters_dtm &lt;- word_counts %&gt;%\n  cast_dtm(document, word, n)\nchapters_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 195, terms: 18740)&gt;&gt;\nNon-/sparse entries: 101271/3553029\nSparsity           : 97%\nMaximal term length: 19\nWeighting          : term frequency (tf)\n\n\n\nOnce the data are in this format, we can use the LDA function to fit a topic model. We choose \\(K = 4\\) topics because we expect that each topic will match a book. Different hyperparameters can be set using the control argument.\n\n\nchapters_lda &lt;- LDA(chapters_dtm, k = 4, control = list(seed = 1234))\nchapters_lda\n\nA LDA_VEM topic model with 4 topics.\n\n\n\nThere are two types of outputs produced by the LDA model: the topic word distributions (for each topic, which words are common?) and the document-topic memberships (from which topics does a document come from?). For visualization, it will be easiest to extract these parameters using the tidy function, specifying whether we want the topics (beta) or memberships (gamma).\n\n\ntopics &lt;- tidy(chapters_lda, matrix = \"beta\")\nmemberships &lt;- tidy(chapters_lda, matrix = \"gamma\")\n\n\nThis tidy approach is preferable to extracting the parameters directly from the fitted model (e.g., using chapters_lda@gamma) because it ensures the output is a tidy data.frame, rather than a matrix. Tidy data.frames are easier to visualize using ggplot2.\n\n\n# highest weight words per topic\ntopics %&gt;%\n  arrange(topic, -beta)\n\n# A tibble: 74,960 × 3\n   topic term          beta\n   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1     1 captain    0.00990\n 2     1 _nautilus_ 0.00817\n 3     1 sea        0.00587\n 4     1 nemo       0.00562\n 5     1 ned        0.00514\n 6     1 water      0.00504\n 7     1 conseil    0.00440\n 8     1 time       0.00418\n 9     1 land       0.00389\n10     1 day        0.00343\n# ℹ 74,950 more rows\n\n# topic memberships per document\nmemberships %&gt;%\n  arrange(document, topic)\n\n# A tibble: 780 × 3\n   document               topic     gamma\n   &lt;chr&gt;                  &lt;int&gt;     &lt;dbl&gt;\n 1 Great Expectations_0       1 0.515    \n 2 Great Expectations_0       2 0.479    \n 3 Great Expectations_0       3 0.00316  \n 4 Great Expectations_0       4 0.00316  \n 5 Great Expectations_100     1 0.000476 \n 6 Great Expectations_100     2 0.000476 \n 7 Great Expectations_100     3 0.999    \n 8 Great Expectations_100     4 0.000476 \n 9 Great Expectations_101     1 0.0000158\n10 Great Expectations_101     2 0.0000158\n# ℹ 770 more rows"
  },
  {
    "objectID": "content/9-5.html",
    "href": "content/9-5.html",
    "title": "Cluster Stability",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(\"MASS\")\nlibrary(\"Matrix\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"pdist\")\nlibrary(\"superheat\")\nlibrary(\"tidyr\")\ntheme_set(theme_minimal())\nset.seed(1234)\n\n\nOne of the fundamental principles in statistics is that, no matter how the experiment / study was conducted, if we ran it again, we would get different results. More formally, sampling variability creates uncertainty in our inferences.\nHow should we think about sampling variability in the context of clustering? This is a tricky problem, because you can permute the labels of the clusters without changing the meaning of the clustering. However, it is possible to measure and visualize the stability of a point’s cluster assignment.\nTo make this less abstract, consider an example. A study has found a collection of genes that are differentially expressed between patients with two different subtypes of a disease. There is an interest in clustering genes that have similar expression profiles across all patients — these genes probably belong to similar biological processes.\nOnce you run the clustering, how sure can you be that, if the study would run again, you would recover a similar clustering? Are there some genes that you are sure belong to a particular cluster? Are there some that lie between two clusters?\nTo illustrate, consider the simulated dataset below. Imagine that the rows are patients, the column are genes, and the colors are the expression levels of genes within patients. There are 5 clusters of genes here (columns 1 - 20 are cluster 1, 21 - 41 are cluster 2, …). The first two clusters are only weakly visible, while the last three stand out strongly.\n\n\nn_per &lt;- 20\np &lt;- n_per * 5\nSigma1 &lt;- diag(2) %x% matrix(rep(0.3, n_per ** 2), nrow = n_per)\nSigma2 &lt;- diag(3) %x% matrix(rep(0.6, n_per ** 2), nrow = n_per)\nSigma &lt;- bdiag(Sigma1, Sigma2)\ndiag(Sigma) &lt;- 1\nmu &lt;- rep(0, 100)\nx &lt;- mvrnorm(25, mu, Sigma)\n\ncols &lt;- c('#f6eff7','#bdc9e1','#67a9cf','#1c9099','#016c59')\nsuperheat(\n  x, \n  pretty.order.rows = TRUE, \n  bottom.label = \"none\", \n  heat.pal = cols,\n  left.label.text.size = 3,\n  legend = FALSE\n)\n\n\n\n\nA simulated clustering of genes (columns) across rows (patients).\n\n\n\n\n\nThe main idea for how to compute cluster stability is to bootstrap (i.e., randomly resample) the patients and see whether the cluster assignments for each gene change. More precisely, we use the following strategy,\n\n\nUsing all the patients, \\(X\\), estimate the cluster centroids \\(c_{1}, \\dots,\nc_{K}\\).\nFor \\(B\\) bootstrap iterations, perform the following.\n\nSample the patients with replacement, generating a bootstrap resampled version of the dataset \\(X_{b}^{\\ast}\\).\nPermute the original cluster centroids to reflect the order of patients in \\(X_{b}^{\\ast}\\). Call the permuted centroids \\(c_{1b}^{\\ast}, \\dots,\nc_{Kb}^{\\ast}\\).\nAssign genes in \\(X_{b}^{\\ast}\\) to the cluster \\(k\\) of the closest \\(c_{bk}^{\\ast}\\).\n\nWe quantify our certainty that gene \\(j\\) belongs to cluster \\(k\\) by counting the number of times that gene \\(j\\) was assigned to cluster \\(k\\).\n\n\nThe picture below describes the bootstrapping process for a gene. The two rows correspond to the original and bootstrapped representations a specific gene, respectively. Each bar gives the expression level of the gene for one individual. Due to the random sampling in the bootstrapped dataset, some individuals become overrepresented and some are removed. If we also permute the centroids in the same way, we get a new distance between genes and their centroids. Since the patients who are included changes, the distances between each gene and each centroid changes, so the genes might be assigned to different clusters.\n\n\n\n\n\n\n\n\n\n\n\nK &lt;- 5\nB &lt;- 1000\ncluster_profiles &lt;- kmeans(t(x), centers = K)$centers\ncluster_probs &lt;- matrix(nrow = ncol(x), ncol = B)\n\nfor (b in seq_len(B)) {\n  b_ix &lt;- sample(nrow(x), replace = TRUE)\n  dists &lt;- as.matrix(pdist(t(x[b_ix, ]), cluster_profiles[, b_ix]))\n  cluster_probs[, b] &lt;- apply(dists, 1, which.min)\n}\n\ncluster_probs &lt;- as_tibble(cluster_probs) %&gt;%\n  mutate(gene = row_number()) %&gt;%\n  pivot_longer(-gene, names_to = \"b\", values_to = \"cluster\")\n\n\nThe table below shows the result of this procedure. In each bootstrap iteration, gene 1 was assigned to cluster 4, so we can rely on that assignment. On the other hand, gene 3 is assigned to cluster 4 75% of the time, but occasionally appears in clusters 1, 2, and 5.\n\n\ncluster_probs &lt;- cluster_probs %&gt;%\n  mutate(cluster = as.factor(cluster)) %&gt;%\n  group_by(gene, cluster) %&gt;%\n  summarise(prob = n() / B)\n\ncluster_probs\n\n# A tibble: 277 × 3\n# Groups:   gene [100]\n    gene cluster  prob\n   &lt;int&gt; &lt;fct&gt;   &lt;dbl&gt;\n 1     1 1       0.334\n 2     1 2       0.006\n 3     1 3       0.001\n 4     1 4       0.659\n 5     2 1       0.981\n 6     2 3       0.002\n 7     2 4       0.017\n 8     3 1       0.754\n 9     3 2       0.008\n10     3 3       0.112\n# ℹ 267 more rows\n\n\n\nThese fractions for all genes are summarized by the plot below. Each row is a gene. The length of each color gives the number of times that gene was assigned to that cluster. The genes from rows 41 - 100 are all clearly distinguished, which is in line with what we saw visually in the heatmap above. The first two clusters are somewhat recovered, but since they were often assigned to alternative clusters, we can conclude that they were harder to demarcate out than the others.\n\n\nggplot(cluster_probs) +\n  geom_bar(aes(y = as.factor(gene), x = prob, col = cluster, fill = cluster), stat = \"identity\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = \"Gene\", x = \"Proportion\") +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_text(size = 7),\n    legend.position = \"bottom\"\n  )"
  },
  {
    "objectID": "content/13-1.html",
    "href": "content/13-1.html",
    "title": "Introduction to Feature Learning",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(keras)\n\n\nIn classical machine learning, we assume that the features most relevant to prediction are already available. E.g., when we want to predict home price, we already have features about square feet and neighborhood income, which are clearly relevant to the prediction task.\nIn many modern problems though, we have only access to data where the most relevant features have not been directly encoded.\n\nIn image classification, we only have raw pixel values. We may want to predict whether a pedestrian is in an image taken from a self-driving car, but we have only the pixels to work with. It would be useful to have an algorithm come up with labeled boxes like those in the examples below.\nFor sentiment analysis, we want to identify whether a piece of text is a positive or negative sentiment. However, we only have access to the raw sequence of words, without any other context. Examples from the IMDB dataset are shown below.\n\nIn both of these examples, this information could be encoded manually, but it would a substantial of effort, and the manual approach could not be used in applications that are generating data constantly. In a way, the goal of these algorithms is to distill the raw data down into a succinct set of descriptors that can be used for more classical machine learning or decision making.\n\n\n\n\n\n\nAn example of the types of labels that would be useful to have starting from just the raw image.\n\n\n\n\nExample reviews from the IMDB dataset:\n  positive,\"A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"\"has got all the polari ....\"\n  positive,\"I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be ...\"\n  negative,\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going to ...\"\n  positive,\"Petter Mattei's \"\"Love in the Time of Money\"\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a ...\"\n\nIn these problems, the relevant features only arise as complex interactions between the raw data elements.\n\nTo recognize a pedestrian, we need many adjacent pixels to have a particular configuration of values, leading to combinations of edges and shapes, which when viewed together, become recognizable as a person.\nTo recognize a positive or negative sentiment, we need to recognize interactions between words. “The movie was good until” clearly has bad sentiment, but you cannot tell that from the isolated word counts alone.\n\nThe main idea of deep learning is to learn these more complex features one layer at a time. For image data, the first layer recognizes interactions between individual pixels. Specifically, individual features are designed to “activate” when particular pixel interactions are present. The second layer learns to recognize interactions between features in the first layer, and so on, until the learned features correspond to more “high-level” concepts, like sidewalk or pedestrian.\nBelow is a toy example of how an image is processed into feature activations along a sequence of layers. Each pixel within the feature maps correspond to a patch of pixels in the original image – those later in the network have a larger field of view than those early on. A pixel in a feature map has a large value if any of the image features that it is sensitive to are present within its field of vision.\n\n\n\n\n\n\nA toy diagram of feature maps from the model loaded below. Early layers have fewer but larger feature maps while later layers have many but small ones. The later layers typically contain higher-level concepts used in the final predictions.\n\n\n\n\n\nAt the end of the feature extraction process, all the features are passed into a final linear or logistic regression module that completes the regression or classification task, respectively.\nIt is common to refer to each feature map as a neuron. Different neurons activate when different patterns are present in the original, underlying image.\n\n\n\n\n\n\nAn illustration of the different spatial contexts of feature maps at different layers. An element of a feature map has a large value (orange in the picture) if the feature that it is sensitive to is present in its spatial context. Higher-level feature maps are smaller but each pixel within it has a larger spatial context.\n\n\n\n\n\nBelow, we load a model to illustrate the concept of multilayer networks. This model has 11 layers followed by a final logistic regression layer. There are many types of layers. Each type of layer contributes in a different way to the feature learning goal, and learning how design and compose these different types of layers is one of the central concerns of deep learning. The Output Shape column describes the number and shape of feature maps associated with each layer. For example, the first layer has 32 feature maps, each of size \\(148\n\\times 148\\). Deeper parts of the network have more layers, but each is smaller. We will see how to load and inspect these features in the next lecture.\n\n\nf &lt;- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel &lt;- load_model_hdf5(f)\nmodel\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_7 (Conv2D)                  (None, 148, 148, 32)            896         \n max_pooling2d_7 (MaxPooling2D)     (None, 74, 74, 32)              0           \n conv2d_6 (Conv2D)                  (None, 72, 72, 64)              18496       \n max_pooling2d_6 (MaxPooling2D)     (None, 36, 36, 64)              0           \n conv2d_5 (Conv2D)                  (None, 34, 34, 128)             73856       \n max_pooling2d_5 (MaxPooling2D)     (None, 17, 17, 128)             0           \n conv2d_4 (Conv2D)                  (None, 15, 15, 128)             147584      \n max_pooling2d_4 (MaxPooling2D)     (None, 7, 7, 128)               0           \n flatten_1 (Flatten)                (None, 6272)                    0           \n dropout (Dropout)                  (None, 6272)                    0           \n dense_3 (Dense)                    (None, 512)                     3211776     \n dense_2 (Dense)                    (None, 1)                       513         \n================================================================================\nTotal params: 3453121 (13.17 MB)\nTrainable params: 3453121 (13.17 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\n\nWhile we will only consider image data in this course, the idea of learning complex features by composing a few types of layers is a general one. For example, in sentiment analysis, the first layer learns features that activate when specific combinations of words are present in close proximity to one another. The next layer learns interactions between phrases, and later layers are responsive to more sophisticated grammar.\nDeep learning is often called a black box because these intermediate features are often complex and not directly interpretable according to human concepts. The problem is further complicated by the fact that features are “distributed” in the sense that a single human concept can be encoded by a configuration of multiple features. Conversely, the same model feature can encode several human concepts.\nFor this reason, a literature has grown around the question of interpreting neural networks. The field relies on visualization and interaction to attempt to understand the learned representations, with the goal of increasing the safety and scientific usability of deep learning models. While our class will not discuss how to design or develop deep learning models, we will get a taste of the interpretability literature in the next few lectures."
  },
  {
    "objectID": "content/13-4.html",
    "href": "content/13-4.html",
    "title": "Optimizing Feature Maps",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(\"dplyr\")\nlibrary(\"purrr\")\nlibrary(\"keras\")\nlibrary(\"tensorflow\")\n\n\nSo far, we’ve visualized neural networks by analyzing the activations of learned features across observed samples. A complementary approach is to ask instead — is there a hypothetical image that would maximize the activation of a particular neuron? If we can construct such an image, then we might have a better sense of the types of image concepts to which a neuron is highly sensitive.\nWe will illustrate these ideas on a network that has been trained on Imagenet. This is a large image dataset with many (thousands of) class labels, and it is often used to evaluate image classification algorithms. The network is loaded below.\n\n\nmodel &lt;- application_vgg16(weights = \"imagenet\", include_top = FALSE)\n\n\nThe main idea is to setup an optimization problem that searches through image space for an image that maximizes the activation for a particular neuron. The function below computes the average activation of a one of the feature maps. The goal is to find an image that maximizes this value for a given feature.\n\n\nmean_activation &lt;- function(image, layer, ix=1) {\n  h &lt;- layer(image)\n  k_mean(h[,,, ix])\n}\n\n\nTo implement this, we can compute the gradient of a neuron’s average activation with respect to input image pixel values. This is a measure of how much the activation would change when individual pixel values are perturbed. The function below moves an input image in the direction of steepest ascent for the mean_activation function above.\n\n\ngradient_step &lt;- function(image, layer, ix=1, lr=1e-3) {\n  with(tf$GradientTape() %as% tape, {\n    tape$watch(image)\n    objective &lt;- mean_activation(image, layer, ix)\n  })\n  grad &lt;- tape$gradient(objective, image)\n  image &lt;- image + lr * grad\n}\n\n\n\n\n\n\nStarting from a random image, we can take a gradient step in the image space to increase a given neuron’s mean activation.\n\n\n\n\n\nOnce these gradients can be computed, it’s possible to perform gradient ascent to solve the activation maximization problem. This ascent is encoded by the function below. We initialize with a random uniform image and then take n_iter gradient steps in the direction that maximizes the activation of feature ix.\n\n\nrandom_image &lt;- function() {\n  tf$random$uniform(map(c(1, 150, 150, 3), as.integer))\n}\n\ngradient_ascent &lt;- function(layer, ix = 1, n_iter = 100, lr = 10) {\n  im_seq &lt;- array(0, dim = c(n_iter, 150, 150, 3))\n  image &lt;- random_image()\n  for (i in seq_len(n_iter)) {\n    image &lt;- gradient_step(image, layer, ix, lr)\n    im_seq[i,,,] &lt;- as.array(image[1,,,])\n  }\n  \n  im_seq\n}\n\n\n\n\n\n\nTaking many gradient steps leads us towards an image that optimizes a neuron’s activation.\n\n\n\n\n\nBelow, we visualize the images that optimize the activations for a few neurons in layer 3. These neurons seem to be most responsive particular colors and edge orientations.\n\n\nsquash &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\npar(mfrow = c(5, 8), mai = rep(0.00, 4))\nactivation_model &lt;- keras_model(inputs = model$input, outputs = model$layers[[3]]$output)\nfor (i in seq_len(40)) {\n  im_seq &lt;- gradient_ascent(activation_model, ix = i)\n  plot(as.raster(squash(im_seq[100,,,])))\n}\n\n\n\n\nThe hypothetical images that maximize the activations for 40 different neurons. These neurons seem to pull out features related to color and edge orientations.\n\n\n\n\n\nWe can think of these features as analogous to a collection of basis functions. At the first layer, the network is representing each image as a combination of basis images, related to particular color or edge patterns.\nWe can compare these activation maximizing inputs with those associated with later layers. It seems that the basis images at this level are more intricate, reflecting textures and common objects across this dataset. For example, the polka dot pattern may be strongly activated by cat eyes.\n\n\npar(mfrow = c(5, 8), mai = rep(0.00, 4))\nactivation_model &lt;- keras_model(inputs = model$input, outputs = model$layers[[8]]$output)\nfor (i in seq_len(40)) {\n  im_seq &lt;- gradient_ascent(activation_model, ix = i)\n  plot(as.raster(squash(im_seq[100,,,])))\n}\n\n\n\n\nThe results of the corresponding optimization for 40 neurons in layer 8."
  },
  {
    "objectID": "content/9-2.html",
    "href": "content/9-2.html",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(knitr)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\nIn reality, data are rarely separated into a clear number of homogeneous clusters. More often, even once a cluster formed, it’s possible to identify a few subclusters. For example, if you initially clustered movies into “drama” and “scifi”, you might be able to further refine the scifi cluster into “time travel” and “aliens.”\n\\(K\\)-means only allows clustering at a single level of magnification. To instead simultaneously cluster across scales, you can use an approach called hierarchical clustering. As a first observation, note that a tree can be used to implicitly store many clusterings at once. You can get a standard clustering by cutting the tree at some level.\n\n\n\n\n\n\nWe can recover clusters at different levels of granularity by cutting a hierarchical clustering tree.\n\n\n\n\n\nThese hierarchical clustering trees can be thought of abstract versions of the taxonomic trees. Instead of relating species, they relate observations in a dataset.\n\n\n\n\nElaborating on this analogy, the leaves of a hierarchical clustering tree are the original observations. The more recently two nodes share a common ancestor, the more similar those observations are.\nThe specific algorithm proceeds as follows,\n\nInitialize: Associate each point with a cluster \\(C_i := \\{x_i\\}\\).\nIterate until only one cluster: Look at all pairs of clusters. Merge the pair \\(C_k, C_{k^{\\prime}}\\) which are the most similar.\n\n\n\n\n\n\n\nAt initialization the hierarchical clustering routine has a cluster for each observation.\n\n\n\n\n\n\n\n\n\nNext the two closest observations are merged into one cluster. This is the first merge point on the tree.\n\n\n\n\n\n\n\n\n\nWe continue this at the next iteration though this time we have compute the pairwise distance between all clusters not observations (technically all the observations were their own cluster at the first step and in both cases we compare the pairwise distances between clusters).\n\n\n\n\n\n\n\n\n\nWe can continue this process…\n\n\n\n\n\n\n\n\n\n… and eventually we will construct the entire tree.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn R, this can be accomplished by using the hclust function. First, we compute the distances between all pairs of observations (this provides the similarities used in the algorithm). Then, we apply hclust to the matrix of pairwise distances.\nWe apply this to a movie ratings dataset. Movies are considered similar if they tend to receive similar ratings across all audience members. The result is visualized below.\n\n\nmovies_mat &lt;- read_csv(\"https://uwmadison.box.com/shared/static/wj1ln9xtigaoubbxow86y2gqmqcsu2jk.csv\")\n\nD &lt;- movies_mat %&gt;%\n  column_to_rownames(var = \"title\") %&gt;%\n  dist()\n\nhclust_result &lt;- hclust(D)\nplot(hclust_result, cex = 0.5)\n\n\n\n\n\n\n\n\n\nWe can customize our tree visualization using the ggraph package. We can convert the hclust object into a ggraph, using the same as_tbl_graph function from the network and trees lectures.\n\n\nhclust_graph &lt;- as_tbl_graph(hclust_result, height = height)\nhclust_graph &lt;- hclust_graph %&gt;%\n  mutate(height = ifelse(height == 0, 27, height)) # shorten the final edge\nhclust_graph\n\n# A tbl_graph: 99 nodes and 98 edges\n#\n# A rooted tree\n#\n# Node Data: 99 × 4 (active)\n   height leaf  label                                members\n    &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;                                  &lt;int&gt;\n 1   27   TRUE  \"Schindler's List\"                         1\n 2   27   TRUE  \"Forrest Gump\"                             1\n 3   27   TRUE  \"Shawshank Redemption, The\"                1\n 4   27   TRUE  \"Pulp Fiction\"                             1\n 5   27   TRUE  \"Silence of the Lambs, The\"                1\n 6   58.7 FALSE \"\"                                         2\n 7   63.9 FALSE \"\"                                         3\n 8   64.9 FALSE \"\"                                         4\n 9   67.9 FALSE \"\"                                         5\n10   27   TRUE  \"Star Wars: Episode IV - A New Hope\"       1\n# ℹ 89 more rows\n#\n# Edge Data: 98 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1     6     4\n2     6     5\n3     7     3\n# ℹ 95 more rows\n\n\n\nggraph(hclust_graph, \"dendrogram\", height = height, circular = TRUE) +\n  geom_edge_elbow() +\n  geom_node_text(aes(label = label), size = 4) +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nWe can cut the tree to recover a standard clustering. This is where the grammar-of-graphics approach from ggraph becomes useful – we can encode the cluster membership of a movie using color, for example.\n\n\ncluster_df &lt;- cutree(hclust_result, k = 10) %&gt;% # try changing K and regenerating the graph below\n  tibble(label = names(.), cluster = as.factor(.))\ncluster_df\n\n# A tibble: 50 × 3\n       . label                cluster\n   &lt;int&gt; &lt;chr&gt;                &lt;fct&gt;  \n 1     1 Seven (a.k.a. Se7en) 1      \n 2     1 Usual Suspects, The  1      \n 3     2 Braveheart           2      \n 4     2 Apollo 13            2      \n 5     3 Pulp Fiction         3      \n 6     4 Forrest Gump         4      \n 7     2 Lion King, The       2      \n 8     2 Mask, The            2      \n 9     2 Speed                2      \n10     2 Fugitive, The        2      \n# ℹ 40 more rows\n\n\n\n# colors chosen using https://medialab.github.io/iwanthue/\ncols &lt;- c(\"#51b48c\", \"#cf3d6e\", \"#7ab743\", \"#7b62cb\", \"#c49644\", \"#c364b9\", \"#6a803a\", \"#688dcd\", \"#c95a38\", \"#c26b7e\")\nhclust_graph %&gt;%\n  left_join(cluster_df) %&gt;%\n  ggraph(\"dendrogram\", height = height, circular = TRUE) +\n  geom_edge_elbow() +\n  geom_node_text(aes(label = label, col = cluster), size = 4) +\n  coord_fixed() +\n  scale_color_manual(values = cols) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "content/8-4.html",
    "href": "content/8-4.html",
    "title": "Enclosure",
    "section": "",
    "text": "Reading (Chapter 9), Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(gridExtra)\ntheme_set(theme_graph())\n\n\nIf nodes can be conceptually organized into a hierarchy, then it’s possible to use enclosure (i.e., the containment of some visual marks within others) to encode those relationships.\n\n\ngraph &lt;- tbl_graph(flare$vertices, flare$edges)\n\np1 &lt;- ggraph(graph, \"tree\") +\n  geom_edge_link() +\n  geom_node_point(aes(size = size)) +\n  scale_size(range = c(0.1, 5))\np2 &lt;- ggraph(graph, \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth)) +\n  scale_fill_distiller(direction = 1) +\n  coord_fixed()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nA tree and the equivalent representation using containment. The outer circle corresponds to the root node in the tree and paths down the tree are associated with sequences of nested circles.\n\n\n\n\n\nHierarchy most obviously occurs in trees, but it can also be present in networks with clustering structure at several levels. (see point 6 below).\nEnclosure is used in treemaps. In this visualization, each node is allocated an area, and all its children are drawn within that area (and so on, recursively, down to the leaves).\n\n\nggraph(graph, \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth, size = depth), size = 0.25) +\n  scale_fill_distiller(direction = 1) +\n  coord_fixed()\n\n\n\n\nA treemap representation associated with the tree from above.\n\n\n\n\n\nThis is a particularly useful visualization when it’s important to visualize a continuous attribute associated with each node. For example, a large node might correspond to a large part of a budget or a large directory in a filesystem. Here is an example visualization of Obama’s budget proposal in 2013.\nA caveat: treemaps are not so useful for making topological comparisons, like the distance between two nodes in the tree.\nIn situations where network nodes can be organized hierarchically, containment marks can directly represent the these relationships. For example, in the network below, the red and yellow clusters are contained in a green supercluster. The combination of node-link diagram and containment sets makes this structure clear.\n\n\n\n\n\n\nAn example of how hierarchy across groups of nodes can be encoded within a network."
  },
  {
    "objectID": "content/12-1.html",
    "href": "content/12-1.html",
    "title": "Partial Dependence Profiles I",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nlibrary(caret)\nlibrary(tidyverse)\nlibrary(DALEX)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nIllustration of the \\(\\mathbf{x^{j \\vert = z}\\) operation. The \\(j^{th}\\) coordinate (1 in this case) for a selected observation is set equal to \\(z\\).\nAn example of why it is difficult to summarize the relationship between an input variable and a fitted surface for nonlinear models.\nVisual intuition behind the CP profile. Varying the \\(j^{th\\) coordinate for an observation traces out a curve in the prediction surface.\ndata(titanic)\ntitanic &lt;- select(titanic, -country) %&gt;%\n  na.omit()\n\nx &lt;- select(titanic, -survived)\nhyper &lt;- data.frame(n.trees = 100, interaction.depth = 8, shrinkage = 0.1, n.minobsinnode = 10)\nfit &lt;- train(x = x, y = titanic$survived, method = \"gbm\", tuneGrid = hyper, verbose = F)\nexplanation &lt;- explain(model = fit, data = x, y = titanic$survived)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  train  (  default  )\n  -&gt; data              :  2179  rows  7  cols \n  -&gt; target variable   :  2179  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 7.0.1 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0.01454706 , mean =  0.3274567 , max =  0.9869868  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \n\nprofile &lt;- model_profile(explainer = explanation)\nplot(profile, geom = \"profiles\", variables = \"age\") +\n  theme479\n\n\n\n\nCP and PDP profiles for age for a GBM fitted to the Titanic dataset.\nplot(profile, geom = \"aggregates\") +\n  theme479"
  },
  {
    "objectID": "content/12-1.html#footnotes",
    "href": "content/12-1.html#footnotes",
    "title": "Partial Dependence Profiles I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCeteris Paribus means « All Else Held Equal. »↩︎\nTechnically, these are all predicted probabilities from the model.↩︎"
  },
  {
    "objectID": "content/10-1.html",
    "href": "content/10-1.html",
    "title": "Introduction to Dimensionality Reduction",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nHigh-dimensional data are data where many features are collected for each observation. These tend to be wide datasets with many columns. The name comes from the fact that each row of the dataset can be viewed as a vector in a high-dimensional space (one dimension for each feature). These data are common in modern applications,\n\n\nEach cell in a genomics dataset might have measurements for hundreds of molecules.\nEach survey respondent might provide answers to dozens of questions.\nEach image might have several thousand pixels.\nEach document might have counts across several thousand relevant words.\n\n\nFor low-dimensional data, we could visually encode all the features in our data directly, either using properties of marks or through faceting. In high-dimensional data, this is no longer possible.\nHowever, though there are many features associated with each observation, it may still be possible to organize samples across a smaller number of meaningful, derived features.\nFor example, consider the Metropolitan Museum of Art dataset, which contains images of many artworks. Abstractly, each artwork is a high-dimensional object, containing pixel intensities across many pixels. But it is reasonable to derive a feature based on the average brightness.\n\n\n\n\n\n\nAn arrangement of artworks according to their average pixel brightness as given in the reading.\n\n\n\n\n\nIn general, manual feature construction can be difficult. Algorithmic approaches try streamline the process of generating these maps by optimizing some more generic criterion. Different algorithms use different criteria, which we will review in the next couple of lectures.\n\n\n\n\n\n\nThe dimensionality reduction algorithm in this animation converts a large number of raw features into a position on a one-dimensional axis defined by average pixel brightness. In general we might reduce to dimensions other than 1D and we will often want to define features tailored to the dataset at hand.\n\n\n\n\n\nInformally, the goal of dimensionality reduction techniques is to produce a low-dimensional “atlas” relating members of a collection of complex objects. Samples that are similar to one another in the high-dimensional space should be placed near one another in the low-dimensional view. For example, we might want to make an atlas of artworks, with similar styles and historical periods being placed near to one another."
  },
  {
    "objectID": "content/12-2.html",
    "href": "content/12-2.html",
    "title": "Partial Dependence Profiles II",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(DALEX)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)"
  },
  {
    "objectID": "content/12-2.html#footnotes",
    "href": "content/12-2.html#footnotes",
    "title": "Partial Dependence Profiles II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is a simulated dataset, but designed to reflect properties of a real dataset.↩︎"
  },
  {
    "objectID": "content/8-2.html",
    "href": "content/8-2.html",
    "title": "Node - Link Diagrams",
    "section": "",
    "text": "Reading (Chapter 9), Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(ggraph)\nlibrary(gridExtra)\nlibrary(networkD3)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\nA node-link diagram is a visual encoding strategy for network data, where nodes are drawn as points and links between nodes are drawn as lines between them. The dataset below is a friendship network derived from a survey of high schoolers in 1957 and 1958, available in the tidygraph package.\n\n\nG_school &lt;- as_tbl_graph(highschool) %&gt;%\n  activate(edges) %&gt;%\n  mutate(year = factor(year))\n\nggraph(G_school) +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\nFor trees, the vertical or radial position can further encode the depth of a node in the tree. The data below represent the directory structure from a widely used web package called flare.\n\n\nG_flare &lt;- tbl_graph(flare$vertices, flare$edges)\np1 &lt;- ggraph(G_flare, 'tree') + \n  geom_edge_link() +\n  geom_node_label(aes(label = shortName), size = 3)\n\np2 &lt;- ggraph(G_flare, 'tree', circular = TRUE) + \n  geom_edge_link() +\n  geom_node_label(aes(label = shortName), size = 3)\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nThe same node-link diagram with either height or radius encoding depth in the tree.\n\n\n\n\n\nIn either trees or networks, attributes of nodes and edges can be encoded using size (node radius or edge width) or color.\nThe node-link representation is especially effective for the task of following paths. It’s an intuitive visualization for examining the local neighborhood of one node or describing the shortest path between two nodes.\nIn node-link diagrams, spatial position is subtle. It does not directly encode any attribute in the dataset, but layout algorithms (i.e., algorithms that try to determine the spatial positions of nodes in a node-link diagram) try to ensure that nodes that are close to one another in the shortest-path-sense also appear close to one another on the page.\n\n\np1 &lt;- ggraph(G_school, layout = \"kk\") +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\np2 &lt;- ggraph(G_school, layout = \"fr\") +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nA comparison of two layout algorithms for the same network.\n\n\n\n\n\nOne common layout algorithm uses force-directed placement. The edges here are interpreted as physical springs, and the node positions are iteratively updated according to the forces induced by the springs.\nHere is an interactive force-directed layout of the network from above, generated using the networkD3 package.\n\n\nschool_edges &lt;- G_school %&gt;%\n  activate(edges) %&gt;%\n  as.data.frame()\nsimpleNetwork(school_edges)\n\n\n\n\n\n\nThe key drawback of node-link diagrams is that they do not scale well to networks with a large number of nodes or with a large number of edges per node. The nodes and edges begin to overlap too much, and the result looks like a « hairball. »\nIn this situation, it is possible to use additional structure in the data to salvage the node-link display. For example, in a large tree, a rectangular or BubbleTree layout can be used.\n\n\n\n\n\n\nExample rectangular and BubbleTree layouts for very large trees as shown in Visualization Analysis and Design.\n\n\n\n\nIf a large network has a modular structure, then it is possible to first lay out the separate clusters far apart from one another, before running force directed placement.\n\n\n\n\n\nA hierarchical force directed layout algorithm as shown in Visualization Analysis and Design.\n\n\n\n\nIf many edges go through a few shared paths, it may be possible to bundle them.\n\n\n\n\n\nIn edge bundling similar paths are placed close to one another.\n\n\n\n\nBundled connections can be visualized using the geom_conn_bundle geometry in ggraph. Before using this layout, it is necessary to have a hierarchy over all the nodes, since shared ancestry among connected nodes is how the proximity of paths is determined.\n\nfrom &lt;- match(flare$imports$from, flare$vertices$name)\nto &lt;- match(flare$imports$to, flare$vertices$name)\nggraph(G_flare, layout = 'dendrogram', circular = TRUE) + \n  geom_conn_bundle(data = get_con(from = from, to = to), alpha = 0.1) + \n  geom_node_label(aes(label = shortName), size = 2) +\n  coord_fixed()\n\n\n\n\nAn example of hierarchical edge bundling in R.\n\n\n\n\n\nTo summarize, node-link diagrams are very good for characterizing local structure, but struggle with large networks."
  },
  {
    "objectID": "content/10-5.html",
    "href": "content/10-5.html",
    "title": "PCA and UMAP Examples",
    "section": "",
    "text": "Reading 1 and 2, Recording, Rmarkdown\nlibrary(embed)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(tidymodels)\nlibrary(tidytext)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nset.seed(479)"
  },
  {
    "objectID": "content/10-5.html#footnotes",
    "href": "content/10-5.html#footnotes",
    "title": "PCA and UMAP Examples",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSpecifically, we use a \\(\\log\\left(1 + x\\right)\\) transform, since there are many 0 counts.↩︎\n28 * 28 = 784↩︎"
  },
  {
    "objectID": "content/14-1.html",
    "href": "content/14-1.html",
    "title": "Final Takeaways",
    "section": "",
    "text": "Recording, Rmarkdown\n\nWe’ve covered many practical visualization strategies in this course. However, I hope a few overarching themes have come across as well. In these notes, I will try articulating a few of these themes, and they are also the subject of the readings reviewed in the next few notes.\nFirst, creating a visualization is in many ways like writing an essay. A great visualization cannot simply be summoned on demand. Instead, visualizations go through drafts, as the ideal graphical forms and questions of interest become clearer.\nVisualization is not simply for consumption by external stakeholders. In data science, visualization can be used to support the process of exploration and discovery, before any takeaways have yet been found.\nVisualization can be used throughout the data science process, from data quality assessment to model prediction analysis. In fact, I would be wary of any data science workflow that did not make use of visualization at multiple intermediate steps.\nIt pays dividends to think carefully about data format. A design can be easy to implement when the data are in tidy, but not wide, format, and vice versa.\nStructured, nontabular data – think time series, spatial formats, networks, text, and images – are everywhere, and specific visualization idioms are available for each.\nSimilarly, high-dimensional data are commonplace, but a small catalog of techniques, like faceting, dynamic linking, PCA, and UMAP, are able to take us quite far.\nFinally, data visualization can be enriching. Visual thinking has helped me navigate and appreciate complexity in many contexts. Through this course, I have shared techniques that I’ve found useful over the years, and I hope you find them handy as you go off and solve real-world data science problems."
  },
  {
    "objectID": "content/8-1.html",
    "href": "content/8-1.html",
    "title": "Introduction to Networks and Trees",
    "section": "",
    "text": "Reading 1 (Chapter 9), Reading 2, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\nNetworks and trees can be used to represent information in a variety of contexts. Abstractly, networks and trees are types of graphs, which are defined by (a) a set \\(V\\) of vertices and (b) a set \\(E\\) of edges between pairs of vertices.\nIt is helpful to have a few specific examples in mind,\n\nThe Internet: \\(V = \\{\\text{All Webpages}\\}, \\left(v, v^{\\prime}\\right) \\in E\\) if there is a hyperlink between pages \\(v\\) and \\(v^{\\prime}\\).\nEvolutionary Tree: \\(V = \\{\\text{All past and present species}\\}, \\left(v,\nv^{\\prime}\\right) \\in E\\) if one of the species \\(v\\) or \\(v^{\\prime}\\) is a descendant of the other.\nDisease Transmission: \\(V = \\{\\text{Community Members}\\}, \\left(v,\nv^{\\prime}\\right) \\in E\\) if the two community members have come in close contact.\nDirectory Tree: \\(V = \\{\\text{All directories in a computer}\\}, \\left(v,\nv^{\\prime}\\right) \\in E\\) if one directory is contained in the other.\n\n\n\n\n\n\n\nA visualization of the internet from the Opte Project.\n\n\n\n\n\n\n\n\n\nAn evolutionary tree from the Interactive Tree of Life.\n\n\n\n\n\n\n\n\n\nA COVID-19 transmission network from Clustering and superspreading potential of SARS-CoV-2 infections in Hong Kong.\n\n\n\n\n\n\n\n\n\nDirectories in a file system can be organized into a tree with parent and child directories.\n\n\n\n\n\nEither vertices or edges might have attributes. For example, in the directory tree, we might know the sizes of the files (vertex attribute), and in the disease transmission network we might know the duration of contact between individuals (edge attribute).\nAn edge may be either undirected or directed. In a directed edge, one vertex leads to the other, while in an undirected edge, there is no sense of ordering.\nIn R, the tidygraph package can be used to manipulate graph data. It’s tbl_graph class stores node and edge attributes in a single data structure. and ggraph extends the usual ggplot2 syntax to graphs.\n\n\nE &lt;- data.frame(\n  source = c(1, 2, 3, 4, 5),\n  target = c(3, 3, 4, 5, 6)\n)\n\nG &lt;- tbl_graph(edges = E)\nG\n\n# A tbl_graph: 6 nodes and 5 edges\n#\n# A rooted tree\n#\n# Node Data: 6 × 0 (active)\n#\n# Edge Data: 5 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1     1     3\n2     2     3\n3     3     4\n# ℹ 2 more rows\n\n\nThis tbl_graph can be plotted using the code below. There are different geoms available for nodes and edges – for example, what happens if you replace geom_edge_link() with geom_edge_arc()?\n\nggraph(G, layout = 'kk') + \n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\nWe can mutate node and edge attributes using dplyr-like syntax. Before mutating edges, it’s necessary to call activate(edges).\n\n\nG &lt;- G %&gt;%\n  mutate(\n    id = row_number(),\n    group = id &lt; 4\n  ) %&gt;%\n  activate(edges) %&gt;%\n  mutate(width = runif(n()))\nG\n\n# A tbl_graph: 6 nodes and 5 edges\n#\n# A rooted tree\n#\n# Edge Data: 5 × 3 (active)\n   from    to width\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     3 0.867\n2     2     3 0.854\n3     3     4 0.846\n4     4     5 0.803\n5     5     6 0.355\n#\n# Node Data: 6 × 2\n     id group\n  &lt;int&gt; &lt;lgl&gt;\n1     1 TRUE \n2     2 TRUE \n3     3 TRUE \n# ℹ 3 more rows\n\n\nNow we can visualize these derived attributes using an aesthetic mapping within the geom_edge_link and geom_node_point geoms.\n\nggraph(G, layout = \"kk\") +\n  geom_edge_link(aes(width = width)) +\n  geom_node_label(aes(label = id))\n\n\n\n\nThe same network as above but with edge size encoding the weight attribute.\n\n\n\n\n\nExample Tasks\n\nWhat types of data that are amenable to representation by networks or trees? What visual comparisons do networks and trees facilitate?\nOur initial examples suggest that trees and networks can be used to represent either physical interactions or conceptual relationships. Typical tasks include,\n\nSearching for groupings\nFollowing paths\nIsolating key nodes\n\nBy “searching for groupings,” we mean finding clusters of nodes that are highly interconnected, but which have few links outside the cluster. This kind of modular structure might lend itself to deeper investigation within each of the clusters.\n\nClusters in a network of political blogs might suggest an echo chamber effect.\nGene clusters in a differential expression study might suggest pathways needed for the production of an important protein.\nClusters in a recipe network could be used identify different culinary techniques or cuisines.\n\n\n\n\n\n\n\nA representation of 1200 blogs before the 2004 election from The political blogosphere and the 2004 US election: divided they blog.\n\n\n\n\n\nBy “following paths,” we mean tracing the paths out from a particular node, to see which other nodes it is close to.\n\nFollowing paths in a citation network might reveal the chain of publications that led to an important discovery.\nFollowing paths in a recommendation network might suggest other users who might be interested in watching a certain movie.\n\n\n\n\n\n\n\nA recommendation network linking individuals and the movies that they viewed.\n\n\n\n\n\n“Isolating key nodes” is a more fuzzy concept, usually referring to the task of finding nodes that are exceptional in some way. For example, it’s often interesting to find nodes with many more connections than others, or which link otherwise isolated clusters.\n\nA node with many edges in a disease transmission network is a superspreader.\nA node that links two clusters in a citation network might be especially interdisciplinary.\nA node with large size in a directory tree might be a good target for reducing disk usage.\n\n\n\n\n\n\n\nThe scientific journal Social Networks links several publication communities as found by Betweenness Centrality as an Indicator of the Interdisciplinarity of Scientific Journals.\n\n\n\n\n\nIf you find these questions interesting, you might enjoy the catalog of examples on the website VisualComplexity."
  },
  {
    "objectID": "content/12-4.html",
    "href": "content/12-4.html",
    "title": "Prior and Posterior Predictives",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nlibrary(tidyverse)\nlibrary(rstan)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nf &lt;- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/2pzgdu7gyobhl5tezo63tns7by1aiy6d.rda\", f)\nGM &lt;- get(load(f))"
  },
  {
    "objectID": "content/12-4.html#footnotes",
    "href": "content/12-4.html#footnotes",
    "title": "Prior and Posterior Predictives",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBayesian regression models that allow different slopes for different groups are called hierarchical models.↩︎"
  },
  {
    "objectID": "content/14-2.html",
    "href": "content/14-2.html",
    "title": "Design Process Case Study",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nA few entries from the data visualization survey. The full data are publicly available here."
  },
  {
    "objectID": "content/14-2.html#footnotes",
    "href": "content/14-2.html#footnotes",
    "title": "Design Process Case Study",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsing vega-lite!↩︎\nAre there really 100x more pie charts than scatterplots in industry??↩︎\nPerhaps an instance of selection bias.↩︎\nThe less our eyes have to travel across the page to make a comparison, the more efficient the visualization.↩︎"
  },
  {
    "objectID": "content/12-5.html",
    "href": "content/12-5.html",
    "title": "Pointwise Diagnostics",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(loo)\nlibrary(ggrepel)\nlibrary(rstan)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\n\n\nAll the model visualization strategies we’ve looked at in the last few lectures have been dataset-wide. That is, we looked at properties of the dataset as a whole, and whether the model made sense globally, across the whole dataset. Individual observations might warrant special attention, though.\nThe block below loads in the fitted models from the previous set of notes.\n\n\ndownloader &lt;- function(link) {\n  f &lt;- tempfile()\n  download.file(link, f)\n  get(load(f))\n}\n\nmodels &lt;- downloader(\"https://uwmadison.box.com/shared/static/x7dotair443mhx34yzie3m3lrsvhk19a.rda\")\nGM &lt;- downloader(\"https://uwmadison.box.com/shared/static/2pzgdu7gyobhl5tezo63tns7by1aiy6d.rda\")\n\n\nA first diagnostic to consider is the leave-one-out predictive distribution. This is the probability \\(p\\left(y_{i} \\vert y_{-i}\\right)\\) of sample \\(i\\) after having fitted a model to all samples except \\(i\\). Ideally, most observations in the dataset to have high predictive probability.\n\nNote that this can be used for model comparison. Some models might have better per-sample leave-one-out predictive probabilities for almost all observations.\nThis is similar to a leave-one-out residual.\n\nIf we use rstan to fit a Bayesian model, then these leave-one-out probabilities can be estimated using the loo function in the loo package. The code below computes these probabilities for each model, storing the difference in predictive probabilities for models two and three in the diff23 variable.\n\n\nelpd &lt;- map(models, ~ loo(., save_psis = TRUE)$pointwise[, \"elpd_loo\"])\nelpd_diffs &lt;- GM@data %&gt;%\n  mutate(\n    ID = row_number(),\n    diff23 = elpd[[3]] - elpd[[2]]\n  )\n\noutliers &lt;- elpd_diffs %&gt;%\n  filter(abs(diff23) &gt; 6)\n\n\nWe plot the difference between these predictive probabilities below. The interpretation is that Ulaanbataar has much higher leave-one-out probability under the cluster-based model, perhaps because that model is able to group the countries with large deserts together with one another. On the other hand, Santo Domingo is better modeled by model 2, since it has higher leave-one-out probability in that model.\n\n\nggplot(elpd_diffs, aes(ID, diff23)) +\n  geom_point(\n    aes(col = super_region_name),\n    size = 0.9, alpha = 0.8\n    ) +\n  geom_text_repel(\n    data = outliers,\n    aes(label = City_locality),\n    size = 3 \n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    y = \"Influence (Model 2 vs. 3)\",\n    col = \"WHO Region\"\n  )\n\n\n\n\nThe difference in leave one out predictive probabilities for each sample according to the WHO-region and cluster based hierarchical models.\n\n\n\n\n\nAnother diagnostic is to consider the influence of an observation. Formally, the influence is a measure of how much the posterior predictive distribution changes when we leave one sample out. The idea is to measure the difference between the posterior predictives using a form of KL divergence, and note down the observations that lead to a very large difference in divergence.\n\n\n\n\n\n\nVisual intuition about the influence of observations. If the posterior predictive distributions shift substantially when an observation is included or removed then it is an influential observation.\n\n\n\n\n\nWhen using rstan, the influence measure can be computed by the psis function. The pareto_k diagnostic summarizes how much the posterior predictive shifts when an observation is or isn’t included. For example, in the figure below, observation 2674 (Ulaanbaatar again) is highly influential.\n\n\nloglik &lt;- map(models, ~ as.matrix(., pars = \"log_lik\"))\nkdata &lt;- GM@data %&gt;%\n  mutate(\n    k_hat = psis(loglik[[2]])$diagnostics$pareto_k,\n    Index = row_number()\n  )\noutliers &lt;- kdata %&gt;%\n  filter(k_hat &gt; 0.25)\n\nggplot(kdata, aes(x = Index, y = k_hat)) + \n  geom_point(aes(col = super_region_name), size = 0.5, alpha = 0.9) + \n  scale_color_brewer(palette = \"Set2\") +\n  geom_text_repel(data = outliers, aes(label = Index)) +\n  labs(y = \"k-hat\")\n\n\n\n\nThe influence of each sample on the final posterior distribution."
  },
  {
    "objectID": "content/14-3.html",
    "href": "content/14-3.html",
    "title": "Asking Better Questions",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nIn these notes, we will review “Tukey, Design Thinking, and Better Questions,” a blog post by Roger Peng. In the post, he discusses the importance of formulating good questions in data analysis. While the process of refining questions is central to any successful, real-world analysis, it is rarely discussed in formal statistics textbooks, which usually restrict themselves to developments in theory, methodology, or computation.\nThe post itself is based on a line of thinking from the statistician John Tukey’s 1962 paper, “The Future of Data Analysis.” Perhaps the most famous quote from this paper is,\n\nFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.\n\nand throughout the article, Tukey argues that data analysts should not (1) be called upon to provide stamps of authority and (2) be distracted by the allure of proving optimal algorithms in settings that are likely not that realistic.\nThis of course begs the question – what should data analysts be doing? Tukey never directly discusses this, but Peng’s thesis is that the purpose of data analysis is to spark the formulation of better questions. This idea is captured in the diagram below. The idea is that algorithms (visual, predictive, and inferential) can help us strengthen evidence that certain patterns exist. However, if those patterns answer irrelevant questions, then there is little point in the analysis. With so many algorithms available, it’s easy to spend substantial effort gathering evidence to answer low quality questions. Instead, a good data scientist should focus on improving the quality of the questions asked, even if that means that the answers will be less definitive.\n\n\n\n\n\n\nPeng’s summary of misplaced priorities in the data science process.\n\n\n\n\n\nWhat does it mean to ask better questions? Peng argues that better questions are “sharper,” providing more discriminating information and guiding research to more promising directions. In my view, a sharper question is one that helps inform decisions that have to be made (all data aside), either by adding to our body of beliefs or bringing new uncertainties into focus.\nThe implication is that data analysis is not about finding “the answer.” Instead, it should be about clarifying what can and cannot be answered based on the data. But more, it can help clarify what questions we really want answered – in Peng’s words, “we can learn more about ourselves by looking at the data.”\nThe final example in the reading is that the residuals in an analysis are themselves data worth examining. They inform whether a model (and the associated way of conceptualizing the problem) is really appropriate, or whether a reformulation would be beneficial.\nAt the end, the main idea is that data analysis equips us to ask better questions."
  },
  {
    "objectID": "content/8-3.html",
    "href": "content/8-3.html",
    "title": "Adjacency Matrix Views",
    "section": "",
    "text": "Reading (Chapter 9), Recording, Rmarkdown\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(gridExtra)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\ntheme_set(theme_graph())\nE &lt;- matrix(c(1, 3, 2, 3, 3, 4, 4, 5, 5, 6),\n  byrow = TRUE, ncol = 2) %&gt;%\n  as_tbl_graph() %&gt;%\n  mutate(\n    id = row_number(),\n    group = id &lt; 4\n  ) \n\np1 &lt;- ggraph(E, layout = 'kk') + \n    geom_edge_fan() +\n    geom_node_label(aes(label = id))\n\np2 &lt;- ggraph(E, \"matrix\") +\n    geom_edge_tile(mirror = TRUE, show.legend = TRUE) +\n    geom_node_text(aes(label = id), x = -.5, nudge_y = 0.5) +\n    geom_node_text(aes(label = id), y = 0.5, nudge_x = -0.5) +\n    scale_y_reverse(expand = c(0, 0, 0, 1.5)) + # make sure the labels aren't hidden\n    scale_x_discrete(expand = c(0, 1.5, 0, 0)) +\n    coord_fixed() # make sure squares, not rectangles\n\ngrid.arrange(p1, p2, ncol = 2)\nG_school &lt;- as_tbl_graph(highschool) %&gt;%\n  activate(edges) %&gt;%\n  mutate(year = factor(year))\n\nggraph(G_school, \"matrix\") +\n  geom_edge_tile() +\n  coord_fixed() +\n  facet_wrap(~ year)\nG &lt;- sample_islands(4, 40, 0.4, 15) %&gt;%\n  as_tbl_graph() %&gt;%\n  mutate(\n    group = rep(seq_len(4), each = 40),\n    group = as.factor(group)\n  )\n\nggraph(G, \"matrix\") +\n  geom_node_point(aes(col = group), x = -1) +\n  geom_node_point(aes(col = group), y = 1) +\n  geom_edge_tile(mirror = TRUE) +\n  scale_y_reverse() +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()\nThe example below takes a protein network and colors each edge according to the weight of experimental evidence behind that interaction.\nproteins &lt;- read_tsv(\"https://uwmadison.box.com/shared/static/t97v5315isog0wr3idwf5z067h4j9o7m.tsv\") %&gt;%\n  as_tbl_graph(from = node1, to = node2)\n\nggraph(proteins, \"matrix\") +\n  geom_edge_tile(aes(fill = combined_score), mirror = TRUE) +\n  coord_fixed() +\n  geom_node_text(aes(label = name), size = 3, x = -2.5, nudge_y = 0.5, hjust = 0) +\n  geom_node_text(aes(label = name), size = 3, angle = 90, y = 0.5, nudge_x = -0.5, hjust = 0) +\n  scale_y_reverse(expand = c(0, 0, 0, 2.7)) + # make sure the labels aren't hidden\n  scale_x_discrete(expand = c(0, 3, 0, 0)) +\n  scale_edge_fill_distiller() +\n  labs(edge_fill = \"Edge Confidence\")\nCliques bicliques and clusters are visually salient using either node-link or adjacency matrix representations of a matrix.\ngrid.arrange(p1, p2, ncol = 2)\nggraph(G, \"matrix\", sort.by = sample(1:160)) +\n  geom_node_point(aes(col = group), x = -1) +\n  geom_node_point(aes(col = group), y = 1) +\n  geom_edge_tile(mirror = TRUE) +\n  scale_y_reverse() +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()"
  },
  {
    "objectID": "content/8-3.html#footnotes",
    "href": "content/8-3.html#footnotes",
    "title": "Adjacency Matrix Views",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough viewed differently, this additional degree of freedom can facilitate richer comparisons.↩︎"
  },
  {
    "objectID": "content/10-4.html",
    "href": "content/10-4.html",
    "title": "Uniform Manifold Approximation and Projection",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(embed)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nNonlinear dimension reduction methods can give a more faithful representation than PCA when the data don’t lie on a low-dimensional linear subspace.\nFor example, suppose the data were shaped like this. There is no one-dimensional line through these data that separate the groups well. We will need an alternative approach to reducing dimensionality if we want to preserve nonlinear structure.\n\n\nmoons &lt;- read_csv(\"https://uwmadison.box.com/shared/static/kdt9qqvonhcz2ssb599p1nqganrg1w6k.csv\")\nggplot(moons, aes(X, Y, col = Class)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nAn example nonlinear dataset where projections onto any straight line will necessarily cause the classes to bleed together.\n\n\n\n\n\nFrom a high-level, the intuition behind UMAP is to (a) build a graph joining nearby neighbors in the original high-dimensional space, and then (b) layout the graph in a lower-dimensional space.\nFor example, consider the 2-dimensional sine wave below. If we build a graph, we can try to layout the resulting nodes and edges on a 1-dimensional line in a way that approximately preserves the ordering.\n\n\n\n\n\n\nUMAP (and many other nonlinear methods) begins by constructing a graph in the high-dimensional space whose layout in the lower dimensional space will ideally preserve the essential relationships between samples.\n\n\n\n\n\nA natural way to build a graph is to join each node to its \\(K\\) closest neighbors. The choice of \\(K\\) will influence the final reduction, and it is treated as a hyperparameter of UMAP.\n\n\n\n\n\n\nWhen using fewer nearest neighbors the final dimensionality reduction will place more emphasis on effectively preserving the relationships between points in local neighborhoods.\n\n\n\n\n\nLarger values of \\(K\\) prioritize preservation of global structure, while smaller \\(K\\) will better reflect local differences. This property is not obvious a priori, but is suggested by the simulations described in the reading.\n\n\n\n\n\n\nWhen using larger neighborhoods UMAP will place more emphasis on preserving global structure sometimes at the cost of local relationships between points.\n\n\n\n\n\nOne detail in the graph construction: In UMAP, the edges are assigned weights depending on the distance they span, normalized by the distance to the closest neighbor. Neighbors that are close, relative to the nearest neighbors, are assigned higher weights than those that are far away, and points that are linked by high weight edges are pulled together with larger force in the final graph layout. This is what the authors mean by using a “fuzzy” nearest neighbor graph. The fuzziness allows the algorithm to distinguish neighbors that are very close from those that are far, even though they all lie within a \\(K\\)-nearest-neighborhood.\nOnce the graph is constructed, there is the question of how the graph layout should proceed. UMAP uses a variant of force-directed layout, and the global strength of the springs is another hyperparameter. Lower tension on the springs allow the points to spread out more loosely, higher tension forces points closer together. This is a second hyperparameter of UMAP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two hyperparameters — the number of nearest neighbors \\(K\\) and the layout tension — are the only two hyperparameters of UMAP.\nYou can see more examples of what this algorithm does to toy datasets in the reading. Note in particular the properties that the algorithm does not preserve. The distance between clusters should not be interpreted, since it just means that the graph components were not connected. Similarly, the density of points is not preserved.\nIn R, we can implement this using almost the same code as we used for PCA. The step_umap command is available through the embed package.\n\n\ncocktails_df &lt;- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\numap_rec &lt;- recipe(~., data = cocktails_df) %&gt;%\n  update_role(name, category, new_role = \"id\") %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_umap(all_predictors(), neighbors = 20, min_dist = 0.1)\numap_prep &lt;- prep(umap_rec)\n\n\nUMAP returns a low-dimensional atlas relating the points, but it does not provide any notion of derived features.\n\n\nggplot(juice(umap_prep), aes(UMAP1, UMAP2)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 0.8) +\n  geom_text(aes(label = name), check_overlap = TRUE, size = 3, hjust = \"inward\")\n\n\n\n\nThe learned UMAP representation of the cocktails dataset.\n\n\n\n\n\nWe can summarize the properties of UMAP,\n\n\nGlobal or local structure: The number of nearest neighbors \\(K\\) used during graph construction can be used modulate the emphasis of global vs. local structure.\nNonlinear: UMAP can reflect nonlinear structure in high-dimensions.\nNo interpretable features: UMAP only returns the map between points, and there is no analog of components to describe how the original features were used to construct the map.\nSlower: While UMAP is much faster than comparable nonlinear dimensionality reduction algorithms, it is still slower than linear approaches.\nNondeterministic: The output from UMAP can change from run to run, due to randomness in the graph layout step. If exact reproducibility is required, a random seed should be set."
  },
  {
    "objectID": "content/14-4.html",
    "href": "content/14-4.html",
    "title": "A History of Data Visualization up to 1900",
    "section": "",
    "text": "Reading, Recording, Rmarkdown"
  },
  {
    "objectID": "content/14-4.html#the-golden-age",
    "href": "content/14-4.html#the-golden-age",
    "title": "A History of Data Visualization up to 1900",
    "section": "1850 - 1900: The Golden Age",
    "text": "1850 - 1900: The Golden Age\n\nIt might be counterintuitive that there was a golden age of visualization a century before the first computers were invented. However, a look at the visualizations from this period demonstrate that this was a period where visualizations inspired scientific discoveries, informed commercial decisions, and guided social reform.\nFor example, in public health, Florence Nightingale invented new visualizations to demonstrate the impact of sanitary practices in hospital-induced infections and death. Similarly, it was a visualization that guided John Snow to the source of the 1855 cholera epidemic.\n\n\n\n\n\n\nFlorence Nightingale’s visualization of hospital mortality statistics from the Crimean War used to support a campaign for sanitary reforms.\n\n\n\n\n\nSome of the graphical innovations include,\n\n\n3D function plots. The plot below, by Luigi Perozzo, shows population size broken down into age groups and traced over time.\n\n\n\n\n\n\nLuigi Perrozo’s 1879 3D visualizations of population over time.\n\n\n\n\n\nFlow diagrams. Charles-Joseph Minard, who we met before with the canal visualization, was a master of these displays. One in particular is widely considered a masterpiece, it shows the size of Napoleon’s army during it’s Russian Campaign.\n\n\n\n\n\n\nMinard’s flow display of the size of Napoleon’s army during the Russia Campaign.\n\n\n\n\n\nMultivariate visualization. Francis Galton made some of the first efforts to visualize more than 3 variables at a time. His Meteorographica, published in 1863, contained over 600 visualizations of weather data that had been collected for decades, but never visualized. One plot, shown below, led to the discovery of anticyclones.\n\n\n\n\n\n\nGalton’s display of weather patterns. Low pressure (black) areas tend to have clockwise wind patterns while high pressure (red) tends to have anticlockwise wind patterns.\n\n\n\n\n\nThis was also an age of state-sponsored atlases. More than sponsoring the collection of data, governments assembled teams to visualize the results for official publication. From 1879 to 1897, the French Ministry of Public Works published the Albums de Statistique Graphique, which under the guidance of Émile Cheysson, developed some of the most imaginative and ambitious visualizations of the era.\n\n\n\n\n\n\nA visualization of the flow of passengers and goods through railways from Paris. Each square shows the breakdown to cities further away and color encodes the railines."
  },
  {
    "objectID": "content/14-4.html#footnotes",
    "href": "content/14-4.html#footnotes",
    "title": "A History of Data Visualization up to 1900",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe word “statistics” comes from the same root as “state”, since it originally focused on data collected for governance.↩︎\nHe has been described as an “engineer, political economist and scoundrel.”↩︎"
  },
  {
    "objectID": "content/12-3.html",
    "href": "content/12-3.html",
    "title": "Visualization for Model Building",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nlibrary(rstan)\nlibrary(tidyverse)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nf &lt;- tempfile()\ndownload.file(\"https://github.com/jgabry/bayes-vis-paper/blob/master/bayes-vis.RData?raw=true\", f)\nGM &lt;- get(load(f))\nGM@data &lt;- GM@data %&gt;% \n  mutate(\n    log_pm25 = log(pm25), \n    log_sat = log(sat_2014)\n  )\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = super_region_name), size = 0.8, alpha = 0.7) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"WHO Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nThe relationship between satellite and ground station estimates of PM2.5.\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = super_region_name), size = 0.4, alpha = 0.7) +\n  geom_smooth(aes(col = super_region_name), method = \"lm\", se = F, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"WHO Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nThe relationship between these variables is not the same across regions.\naverage &lt;- GM@data %&gt;% \n  group_by(iso3) %&gt;% \n  summarise(pm25 = mean(pm25))\n\nclust &lt;- dist(average) %&gt;%\n  hclust() %&gt;%\n  cutree(k = 6)\n\nGM@data$cluster_region &lt;- map_chr(GM@data$iso3, ~ clust[which(average$iso3 == .)])\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = cluster_region), size = 0.4, alpha = 0.7) +\n  geom_smooth(aes(col = cluster_region), method = \"lm\", se = F, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"Cluster Region\"\n  ) +\n  coord_fixed()\n\n\n\n\nWe can define clusters of regions on our own using a hierarchical clustering."
  },
  {
    "objectID": "content/12-3.html#footnotes",
    "href": "content/12-3.html#footnotes",
    "title": "Visualization for Model Building",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nViewed differently, this is like adding an interaction between the satellite measurements and WHO region.↩︎"
  },
  {
    "objectID": "content/10-3.html",
    "href": "content/10-3.html",
    "title": "Principal Components Analysis II",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(tidymodels)\nlibrary(tidytext)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nHow should we visualize the results from PCA? There are three artifacts produced by the procedure worth considering — components, scores, and variances. The components describe derived features, the scores lay samples out on a map, and the variances summarize how much information was preserved by each dimension.\n\n\n# produced by code in previous notes\ncomponents &lt;- read_csv(\"https://uwmadison.box.com/shared/static/dituepd0751qqsims22v2liukuk0v4bf.csv\") %&gt;%\n  filter(component %in% str_c(\"PC\", 1:5))\nscores &lt;- read_csv(\"https://uwmadison.box.com/shared/static/qisbw1an4lo8naifoxyu4dqv4bfsotcu.csv\")\nvariances &lt;- read_csv(\"https://uwmadison.box.com/shared/static/ye125xf8800zc5eh3rfeyzszagqkaswf.csv\") %&gt;%\n  filter(terms == \"percent variance\")\n\n\nFirst, let’s see how much variance is explained by each dimension of the PCA. Without detouring into the mathematical explanation, the main idea is that, the more rapid the dropoff in variance explained, the better the low-dimensional approximation. For example, if the data actually lie on a 2D plane in a high-dimensional space, then the first two bars would contain all the variance (the rest would be zero).\n\n\nggplot(variances) +\n  geom_col(aes(component, value))\n\n\n\n\nProportion of variance explained by each component in the PCA.\n\n\n\n\n\nWe can interpret components by looking at the linear coefficients of the variables used to define them. From the plot below, we see that the first PC mostly captures variation related to whether the drink is made with powdered sugar or simple syrup. Drinks with high values of PC1 are usually to be made from simple syrup, those with low values of PC1 are usually made from powdered sugar. From the two largest bars in PC2, we can see that it highlights the vermouth vs. non-vermouth distinction.\n\n\nggplot(components, aes(value, terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  labs(y = NULL) +\n  theme(axis.text = element_text(size = 7))\n\n\n\n\nThe top 5 principal components associated with the cocktails dataset.\n\n\n\n\n\nIt is often easier read the components when the bars are sorted according to their magnitude. The usual ggplot approach to reordering axes labels, using either reorder() or releveling the associated factor, will reorder all the facets in the same way. If we want to reorder each facet on its own, we can use the reorder_within function coupled with scale_*_reordered, both from the tidytext package.\n\n\ncomponents_ &lt;- components %&gt;%\n  filter(component %in% str_c(\"PC\", 1:3)) %&gt;%\n  mutate(terms = reorder_within(terms, abs(value), component))\nggplot(components_, aes(value, terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(y = NULL) +\n  theme(axis.text = element_text(size = 7))\n\n\n\n\nThe top 3 principal components with defining variables sorted by the magnitude of their coefficient.\n\n\n\n\n\nNext, we can visualize the scores of each sample with respect to these components. The plot below shows \\(\\left(z_{i1}, z_{i2}\\right)\\). Suppose that the columns of \\(\\Phi\\) are \\(\\varphi_{1}, \\dots, \\varphi_{K}\\). Then, since \\(x_{i}\n\\approx \\varphi_{1}z_{i1} + \\varphi_{2} z_{i2}\\), the samples have large values for variables with large component values in the coordinate directions where \\(z_{i}\\) is farther along.\n\n\nggplot(scores, aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 1.5) +\n  geom_text_repel(check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) # rescale axes to reflect variance\n\n\n\n\nThe scores associated with the cocktails dataset.\n\n\n\n\n\nFor example, El Nino has high value for PC1, which means it has a high value of variables that are positive for PC1 (like simple syrup) and low value for those variables that are negative (like powdered sugar). Similarly, since \\(\\varphi_{2}\\) puts high positive weight on vermouth-related variables, so H. P. W. Cocktail has many vermouth-related ingredients.\nIn practice, it will often be important to visualize several pairs of PC dimensions against one another, not just the top 2.\nLet’s examine the original code in a little more detail. We are using tidymodels, which is a package for decoupling the definition and execution of a data pipeline. This compartmentalization makes it easier to design and reuse across settings.\n\n\ncocktails_df &lt;- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\npca_rec &lt;- recipe(~., data = cocktails_df) %&gt;%\n  update_role(name, category, new_role = \"id\") %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors())\n\npca_prep &lt;- prep(pca_rec)\n\n\nHere is how you would apply PCA without the tidymodels package. You have to first split the data into the “metadata” that is used to interpret the scores and the numerical variables used as input to PCA. Then at the end, you have to join the metadata back in. It’s not impossible, but the code is not as readable.\n\n\n# split name and category out of the data frame\npca_result &lt;- cocktails_df %&gt;%\n  select(-name, -category) %&gt;%\n  scale() %&gt;%\n  princomp()\n\n# join them back into the PCA result\nmetadata &lt;- cocktails_df %&gt;%\n  select(name, category)\nscores_direct &lt;- cbind(metadata, pca_result$scores)\n\nggplot(scores_direct, aes(Comp.1, Comp.2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 1.5) +\n  geom_text_repel(check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) # rescale axes to reflect variance\n\n\n\n\nA plot of the PCA scores made without using tidymodels.\n\n\n\n\n\nThe equivalent tidymodels implementation handles the difference between supplementary and modeling data less bluntly, setting the name and category variables to id roles, so that all_predictors() knows to skip them.\nWe conclude with some characteristics of PCA, which can guide the choice between alternative dimensionality reduction methods.\n\n\nGlobal structure: Since PCA is looking for high-variance overall, it tends to focus on global structure.\nLinear: PCA can only consider linear combinations of the original features. If we expect nonlinear features to be more meaningful, then another approach should be considered.\nInterpretable features: The PCA components exactly specify how to construct each of the derived features.\nFast: Compared to most dimensionality reduction methods, PCA is quite fast. Further, it is easy to implement approximate versions of PCA that scale to very large datasets.\nDeterministic: Some embedding algorithms perform an optimization process, which means there might be some variation in the results due to randomness in the optimization. In contrast, PCA is deterministic, with the components being unique up to sign (i.e., you could reflect the components across an axis, but that is the most the results might change)."
  },
  {
    "objectID": "content/10-2.html",
    "href": "content/10-2.html",
    "title": "Principal Components Analysis I",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidymodels)\nlibrary(readr)\n\n\nIn our last notes, we saw how we could organize a collection of images based on average pixel brightness. We can think of average pixel brightness as a derived feature that can be used to build a low-dimensional map.\nWe can partially automate the process of deriving new features. Though, in general, finding the best way to combine raw features into derived ones is a complicated problem, we can simplify things by restricting attention to,\n\n\nFeatures that are linear combinations of the raw input columns.\nFeatures that are orthogonal to one another.\nFeatures that have high variance.\n\n\nRestricting to linear combinations allows for an analytical solution. We will relax this requirement when discussing UMAP.\nOrthogonality means that the derived features will be uncorrelated with one another. This is a nice property, because it would be wasteful if features were redundant.\nHigh variance is desirable because it means we preserve more of the essential structure of the underlying data. For example, if you look at this 2D representation of a 3D object, it’s hard to tell what it is,\n\n\n\n\n\n\nWhat is this object?\n\n\n\n\nBut when viewing an alternative reduction which has higher variance…\n\n\n\n\n\nNot so complicated now. Credit for this example goes to Professor Julie Josse at Ecole Polytechnique.\n\n\n\n\n\nPrincipal Components Analysis (PCA) is the optimal dimensionality reduction under these three restrictions, in the sense that it finds derived features with the highest variance. Formally, PCA finds a matrix \\(\\Phi \\in \\mathbb{R}^{D\n\\times K}\\) and a set of vector \\(z_{i} \\in \\mathbb{R}^{K}\\) such that \\(x_{i}\n\\approx \\Phi z_{i}\\) for all \\(i\\). The columns of \\(\\Phi\\) are called principal components, and they specify the structure of the derived linear features. The vector \\(z_{i}\\) is called the score of \\(x_{i}\\) with respect to these components. The top component explains the most variance, the second captures the next most, and so on.\nFor example, if one of the columns of \\(\\Phi\\) was equal to \\(\\left(\\frac{1}{D},\n\\dots, \\frac{1}{D}\\right)\\), then that feature computes the average of all coordinates (e.g., to get average brightness), and the corresponding \\(z_{i}\\) would be a measure of the average brightness of sample \\(i\\).\nGeometrically, the columns of \\(\\Phi\\) span a plane that approximates the data. The \\(z_{i}\\) provide coordinates of points projected onto this plane.\n\n\n\n\n\n\nPCA finds a low-dimensional linear subspace that closely approximates the high-dimensional data.\n\n\n\n\n\nIn R, PCA can be conveniently implemented using the tidymodels package. We will see a base R implementation in the next lecture. The dataset below contains properties of a variety of cocktails, from the Boston Bartender’s guide. The first two columns are qualitative descriptors, while the rest give numerical ingredient information.\n\n\ncocktails_df &lt;- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\ncocktails_df[, 1:6]\n\n# A tibble: 937 × 6\n   name                 category light_rum lemon_juice lime_juice sweet_vermouth\n   &lt;chr&gt;                &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1 Gauguin              Cocktai…      2           1          1               0  \n 2 Fort Lauderdale      Cocktai…      1.5         0          0.25            0.5\n 3 Cuban Cocktail No. 1 Cocktai…      2           0          0.5             0  \n 4 Cool Carlos          Cocktai…      0           0          0               0  \n 5 John Collins         Whiskies      0           1          0               0  \n 6 Cherry Rum           Cocktai…      1.25        0          0               0  \n 7 Casa Blanca          Cocktai…      2           0          1.5             0  \n 8 Caribbean Champagne  Cocktai…      0.5         0          0               0  \n 9 Amber Amour          Cordial…      0           0.25       0               0  \n10 The Joe Lewis        Whiskies      0           0.5        0               0  \n# ℹ 927 more rows\n\n\n\nThe pca_rec object below defines a tidymodels recipe for performing PCA. Computation of the lower-dimensional representation is deferred until prep() is called. This delineation between workflow definition and execution helps clarify the overall workflow, and it is typical of the tidymodels package.\n\n\npca_rec &lt;- recipe(~., data = cocktails_df) %&gt;%\n  update_role(name, category, new_role = \"id\") %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors())\n\npca_prep &lt;- prep(pca_rec)\n\n\nThe step_normalize call is used to center and scale all the columns. This is needed because otherwise columns with larger variance will have more weight in the final dimensionality reduction, but this is not conceptually meaningful. For example, if one of the columns in a dataset were measuring length in kilometers, then we could artificially increase its influence in a PCA by expressing the same value in meters. To achieve invariance to this change in units, it would be important to normalize first.\nWe can tidy each element of the workflow object. Since PCA was the second step in the workflow, the PCA components can be obtained by calling tidy with the argument “2.” The scores of each sample with respect to these components can be extracted using juice. The amount of variance explained by each dimension is also given by tidy, but with the argument type = \"variance\". We’ll see how to visualize and interpret these results in the next lecture.\n\n\ntidy(pca_prep, 2)\n\n# A tibble: 1,600 × 4\n   terms             value component id       \n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 light_rum        0.163  PC1       pca_zn4D4\n 2 lemon_juice     -0.0140 PC1       pca_zn4D4\n 3 lime_juice       0.224  PC1       pca_zn4D4\n 4 sweet_vermouth  -0.0661 PC1       pca_zn4D4\n 5 orange_juice     0.0308 PC1       pca_zn4D4\n 6 powdered_sugar  -0.476  PC1       pca_zn4D4\n 7 dark_rum         0.124  PC1       pca_zn4D4\n 8 cranberry_juice  0.0954 PC1       pca_zn4D4\n 9 pineapple_juice  0.119  PC1       pca_zn4D4\n10 bourbon_whiskey  0.0963 PC1       pca_zn4D4\n# ℹ 1,590 more rows\n\n\n\njuice(pca_prep)\n\n# A tibble: 937 × 7\n   name                 category              PC1     PC2     PC3     PC4    PC5\n   &lt;chr&gt;                &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 Gauguin              Cocktail Classics   1.38  -1.15    1.34   -1.12    1.52 \n 2 Fort Lauderdale      Cocktail Classics   0.684  0.548   0.0308 -0.370   1.41 \n 3 Cuban Cocktail No. 1 Cocktail Classics   0.285 -0.967   0.454  -0.931   2.02 \n 4 Cool Carlos          Cocktail Classics   2.19  -0.935  -1.21    2.47    1.80 \n 5 John Collins         Whiskies            1.28  -1.07    0.403  -1.09   -2.21 \n 6 Cherry Rum           Cocktail Classics  -0.757 -0.460   0.909   0.0154 -0.748\n 7 Casa Blanca          Cocktail Classics   1.53  -0.392   3.29   -3.39    3.87 \n 8 Caribbean Champagne  Cocktail Classics   0.324  0.137  -0.134  -0.147   0.303\n 9 Amber Amour          Cordials and Liqu…  1.31  -0.234  -1.55    0.839  -1.19 \n10 The Joe Lewis        Whiskies            0.138 -0.0401 -0.0365 -0.100  -0.531\n# ℹ 927 more rows\n\n\n\ntidy(pca_prep, 2, type = \"variance\")\n\n# A tibble: 160 × 4\n   terms    value component id       \n   &lt;chr&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance  2.00         1 pca_zn4D4\n 2 variance  1.71         2 pca_zn4D4\n 3 variance  1.50         3 pca_zn4D4\n 4 variance  1.48         4 pca_zn4D4\n 5 variance  1.37         5 pca_zn4D4\n 6 variance  1.32         6 pca_zn4D4\n 7 variance  1.30         7 pca_zn4D4\n 8 variance  1.20         8 pca_zn4D4\n 9 variance  1.19         9 pca_zn4D4\n10 variance  1.18        10 pca_zn4D4\n# ℹ 150 more rows"
  },
  {
    "objectID": "content/9-3.html",
    "href": "content/9-3.html",
    "title": "Heatmaps",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"superheat\")\nlibrary(\"tibble\")\ntheme_set(theme_minimal())\n\n\nThe direct outputs of a standard clustering algorithim are (a) cluster assignments for each sample, (b) the centroids associated with each cluster. A hierarchical clustering algorithm enriches this output with a tree, which provide (a) and (b) at multiple levels of resolution.\nThese outputs can be used to improve visualizations. For example, they can be used to define small multiples, faceting across clusters. One especially common idea is to reorder the rows of a heatmap using the results of a clustering, and this is the subject of these notes.\nIn a heatmap, each mark (usually a small tile) corresponds to an entry of a matrix. The \\(x\\)-coordinate of the mark encodes the index of the observation, while the \\(y\\)-coordinate encodes the index of the feature. The color of each tile represents the value of that entry. For example, here are the first few rows of the movies data, along with the corresponding heatmap, made using the superheat package.\n\n\nmovies_mat &lt;- read_csv(\"https://uwmadison.box.com/shared/static/wj1ln9xtigaoubbxow86y2gqmqcsu2jk.csv\") %&gt;%\n  column_to_rownames(var = \"title\")\n\n\ncols &lt;- c('#f6eff7','#bdc9e1','#67a9cf','#1c9099','#016c59')\nsuperheat(movies_mat, left.label.text.size = 4, heat.pal = cols, heat.lim = c(0, 5))\n\n\n\n\n\n\n\n\n\nJust like in adjacency matrix visualizations, the effectiveness of a heatmap can depend dramatically on the way in which rows and columns are ordered. To provide a more coherent view, we cluster both rows and columns, placing rows / columns belonging to the same cluster next to one another.\n\n\nmovies_clust &lt;- movies_mat %&gt;%\n  kmeans(centers = 10)\n\nusers_clust &lt;- movies_mat %&gt;%\n  t() %&gt;%\n  kmeans(centers = 10)\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  order.rows = order(movies_clust$cluster),\n  order.cols = order(users_clust$cluster),\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\n\n\n\n\n\nsuperheat also makes it easy to visualize plot statistics adjacent ot the adjacent to the main heatmap. These statistics can be plotted as points, lines, or bars. Points are useful when we want to highlight the raw value, lines are effective for showing change, and bars give a sense of the area below a set of observations. In this example, we use an added panel on the right hand side (yr) to encode the total number of ratings given to that movie. The yr.obs.cols allows us to change the color of each point in the adjacent plot. In this example, we change color depending on which cluster the movie was found to belong to.\n\n\ncluster_cols &lt;- c('#8dd3c7','#ccebc5','#bebada','#fb8072','#80b1d3','#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd')\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  order.rows = order(movies_clust$cluster),\n  order.cols = order(users_clust$cluster),\n  heat.pal = cols,\n  heat.lim = c(0, 5),\n  yr = rowSums(movies_mat &gt; 0),\n  yr.axis.name = \"Number of Ratings\",\n  yr.obs.col = cluster_cols[movies_clust$cluster],\n  yr.plot.type = \"bar\"\n)\n\n\n\n\n\n\n\n\n\nIt also makes sense to order the rows / columns using hierarchical clustering. This approach is especially useful when the samples fall along a continuous gradient, rather than belonging to clearly delineated groups. The pretty.order.rows and pretty.order.cols arguments use hierarchical clustering to reorder the heatmap.\n\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  pretty.order.cols = TRUE,  \n  pretty.order.rows = TRUE,\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\n\n\n\n\n\nThe hierarchical clustering trees estimated by pretty.order.rows and pretty.order.cols can be also visualized.\n\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  pretty.order.cols = TRUE,  \n  pretty.order.rows = TRUE, \n  row.dendrogram = TRUE,\n  col.dendrogram = TRUE,\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)"
  },
  {
    "objectID": "content/11-4.html",
    "href": "content/11-4.html",
    "title": "Topic Modeling Case Study",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nlibrary(tidyverse)\nlibrary(superheat)\nlibrary(tidytext)\nlibrary(topicmodels)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nx &lt;- read_csv(\"https://uwmadison.box.com/shared/static/fd437om519i5mrnur14xy6dq3ls0yqt2.csv\")\nx\n\n# A tibble: 2,000,000 × 6\n   sample                       gene      tissue tissue_detail Description value\n   &lt;chr&gt;                        &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt;       &lt;dbl&gt;\n 1 GTEX-NFK9-0926-SM-2HMJU      ENSG0000… Heart  Heart - Left… FGR           368\n 2 GTEX-OXRO-0011-R10A-SM-2I5EH ENSG0000… Brain  Brain - Fron… FGR           593\n 3 GTEX-QLQ7-0526-SM-2I5G3      ENSG0000… Heart  Heart - Left… FGR           773\n 4 GTEX-POMQ-0326-SM-2I5FO      ENSG0000… Heart  Heart - Left… FGR           330\n 5 GTEX-QESD-0526-SM-2I5G5      ENSG0000… Heart  Heart - Left… FGR           357\n 6 GTEX-OHPN-0011-R4A-SM-2I5FD  ENSG0000… Brain  Brain - Amyg… FGR           571\n 7 GTEX-OHPK-0326-SM-2HMJO      ENSG0000… Heart  Heart - Left… FGR           391\n 8 GTEX-OIZG-1126-SM-2HMIU      ENSG0000… Heart  Heart - Left… FGR           425\n 9 GTEX-O5YW-0326-SM-2I5EI      ENSG0000… Heart  Heart - Left… FGR           172\n10 GTEX-REY6-1026-SM-2TF4Y      ENSG0000… Heart  Heart - Left… FGR           875\n# ℹ 1,999,990 more rows\nx_dtm &lt;- cast_dtm(x, sample, gene, value)\n#fit &lt;- LDA(x_dtm, k = 10, control = list(seed = 479))\n#save(fit, file = \"lda_gtex.rda\")\nf &lt;- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/ifgo6fbvm8bdlshzegb5ty8xif5istn8.rda\", f)\nfit &lt;- get(load(f))\ntissue_info &lt;- x %&gt;%\n  select(sample, starts_with(\"tissue\")) %&gt;%\n  unique()\n\ntopics &lt;- tidy(fit, matrix = \"beta\") %&gt;%\n  mutate(topic = factor(topic))\nmemberships &lt;- tidy(fit, matrix = \"gamma\") %&gt;%\n  mutate(topic = factor(topic)) %&gt;%\n  left_join(tissue_info, by = c(\"document\" = \"sample\"))\ndiscriminative_genes &lt;- topics %&gt;%\n  group_by(term) %&gt;%\n  mutate(D = discrepancy(beta)) %&gt;%\n  ungroup() %&gt;%\n  slice_max(D, n = 400) %&gt;%\n  mutate(term = fct_reorder(term, -D))\n\ndiscriminative_genes %&gt;%\n  pivot_wider(names_from = topic, values_from = beta) %&gt;%\n  column_to_rownames(\"term\") %&gt;%\n  superheat(\n    pretty.order.rows = TRUE,\n    left.label.size = 1.5,\n    left.label.text.size = 3,\n    bottom.label.size = 0.05,\n    legend = FALSE\n  )\n\n\n\n\nA heatmap of the most discriminative genes across the 10 estimated topics.\nkeep_tissues &lt;- memberships %&gt;%\n  count(tissue) %&gt;%\n  filter(n &gt; 70) %&gt;%\n  pull(tissue)\n\nhclust_result &lt;- hclust(dist(fit@gamma))\ndocument_order &lt;- fit@documents[hclust_result$order]\nmemberships &lt;- memberships %&gt;%\n  filter(tissue %in% keep_tissues) %&gt;%\n  mutate(document = factor(document, levels = document_order))\nggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +\n  geom_col(position = position_stack()) +\n  facet_grid(tissue ~ ., scales = \"free\", space = \"free\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_color_brewer(palette = \"Set3\", guide = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(x = \"Topic Membership\", y = \"Sample\", fill = \"Topic\") +\n  theme(\n    panel.spacing = unit(0.5, \"lines\"),\n    strip.switch.pad.grid = unit(0, \"cm\"),\n    strip.text.y = element_text(size = 8, angle = 0),\n    axis.text.y = element_blank(),\n  )\n\n\n\n\nA structure plot showing the topic memberships across all tissue samples in the dataset."
  },
  {
    "objectID": "content/11-4.html#footnotes",
    "href": "content/11-4.html#footnotes",
    "title": "Topic Modeling Case Study",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, topic models are an example of a larger family of models, called mixed-membership models. All of these models generalize clustering, and different variants can be applied to other data types, like continuous, categorical, and network data.↩︎"
  },
  {
    "objectID": "content/9-1.html",
    "href": "content/9-1.html",
    "title": "K-means",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nlibrary(\"dslabs\")\nlibrary(\"ggplot2\")\nlibrary(\"knitr\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\ntheme_set(theme_minimal())\nHere is an animation from the tidymodels page on \\(K\\)-means,\ndata(\"movielens\")\nfrequently_rated &lt;- movielens %&gt;%\n  group_by(movieId) %&gt;%\n  summarize(n=n()) %&gt;%\n  top_n(50, n) %&gt;%\n  pull(movieId)\n\nmovie_mat &lt;- movielens %&gt;% \n  filter(movieId %in% frequently_rated) %&gt;%\n  select(title, userId, rating) %&gt;%\n  pivot_wider(title, names_from = userId, values_from = rating, values_fill = 0)\n\nmovie_mat[1:10, 1:20]\n\n# A tibble: 10 × 20\n   title   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`  `13`\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Seve…     4   0       0     0     0     0   5       3     0     0     0   2.5\n 2 Usua…     4   0       0     0     0     0   5       0     5     5     0   0  \n 3 Brav…     4   4       0     0     0     5   4       0     0     0     0   4  \n 4 Apol…     5   0       0     4     0     0   0       0     0     0     0   0  \n 5 Pulp…     4   4.5     5     0     0     0   4       0     0     5     0   3.5\n 6 Forr…     3   5       5     4     0     3   4       0     0     0     0   5  \n 7 Lion…     3   0       5     4     0     3   0       0     0     0     0   0  \n 8 Mask…     3   0       4     4     0     3   0       0     0     0     0   0  \n 9 Speed     3   2.5     0     4     0     3   0       0     0     0     0   0  \n10 Fugi…     3   0       0     0     0     0   4.5     0     0     0     0   0  \n# ℹ 7 more variables: `14` &lt;dbl&gt;, `15` &lt;dbl&gt;, `16` &lt;dbl&gt;, `17` &lt;dbl&gt;,\n#   `18` &lt;dbl&gt;, `19` &lt;dbl&gt;, `20` &lt;dbl&gt;\nkclust &lt;- movie_mat %&gt;%\n  select(-title) %&gt;%\n  kmeans(centers = 10)\n\nmovie_mat &lt;- augment(kclust, movie_mat) # creates column \".cluster\" with cluster label\nkclust &lt;- tidy(kclust)\n\nmovie_mat %&gt;%\n  select(title, .cluster) %&gt;%\n  arrange(.cluster)\n\n# A tibble: 50 × 2\n   title                                                                .cluster\n   &lt;chr&gt;                                                                &lt;fct&gt;   \n 1 Star Wars: Episode VI - Return of the Jedi                           1       \n 2 Star Wars: Episode IV - A New Hope                                   1       \n 3 Star Wars: Episode V - The Empire Strikes Back                       1       \n 4 Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost … 1       \n 5 Princess Bride, The                                                  2       \n 6 Men in Black (a.k.a. MIB)                                            2       \n 7 Titanic                                                              2       \n 8 E.T. the Extra-Terrestrial                                           2       \n 9 Terminator, The                                                      2       \n10 Groundhog Day                                                        2       \n# ℹ 40 more rows\nkclust_long &lt;- kclust %&gt;%\n  pivot_longer(`2`:`671`, names_to = \"userId\", values_to = \"rating\")\n\nggplot(kclust_long) +\n  geom_bar(\n    aes(x = reorder(userId, rating), y = rating),\n    stat = \"identity\"\n  ) +\n  facet_grid(cluster ~ .) +\n  labs(x = \"Users (sorted)\", y = \"Rating\") +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 5),\n    strip.text.y = element_text(angle = 0)\n  )\n\n\n\n\nWe can visualize each cluster by seeing the average ratings each user gave to the movies in that cluster (this is the definition of the centroid). An alternative visualization strategy would be to show a heatmap – we’ll discuss this soon in the superheat lecture.\nThe difficulty that variations in density poses to k-means from Cluster Analysis using K-Means Explained."
  },
  {
    "objectID": "content/9-1.html#footnotes",
    "href": "content/9-1.html#footnotes",
    "title": "K-means",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo work with data of different types, it’s possible to use a variant called \\(K\\)-medoids, but this is beyond the scope of this class.↩︎\nThis is an approximation – there are probably many movies that the users would have enjoyed had they had a chance to watch them.↩︎"
  },
  {
    "objectID": "content/11-3.html",
    "href": "content/11-3.html",
    "title": "Visualizing Topic Models",
    "section": "",
    "text": "Reading 1 and 2, Recording, Rmarkdown\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(superheat)\nlibrary(tidytext)\nlibrary(topicmodels)\ntheme479 &lt;- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nmemberships &lt;- read_csv(\"https://uwmadison.box.com/shared/static/c5k5iinwo9au44fb3lc00vq6isbi72c5.csv\")\ntopics &lt;- read_csv(\"https://uwmadison.box.com/shared/static/uh34hhc1wnp072zcryisvgr3z0yh25ad.csv\")"
  },
  {
    "objectID": "content/11-3.html#footnotes",
    "href": "content/11-3.html#footnotes",
    "title": "Visualizing Topic Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nColor is in general harder to compare than bar height.↩︎"
  },
  {
    "objectID": "content/9-4.html",
    "href": "content/9-4.html",
    "title": "Silhouette Statistics",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nlibrary(\"cluster\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"tidymodels\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\ntheme_set(theme_bw())\nset.seed(123)\npenguins &lt;- read_csv(\"https://uwmadison.box.com/shared/static/ijh7iipc9ect1jf0z8qa2n3j7dgem1gh.csv\") %&gt;%\n  na.omit() %&gt;%\n  mutate(id = row_number())\n\ncluster_penguins &lt;- function(penguins, K) {\n  x &lt;- penguins %&gt;%\n    select(matches(\"length|depth|mass\")) %&gt;%\n    scale()\n    \n  kmeans(x, center = K) %&gt;%\n    augment(penguins) %&gt;% # creates column \".cluster\" with cluster label\n    mutate(silhouette = silhouette(as.integer(.cluster), dist(x))[, \"sil_width\"])\n}\ncur_id &lt;- 2\npenguins3 &lt;- cluster_penguins(penguins, K = 3)\nobs_i &lt;- penguins3 %&gt;%\n  filter(id == cur_id)\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nThe observation on which we will compute the silhouette statistic.\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_segment(\n    data = penguins3 %&gt;% filter(.cluster == obs_i$.cluster), \n    aes(xend = obs_i$bill_length_mm, yend = obs_i$bill_depth_mm),\n    size = 0.6, alpha = 0.3\n  ) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1)) +\n  labs(title = expression(paste(\"Distances used for \", a[i])))\n\n\n\n\nThe average distance between the target observation and all others in the same cluster.\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_segment(\n    data = penguins3 %&gt;% filter(.cluster != obs_i$.cluster), \n    aes(xend = obs_i$bill_length_mm, yend = obs_i$bill_depth_mm, col = .cluster),\n    size = 0.5, alpha = 0.3\n  ) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1)) +\n  labs(title = expression(paste(\"Distances used for \", b[i][1], \" and \", b[i][2])))\n\n\n\n\nThe average distance between the target observation and all others in different clusters.\nggplot(penguins3) +\n  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster, size = silhouette)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nThe silhouette statistics on the Palmers Penguins dataset when using \\(K\\)-means with \\(K: 3\\).\nggplot(penguins3) +\n  geom_histogram(aes(x = silhouette), binwidth = 0.05) +\n  facet_grid(~ .cluster)\n\n\n\n\nThe per-cluster histograms of silhouette statistics summarize how well-defined each cluster is.\npenguins4 &lt;- cluster_penguins(penguins, K = 4)\nggplot(penguins4) +\n  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster, size = silhouette)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\n\nWe can repeat the same exercise but with \\(K: 4\\) clusters instead.\nggplot(penguins4) +\n  geom_histogram(aes(x = silhouette), binwidth = 0.05) +\n  facet_grid(~ .cluster)"
  },
  {
    "objectID": "content/9-4.html#footnotes",
    "href": "content/9-4.html#footnotes",
    "title": "Silhouette Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can change cur_id to try different observations.↩︎\nThis last case likely indicates a misclustering.↩︎"
  },
  {
    "objectID": "content/13-2.html",
    "href": "content/13-2.html",
    "title": "Visualizing Learned Features",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nlibrary(dplyr)\nlibrary(keras)\nlibrary(purrr)\nlibrary(RColorBrewer)\nf &lt;- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/o7t3nt77iv3twizyv7yuwqnca16f9nwi.rda\", f)\nimages &lt;- get(load(f))\ndim(images) # 20 sample images\n\n[1]  20 150 150   3\npar(mfrow = c(4, 5), mai = rep(0.00, 4))\nout &lt;- images %&gt;%\n  array_tree(1) %&gt;%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\n\nA sample of 20 random images from the dog vs. cat training dataset.\n# download model\nf &lt;- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel &lt;- load_model_hdf5(f)\n\nlayer_outputs &lt;- map(model$layers, ~ .$output)\nactivation_model &lt;- keras_model(inputs = model$input, outputs = layer_outputs)\nfeatures &lt;- predict(activation_model, images)\n\n1/1 - 0s - 124ms/epoch - 124ms/step\ndim(features[[1]])\n\n[1]  20 148 148  32\nplot_feature &lt;- function(feature) {\n  rotate &lt;- function(x) t(apply(x, 2, rev))\n  image(rotate(feature), axes = FALSE, asp = 1, col = brewer.pal(4, \"Blues\"))\n}\n\nix &lt;- 3\npar(mfrow = c(1, 2), mai = rep(0.00, 4))\nplot(as.raster(images[ix,,, ], max = 255))\nplot_feature(features[[1]][ix,,, 1])\n\n\n\n\nAn image and its activations for the first neuron in layer 1.\npar(mfrow = c(6, 7), mai = rep(0.00, 4))\nout &lt;- features[[2]][ix,,,] %&gt;%\n  array_branch(margin = 3) %&gt;%\n  map(~ plot_feature(.))\n\n\n\n\nActivations for a collection of neurons at layer 2 for the same image as given above.\npar(mfrow = c(6, 7), mai = rep(0.00, 4))\nout &lt;- features[[6]][ix,,,1:40] %&gt;%\n  array_branch(margin = 3) %&gt;%\n  map(~ plot_feature(.))\n\n\n\n\nActivations for a collection of neurons at layer 6."
  },
  {
    "objectID": "content/13-2.html#footnotes",
    "href": "content/13-2.html#footnotes",
    "title": "Visualizing Learned Features",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor those who are curious, the training code is here). Of course, you will not be responsible for understanding this step.↩︎"
  },
  {
    "objectID": "content/13-3.html",
    "href": "content/13-3.html",
    "title": "Collections of Features",
    "section": "",
    "text": "Reading (1, 2), Recording, Rmarkdown\nlibrary(dplyr)\nlibrary(keras)\nlibrary(magrittr)\nlibrary(pdist)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(superheat)\nlibrary(tidymodels)\nset.seed(479)\nf &lt;- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/dxibamcr0bcmnj7xazqxnod8wtew70m2.rda\", f)\nimages &lt;- get(load(f))\n\nf &lt;- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel &lt;- load_model_hdf5(f)\nl &lt;- c(model$layers[[6]]$output, model$layers[[8]]$output)\nactivation_model &lt;- keras_model(inputs = model$input, outputs = l)\nfeatures &lt;- predict(activation_model, images)\n\n32/32 - 2s - 2s/epoch - 57ms/step\nfeature_means &lt;- function(h) {\n  apply(h, c(1, 4), mean) |&gt;\n    as_tibble()\n}\n\nh &lt;- map_dfc(features, feature_means) %&gt;%\n  set_colnames(str_c(\"feature_\", 1:ncol(.))) |&gt;\n  mutate(id = row_number())\ntop_ims &lt;- h %&gt;%\n  slice_max(feature_3, n = 20) %&gt;%\n  pull(id)\n\npar(mfrow = c(5, 4), mai = rep(0.00, 4))\nout &lt;- images[top_ims,,,] %&gt;% \n  array_tree(1) %&gt;%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\n\nThe 20 images in the training dataset with the highest activations for neuron 3 in layer 6. This neuron seems to be sensitive to the presence of grass in an image (which happens to be correlated with whether a dog is present).\nsuperheat(\n  h %&gt;% select(-id),\n  pretty.order.rows = TRUE,\n  pretty.order.cols = TRUE,\n  legend = FALSE\n)\n\n\n\n\nA heatmap of feature map activations for layers 6 and 8 across the entire dataset. Each row is an image and each column is a neuron. There is limited clustering structure but there are substantial differences in how strongly different neurons activate on average.\nselect_features &lt;- function(x) {\n  dplyr::select(x, starts_with(\"feature\"))\n}\n\ncluster_result &lt;- kmeans(select_features(h), centers = 25, nstart = 20)\ncentroids &lt;- tidy(cluster_result)\nD &lt;- pdist(select_features(centroids), select_features(h)) |&gt;\n  as.matrix()\n\npar(mfrow = c(5, 4), mai = rep(0.00, 4))\nnear_centroid &lt;- order(D[3, ])[1:20]\nout &lt;- images[near_centroid,,, ] |&gt;\n  array_tree(1) %&gt;%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\n\nThe 20 images closest to the centroid of cluster 3 in the feature activation space. This cluster seems to include images with many orange pixels."
  },
  {
    "objectID": "content/13-3.html#footnotes",
    "href": "content/13-3.html#footnotes",
    "title": "Collections of Features",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSometimes, it’s just looking at the color of the floor!↩︎\nIn the deep learning community, this would be called using color augmentation to enforce invariance.↩︎"
  },
  {
    "objectID": "content/7-5.html",
    "href": "content/7-5.html",
    "title": "Geospatial Interaction",
    "section": "",
    "text": "Code, Recording\n\nlibrary(tidyverse)\nlibrary(leaflet)\n\n\nMaps can be information dense, so it’s often useful to make them interactive. These notes review some basic strategies for interactive spatial data visualization.\nleaflet is an easy-to-use R package that’s often sufficient for routine visualization. It offers several types of marks (marks, circles, polygons) and allows them to encode fields in a dataset. Note that its interface is more like base R than ggplot2 — we specify each attribute in one plot command. For example, in the code block below, addTiles fetches the background map. addCircles overlays the new vector features on top of the map. It’s worth noting that the vector features were created automatically – there was no need to create or read in any type of sf object.\n::: {.cell}\ncities &lt;- read_csv(\"https://uwmadison.box.com/shared/static/j98anvdoasfb1h651qxzrow2ua45oap1.csv\")\nleaflet(cities) %&gt;%\n  addTiles() %&gt;%\n  addCircles(\n    lng = ~Long,\n    lat = ~Lat,\n    radius = ~sqrt(Pop) * 30\n  )\n::: {.cell-output-display}\n\n\n::: :::\n\nLeaflet maps can be embedded into Shiny apps using leafletOutput and renderLeaflet. For example, the Superzip Explorer is a visualization designed for showing income and education levels across ZIP codes in the US. In the server, the map is initialized using the leaflet command (without even adding any data layers).\n::: {.cell}\n# Create the map\noutput$map &lt;- renderLeaflet({\n  leaflet() %&gt;%\n    addTiles() %&gt;%\n    setView(lng = -93.85, lat = 37.45, zoom = 4)\n})\n:::\n\nThe most interesting aspect of the explorer is that it lets us zoom into regions and study properties of ZIP codes within the current view. leaflet automatically creates an input$map_bounds input which is triggered anytime we pan or zoom the map. It returns a subset of the full dataset within the current view.\n::: {.cell}\nzipsInBounds &lt;- reactive({\n  if (is.null(input$map_bounds)) return(zipdata[FALSE,]) # return empty data\n  bounds &lt;- input$map_bounds\n  latRng &lt;- range(bounds$north, bounds$south)\n  lngRng &lt;- range(bounds$east, bounds$west)\n\n  # filters to current view\n  subset(zipdata,\n    latitude &gt;= latRng[1] & latitude &lt;= latRng[2] &\n      longitude &gt;= lngRng[1] & longitude &lt;= lngRng[2])\n})\n:::\nWhenever this reactive is run, the histogram (output$histCentile) and scatterplot (output$scatterCollegeIncome) on the side of the app are updated.\nNotice that an observer was created to monitor any interactions with the map. Within this observe block, a leafletProxy call is used. This function makes it possible to modify a leaflet map without redrawing the entire map. It helps support efficient rendering – we’re able to change the colors of the circles without redrawing the entire leaflet view.\n::: {.cell}\nleafletProxy(\"map\", data = zipdata) %&gt;%\n  clearShapes() %&gt;%\n  addCircles(~longitude, ~latitude, radius=radius, layerId=~zipcode,\n    stroke=FALSE, fillOpacity=0.4, fillColor=pal(colorData)) %&gt;%\n  addLegend(\"bottomleft\", pal=pal, values=colorData, title=colorBy,\n    layerId=\"colorLegend\")\n:::\nWe can often dynamically query spatial data. Querying a map can highlight properties of samples within a geographic region. For example, here is a map of in which each US county has been associated with a (random) pair of data features. Clicking on counties (or hovering with the shift key pressed) updates the bars on the right. Each bar shows the average from one of the data fields, across all selected counties.\n\nThis linking is accomplished using event listeners. For example, the map includes the call .on(\"mouseover\", update_selection), and update_selection changes the fill of the currently hovered county,\n::: {.cell}\nsvg.selectAll(\"path\")\n  .attr(\"fill\", (d) =&gt; selected.indexOf(d.id) == -1 ? \"#f7f7f7\" : \"#4a4a4a\");\n:::\nThe full implementation can be read here. Note that interactivity here is done just like in any other D3 visualization. We can treat the map as just another collection of SVG paths, and all our interaction events behave in the same way.\nWe can also imagine selecting geographic regions by interacting with linked views. This is used in Nadieh Bremer’s Urbanization in East Asia visualization, for example, where we can see all the urban areas within a country by hovering its associated bar.\n\nHere is a somewhat more complex version of the earlier random data example where acounties are associated with (random) time series. Redrawing the lower and upper bounds on the time series changes which counties are highlighted.\n\nThough it’s not exactly interaction, another common strategy for spatiotemporal data is animation. The major trends often become apparent by visualizing the flow of visual marks on the screen. For example, how can we visualize where football players go on a field before they score a goal? One approach is to animate the trajectories leading up to the goal. Here is one beautiful visualization (in D3!) by Karim Douieb that shows the most common paths and the speed at which the players run."
  },
  {
    "objectID": "content/1-2.html",
    "href": "content/1-2.html",
    "title": "A Vocabulary of Marks",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nThe choice of encodings can have a strong effect on (1) the types of comparisons that a visualization suggests and (2) the chance that readers leave with complete and accurate conclucions. With this in mind, it’s worthwhile to develop a rich vocabulary of potential visual encodings.\nSo, let’s look at a few different types of encodings available in ggplot2. Before we get started, let’s load up the libraries that will be used in these notes. ggplot2 is our plotting library. readr is used to read data files from a web link, and dplyr is useful for some of the data manipulations below (we dive into it deeply in Week 2).\n\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\ntheme_set(theme_bw()) # create a simpler default theme\n\n\nPoint Marks\n\nLet’s read in the gapminder dataset, which describes the changes in standard of living around the world over the last few decades. The %&gt;% “pipe” operator takes the output of the previous command as input to the next one – it is useful for chains of commands where the intermediate results are not needed. The mutate command makes sure that the country group variable is treated as a categorical, and not numeric, variable.\n\n\ngapminder &lt;- read_csv(\"https://uwmadison.box.com/shared/static/dyz0qohqvgake2ghm4ngupbltkzpqb7t.csv\", col_types = cols()) %&gt;%\n  mutate(cluster = as.factor(cluster))  # specify that cluster is nominal\ngap2000 &lt;- filter(gapminder, year == 2000) # keep only year 2000\n\n\nPoint marks can encode data fields using their \\(x\\) and \\(y\\) positions, color, size, and shape. Below, each mark is a country, and we’re using shape and the \\(y\\) position to distinguish between country clusters.\n\n\nggplot(gap2000) +\n  geom_point(aes(x = fertility, y = cluster, shape = cluster))\n\n\n\n\n\n\n\n\nSince the first two arguments in aes are always the x and y positions, we can omit it from our command. The code below produces the exact same plot (try it!).\n\nggplot(gap2000) +\n  geom_point(aes(fertility, cluster, shape = cluster))\n\n\nWe can specify different types of shapes using the shape parameter outside of the aes encoding.\n\n\nggplot(gap2000) +\n  geom_point(aes(fertility, cluster), shape = 15)\n\n\n\n\n\n\n\n\n\n\nBar Marks\n\nBar marks let us associate a continuous field with a nominal one.\n\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\")\n\n\n\n\n\n\n\n\nThe plot above is messy – it would not be appropriate for a publication or presentation. The grid lines associated with each bar are distracting. Further, the axis labels are all running over one another. For the first issue, we can customize the theme of the plot. Note that we don’t have to memorize the names of these arguments, since they should autocomplete when pressing tab (we just need to memorize the first few letters).\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\") +\n  theme(panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nFor the second issue, one approach is to turn the labels on their side, again by customizing the theme.\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90),\n    panel.grid.major.x = element_blank()\n  )\n\n\n\n\n\n\n\n\nAn approach I like better is to turn the bars on their side. This way, readers don’t have to tilt their heads to read the country names.\n\nggplot(gap2000) +\n  geom_bar(aes(pop, country), stat = \"identity\") +\n  theme(panel.grid.major.y = element_blank()) # note change from x to y\n\n\n\n\n\n\n\n\nI’m also going to remove the small tick marks associated with every name, again because it seems distracting.\n\nggplot(gap2000) +\n  geom_bar(aes(pop, country), stat = \"identity\") +\n  theme(\n    panel.grid.major.y = element_blank(),\n    axis.ticks = element_blank() # remove tick marks\n  )\n\n\n\n\n\n\n\n\n\nTo make comparisons between countries with similar populations easier, we can order them by population (alphabetical ordering is not that meaningful). To compare clusters, we can color in the bars.\n::: {.cell}\nggplot(gap2000) +\n  geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n  theme(\n    axis.ticks = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n::: {.cell-output-display}  ::: :::\nWe’ve been spending a lot of time on this plot. This is because I want to emphasize that a visualization is not just something we can get just by memorizing some magic (programming) incantation. Instead, it is something worth critically engaging with and refining, in a similar way that we would refine an essay or speech.\nPhilosophy aside, there are still a few points that need to be improved in this figure,\n\nThe axis titles are not meaningful.\nThere is a strange gap between the left hand edge of the plot and the start of the bars.\nI would also prefer if the bars were exactly touching one another, without the small vertical gap.\nThe scientific notation for population size is unnecessarily technical.\nThe color scheme is a bit boring.\n\nI’ll address each of these in a separate code block, with comments on the parts that are different. First, improving the axis titles,\n::: {.cell}\nggplot(gap2000) +\n  geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n  labs(x = \"Population\", y = \"Country\", fill = \"Country Group\") + # add better titles\n  theme(\n    axis.ticks = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n::: {.cell-output-display}  ::: :::\nNow we remove the gap. I learned this trick by googling it – there is no shame in doing this! A wise friend of mine once shared, “I am not a programming expert, just an expert at StackOverflow.”\n::: {.cell}\nggplot(gap2000) +\n  geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n  scale_x_continuous(expand = c(0, 0, 0.1, 0.1)) + # remove space to the axis\n  labs(x = \"Population\", y = \"Country\", fill = \"Country Group\") + \n  theme(\n    axis.text.y = element_text(size = 6),\n    axis.ticks = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n::: {.cell-output-display}  ::: :::\nNow, removing the gaps between bars.\n::: {.cell}\nggplot(gap2000) +\n  geom_bar(\n    aes(pop, reorder(country, pop), fill = cluster),\n    width = 1, stat = \"identity\" # increase width of bars\n  ) +\n  scale_x_continuous(expand = c(0, 0, 0.1, 0.1)) +\n  labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n  theme(\n    axis.ticks = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n::: {.cell-output-display}  ::: :::\nNow, we remove scientific notation,\n::: {.cell}\nggplot(gap2000) +\n  geom_bar(\n    aes(pop, reorder(country, pop), fill = cluster),\n    width = 1, stat = \"identity\"\n  ) +\n  scale_x_continuous(label = label_number(scale_cut = cut_short_scale()), expand = c(0, 0, 0.1, 0.1)) + # remove scientific notation. ::omma() is also useful.\n  labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n  theme(\n    axis.ticks = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n::: {.cell-output-display}  ::: :::\nFinally, we customize the colors. I often like to look up neat colors on color.adobe.com, iwanthue or colorhexa, but there are dozens of similar colorpicker sites out there.\n::: {.cell}\nggplot(gap2000) +\n  geom_bar(\n    aes(pop, reorder(country, pop), fill = cluster),\n    width = 1, stat = \"identity\"\n  ) +\n  scale_x_continuous(label = label_number(scale_cut = cut_short_scale()), expand = c(0, 0, 0.1, 0.1)) + # remove scientific notation. comma() is also useful.\n  scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\")) +\n  labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n  theme(\n    axis.ticks = element_blank(),\n    panel.grid.major.y = element_blank()\n  )\n::: {.cell-output-display}  ::: :::\nThis seems like a lot of work for just a lowly bar plot! But I think it’s amazing customizable the figure is – we can give it our own sense of style. With a bit of practice, these sorts of modifications will become second nature, and it won’t be necessary to keep track of all the intermediate code. And really, even though we spent some time on this plot, there are still many things that could be interesting to experiment with, like font styles, background appearance, maybe even splitting the countries into two panels.\nIn the plot above, each bar is anchored at 0. Instead, we could have each bar encode two continuous values, a left and right. To illustrate, let’s compare the minimum and maximimum life expectancies within each country cluster. We’ll need to create a new data.frame with just the summary information. For this, we group_by each cluster, so that a summarise call finds the minimum and maximum life expectancies restricted to each cluster. We’ll discuss the group_by + summarise pattern in detail next week.\n\n# find summary statistics\nlife_ranges &lt;- gap2000 %&gt;%\n  group_by(cluster) %&gt;%\n  summarise(\n    min_life = min(life_expect),\n    max_life = max(life_expect)\n  )\n\n# look at a few rows\nhead(life_ranges)\n\n# A tibble: 6 × 3\n  cluster min_life max_life\n  &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 0           42.1     63.6\n2 1           70.5     80.6\n3 2           43.4     53.4\n4 3           58.1     79.8\n5 4           66.7     82  \n6 5           57.0     79.7\n\nggplot(life_ranges) +\n  geom_segment(\n    aes(min_life, reorder(cluster, max_life), xend = max_life, yend = cluster, col = cluster),\n    size = 5,\n  ) +\n  scale_color_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\")) +\n  labs(x = \"Minimum and Maximum Expected Span\", col = \"Country Group\", y = \"Country Group\") +\n  xlim(0, 85) # otherwise would only range from 42 to 82\n\n\n\n\n\n\n\n\n\n\n\nLine Marks\n\nLine marks are useful for comparing changes. Our eyes naturally focus on rates of change when we see lines. Below, we’ll plot the fertility over time, colored in by country cluster. The group argument is useful for ensuring each country gets its own line; if we removed it, ggplot2 would become confused by the fact that the same x (year) values are associated with multiple y’s (fertility rates).\n\nggplot(gapminder) +\n  geom_line(\n    aes(year, fertility, col = cluster, group = country),\n      alpha = 0.7, size = 0.9\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +  # same trick of removing gap\n  scale_color_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))\n\n\n\n\n\n\n\n\n\n\n\nArea Marks\n\nArea marks have a flavor of both bar and line marks. The filled area supports absolute comparisons, while the changes in shape suggest derivatives.\n\npopulation_sums &lt;- gapminder %&gt;%\n  group_by(year, cluster) %&gt;%\n  summarise(total_pop = sum(pop))\nhead(population_sums)\n\n# A tibble: 6 × 3\n# Groups:   year [1]\n   year cluster total_pop\n  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n1  1955 0       495927174\n2  1955 1       360609771\n3  1955 2        60559800\n4  1955 3       355392405\n5  1955 4       854125031\n6  1955 5        56064015\n\nggplot(population_sums) +\n  geom_area(aes(year, total_pop, fill = cluster)) +\n  scale_y_continuous(expand = c(0, 0, 0.1, 0.1), label = label_number(scale_cut = cut_short_scale()))  + # remove scientific notation. scales::comma() is also useful.\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))\n\n\n\n\n\n\n\n\nJust like in bar marks, we don’t necessarily need to anchor the \\(y\\)-axis at 0. For example, here the bottom and top of each area mark is given by the 30% and 70% quantiles of population within each country cluster.\n\npopulation_ranges &lt;- gapminder %&gt;%\n  group_by(year, cluster) %&gt;%\n  summarise(min_pop = quantile(pop, 0.3), max_pop = quantile(pop, 0.7))\nhead(population_ranges)\n\n# A tibble: 6 × 4\n# Groups:   year [1]\n   year cluster   min_pop   max_pop\n  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1  1955 0       40880121. 83941368.\n2  1955 1        4532940  25990229.\n3  1955 2        6600426. 17377594.\n4  1955 3        2221139   8671500 \n5  1955 4        9014491  61905422 \n6  1955 5        3007625  12316126.\n\nggplot(population_ranges) +\n  geom_ribbon(\n    aes(x = year, ymin = min_pop, ymax = max_pop, fill = cluster),\n    alpha = 0.8\n  ) +\n  scale_y_continuous(expand = c(0, 0, 0.1, 0.1), label = label_number(scale_cut = cut_short_scale())) + # remove scientific notation. scales::comma() is also useful.\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))"
  },
  {
    "objectID": "content/5-4.html",
    "href": "content/5-4.html",
    "title": "Linking using Crosstalk",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(leaflet)\nlibrary(DT)\nlibrary(crosstalk)\n\n\nFor most graphical queries, the click and brush inputs implemented in Shiny will be sufficient. However, a basic limitation of shiny’s plotOutput is that it has to create views by generating static image files – it only creates the illusion of interactivity by rapidly changing the underlying files. In some cases, there will be so many points on the display each update will be slow and the fluidity of interaction will suffer.\nOne approach around this problem is to use a library that directly supports web-based plots. These plots can modify elements in place, without having to redraw and save the entire figure on each interaction. The crosstalk package gives one approach to linked views in this setting. We’ll only give an overview of it here, but the purpose of sharing it is so we have at least one example that we can refer to in case the Shiny approach becomes untenable.\nWe’ll study a problem about the dropoff in Chicago subway ridership after the start of the COVID-19 lockdowns. We have data on the weekday and weekend transit ridership at each subway station, along with the locations of the stations. We are curious about the extent of the change in ridership, along with features that might be responsible for some stations being differentially affected. The block below reads in this raw data,\n::: {.cell}\ndownload.file(\"https://github.com/emilyriederer/demo-crosstalk/blob/master/data/stations.rds?raw=true\", \"stations.rds\")\ndownload.file(\"https://github.com/emilyriederer/demo-crosstalk/blob/master/data/trips_apr.rds?raw=true\", \"trips_apr.rds\")\nstations &lt;- readRDS(\"stations.rds\")\ntrips &lt;- readRDS(\"trips_apr.rds\")\n:::\nThe crosstalk package implements a SharedData object. This is used to track selections across all the plots that refer to it. We can think of it as crosstalk’s analog of our earlier brushedPoints function. These objects are defined by calling SharedData$new() on the data.frame which will be used across views. The key argument provides a unique identifier that is used to match corresponding samples across all displays (notice that it uses ~ formula notation).\n::: {.cell}\ntrips_ct &lt;- SharedData$new(trips, key = ~station_id, group = \"stations\")\n:::\nLet’s see how this object can be used for linked brushing. We’ll first generate static ggplot2 objects giving (1) a view of weekday ridership in 2019 vs. 2020 and (2) a view of what proportion of 2019 ridership at the station took place on weekends.\n::: {.cell}\np1 &lt;- ggplot(trips_ct) +\n  geom_point(aes(year_2019_wday, year_2020_wday)) +\n  geom_abline(slope = 1, col = \"#0c0c0c\")\n\np2 &lt;- ggplot(trips_ct) +\n  geom_point(aes(prop_wend_2019, reorder(station_name, prop_wend_2019)), stat = \"identity\")\n:::\nGiven this ggplot2 base, we can build web-based plotly objects using the ggplotly command. The layout and highlight functions are specifying that we want user interactions to define brushes, not zoom events. The final bscols function allows us to place the views side by side. By brushing the two plots, we can see that those stations with the largest dropoff in riderships were those that were mostly used during the weekdays. This makes sense, considering many of the office workers in Chicago started working from home in 2020.\n::: {.cell}\np1 &lt;- ggplotly(p1, tooltip = \"station_name\") %&gt;%\n  layout(dragmode = \"select\") %&gt;%\n  highlight(on = \"plotly_selected\")\n\np2 &lt;- ggplotly(p2) %&gt;%\n  layout(dragmode = \"select\", direction = \"v\") %&gt;%\n  highlight(on = \"plotly_selected\")\n\nbscols(p1, p2)\n:::\nThis crosstalk approach works with more than plotly-derived plots. In the block below, we also generate a map (using the leaflet package) and a data table (using the DT package). The views are all synchronized because they refer to the same station_id key in SharedData objects. This visualization confirms our intuition that those stations with the largest drop-off in ridership are those that are downtown.\n\n\nstations_ct &lt;- SharedData$new(stations, key = ~station_id, group = \"stations\")\ndt &lt;- datatable(trips_ct)\nlf &lt;- leaflet(stations_ct) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n\nbscols(p1, p2, lf, dt, widths = rep(6, 4))"
  },
  {
    "objectID": "content/1-1.html",
    "href": "content/1-1.html",
    "title": "Introduction to ggplot2",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nggplot2 is an R implementation of the Grammar of Graphics. The idea is to define the basic “words” from which visualizations are built, and then let users compose them in original ways. This is in contrast to systems with prespecified chart types, where the user is forced to pick from a limited dropdown menu of plots. Just like in ordinary language, the creative combination of simple building blocks can support a very wide range of expression.\nThese are libraries we’ll use in this lecture.\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(ggrepel)\nlibrary(scales)"
  },
  {
    "objectID": "content/1-1.html#finishing-touches",
    "href": "content/1-1.html#finishing-touches",
    "title": "Introduction to ggplot2",
    "section": "Finishing touches",
    "text": "Finishing touches\nHow can we improve the readability of this plot? You might already have ideas,\n\nPrevent labels from overlapping. It’s impossible to read some of the state names.\nAdd a line showing the national rate. This serves as a point of reference, allowing us to see whether an individual state is above or below the national murder rate.\nGive meaningful axis / legend labels and a title.\nMove the legend to the top of the figure. Right now, we’re wasting a lot of visual real estate in the right hand side, just to let people know what each color means.\nUse a better color theme.\n\nFor 1., the ggrepel package find better state name positions, drawing links when necessary.\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) + # I moved it up so that the geom_point's appear on top of the lines\n  geom_point(aes(col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nFor 2., let’s first compute the national murder rate,\n\nr &lt;- murders %&gt;% \n  summarize(rate = sum(total) /  sum(population)) %&gt;%\n  pull(rate)\nr\n\n[1] 3.034555e-05\n\n\nNow, we can use this as the slope in a geom_abline layer, which encodes a slope and intercept as a line on a graph.\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nFor 3., we can add a labs layer to write labels and a theme to reposition the legend. I used unit_format from the scales package to change the scientific notation in the \\(x\\)-axis labels to something more readable.\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10(labels = unit_format(unit = \"million\", scale = 1e-6)) + # used to convert scientific notation to readable labels\n  scale_y_log10() +\n  labs(\n    x = \"Population (log scale)\",\n    y = \"Total number of murders (log scale)\",\n    color = \"region\",\n    title = \"US Gun Murders in 2010\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFor 5., I find the gray background with reference lines a bit distracting. We can simplify the appearance using theme_bw. I also like the colorbrewer palette, which can be used by calling a different color scale.\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10(labels = unit_format(unit = \"million\", scale = 1e-6)) +\n  scale_y_log10() +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"Population (log scale)\",\n    y = \"Total number of murders (log scale)\",\n    color = \"Region\",\n    title = \"US Gun Murders in 2010\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nSome bonus exercises, which will train you to look at your graphics more carefully, as well as build your familiarity with ggplot2.\n\nTry reducing the size of the text labels. Hint: use the size argument in geom_text_repel.\nIncrease the size of the circles in the legend. Hint: Use override.aes within a guide.\nRe-order the order of regions in the legend. Hint: Reset the factor levels in the region field of the murders data.frame.\nOnly show labels for a subset of states that are far from the national rate. Hint: Filter the murders data.frame, and use a data field specific to the geom_text_repel layer."
  },
  {
    "objectID": "content/7-3.html",
    "href": "content/7-3.html",
    "title": "Raster Data",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(raster)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\ntheme_set(theme_bw())\n\n\nThe raster data format is used to store spatial data that lie along regular grids. The values along the grid are stored as entries in the matrix. The raster object contains metadata that associates each entry in the matrix with a geographic coordinate.\nSince the data they must lie along a regular grid, rasters are most often used for continuously measured data, like elevation, temperature, population density, or landcover class.\nWe can create a raster using the rast command. The code block below loads an elevation map measured by the space shuttle.\n::: {.cell}\nf &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nzion &lt;- rast(f)\n:::\nTyping the name of the object shows the metadata associated with it (but not the actual grid values). We can see that the grid has 457 rows and 465 columns. We also see its spatial extent: The minimum and maximum longitude are both close to -113 and the latitudes are between 37.1 and 37.5. A quick google map search shows that this is located in Zion national park.\n::: {.cell}\nzion\n::: {.cell-output .cell-output-stdout}\nclass       : SpatRaster \nsize        : 457, 465, 1  (nrow, ncol, nlyr)\nresolution  : 0.0008333333, 0.0008333333  (x, y)\nextent      : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : srtm.tif \nname        : srtm \nmin value   : 1024 \nmax value   : 2892 \n::: :::\n::: {.cell}\nplot(zion)\n::: {.cell-output-display}  ::: :::\nIn contrast, the raster command lets us create raster objects from scratch. For example, the code below makes a raster with increasing values in a 6 x 6 grid. Notice that we had to give a fake spatial extent.\n::: {.cell}\ntest &lt;- raster(\n  nrows = 6, ncols = 6, res = 0.5, \n  xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5,\n  vals = 1:36\n)\n\nplot(test)\n::: {.cell-output-display}  ::: :::\nReal-world rasters typically have more than one layer of data. For example, you might measure both elevation and slope along the same spatial grid, which would lead to a 2 layer raster. Or, for satellite images, you might measure light at multiple wavelengths (usual RGB, plus infrared or thermal for example).\nMulti-layer raster data can be read in using rast. You can refer to particular layers in a multi-layer raster by indexing.\n::: {.cell}\nf &lt;- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nsatellite &lt;- rast(f)\n\nsatellite # all 4 channels\n::: {.cell-output .cell-output-stdout}\nclass       : SpatRaster \nsize        : 1428, 1128, 4  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       : landsat_1, landsat_2, landsat_3, landsat_4 \nmin values  :      7550,      6404,      5678,      5252 \nmax values  :     19071,     22051,     25780,     31961 \n:::\nsatellite[[1:2]] # only first two channels\n::: {.cell-output .cell-output-stdout}\nclass       : SpatRaster \nsize        : 1428, 1128, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       : landsat_1, landsat_2 \nmin values  :      7550,      6404 \nmax values  :     19071,     22051 \n::: :::\nBase R’s plot function supports plotting one layer of a raster at a time. To plot more than one layer in a multichannel image (like ordinary RGB images) you can use the plotRGB function.\n::: {.cell}\nplotRGB(satellite, stretch = \"lin\")\n::: {.cell-output-display}  ::: :::\nSometimes, it’s useful to overlay several visual marks on top of a raster image.\n::: {.cell}\nsatellite &lt;- project(satellite, \"EPSG:4326\")\npoint &lt;- data.frame(geom = st_sfc(st_point(c(-113, 37.3)))) %&gt;%\n  st_as_sf()\n\ntm_shape(satellite) +\n  tm_raster() +\n  tm_shape(point) +\n  tm_dots(col = \"blue\", size = 5)\n::: {.cell-output-display}  ::: :::\nIf we want to visualize just a single layer, we can use tm_rgb with all color channels set to the layer of interest. Note that, here, I’ve rescaled the maximum value of each pixel to 255, since this is the default maximum value for a color image.\n\ntm_shape(satellite / max(satellite) * 255) +\n  tm_rgb(r = 1, g = 1, b = 1)"
  },
  {
    "objectID": "content/3-4.html",
    "href": "content/3-4.html",
    "title": "Patchwork",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nIn the last set of notes we discussed principles for designing effective compound figures. In these notes, we’ll review the patchwork R package, which can be used to implement compound figures.\nThis package creates a simple syntax for combining figures,\n\np1 + p2 concatenates two figures horizontally\np1 / p2 concatenates two figures vertically\n\nThis idea is simple, but becomes very powerful once we realize that we can define a whole algebra on plot layouts,\n\np1 + p2 + p3 concatenates three figures horizontally\np1 / p2 / p3 concatenates three figures vertically\n(p1 + p2) / p3 Concatenates the first two figures horizontally, and places the third below both.\n…\n\nBefore we illustrate the use of this package, let’s read in the athletes data from the previous notes. The code below constructs the three component plots that we want to combine. Though it looks like a lot of code, it’s just because we are making several plots and styling each one of them. Conceptually, this is the same type of ggplot2 code that we have been using all semester – the only difference is that we save all the figure objects into one list, instead of printing them right away.\n::: {.cell}\nlibrary(tidyverse)\nlibrary(patchwork)\n\nathletes &lt;- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat436_s23/main/data/athletes.csv\") %&gt;%\n  filter(sport %in% c(\"basketball\", \"field\", \"rowing\", \"swimming\", \"tennis\", \"track (400m)\")) %&gt;%\n  mutate(sex = recode(sex, \"m\" = \"male\", \"f\" = \"female\"))\n\np &lt;- list()\np[[\"bar\"]] &lt;- ggplot(count(athletes, sex)) +\n  geom_bar(aes(sex, n, fill = sex), stat = \"identity\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(y = \"number\")\n\np[[\"scatter\"]] &lt;- ggplot(athletes) +\n  geom_point(aes(rcc, wcc, col = sex)) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"RBC count\", y = \"WBC Count\")\n\np[[\"box\"]] &lt;- ggplot(athletes) +\n  geom_boxplot(aes(sport, pcBfat, col = sex, fill = sex), alpha = 0.5) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  labs(y = \"% body fat\", x = NULL)\n:::\nNow, we use patchwork to combine the subplots using the different combinations discussed above.\n::: {.cell}\np[[\"bar\"]] + p[[\"scatter\"]] + p[[\"box\"]]\n::: {.cell-output-display}  :::\np[[\"bar\"]] / p[[\"scatter\"]] / p[[\"box\"]]\n::: {.cell-output-display}  :::\n(p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]]\n::: {.cell-output-display}  ::: :::\nA corollary of using the same encodings across panels is that it should be possible to share legends across the entire compound figure. This is most concisely done by setting plot_layout(legend = \"collect\"). For example, compare the athlete physiology dataset with and without the collected legends,\n::: {.cell}\n(p[[\"bar\"]] + p[[\"scatter\"]] + theme(legend.position = \"left\")) / p[[\"box\"]] # turns legends back on\n::: {.cell-output-display}  ::: :::\nThe version with the legends collected is given below.\n::: {.cell}\n(p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n      plot_layout(guides = \"collect\") &\n      plot_annotation(theme = theme(legend.position = \"bottom\"))\n::: {.cell-output-display}  ::: :::\nFor annotation, we can add a title to each figure individually using ggtitle(), before they are combined into the compound figure. The size and font of the titles can be adjusted by using the theme(title = element_text(...)) option. For example, the code below adds the a - c titles for each subpanel.\n::: {.cell}\np[[\"bar\"]] &lt;- p[[\"bar\"]] + ggtitle(\"a\")\np[[\"scatter\"]] &lt;- p[[\"scatter\"]] + ggtitle(\"b\")\np[[\"box\"]] &lt;- p[[\"box\"]] + ggtitle(\"c\")\n\n(p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n  plot_layout(guides = \"collect\") &\n  plot_annotation(theme = theme(legend.position = \"bottom\", title = element_text(size = 10)))\n::: {.cell-output-display}  ::: :::\nPatchwork handles alignment in the background, but sometimes we might want to have control over the relative sizes of different panels. For this, we can again use the plot_layout function, this time using the height and width arguments. For example, the two examples change the height and widths of the first component in the layout.\n\n\n    (p[[\"bar\"]] + p[[\"scatter\"]] + plot_layout(widths = c(1, 3))) / p[[\"box\"]] +\n      plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n    (p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n      plot_layout(guides = \"collect\", heights = c(1, 3))"
  },
  {
    "objectID": "content/7-1.html",
    "href": "content/7-1.html",
    "title": "Spatial Data Formats",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\nknitr::opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE, eval = TRUE)\nlibrary(ceramic)\nlibrary(raster)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "content/7-1.html#footnotes",
    "href": "content/7-1.html#footnotes",
    "title": "Spatial Data Formats",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt can be constructed easily using the wizard↩︎"
  },
  {
    "objectID": "content/2-2.html",
    "href": "content/2-2.html",
    "title": "Pivoting",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\nPivoting refers to the process of changing the interpretation of each row in a data frame. It is useful for addressing problems 1 - 2 in the previous lecture, which we repeat here for completeness.\n\nA variable might be implicitly stored within column names, rather than explicitly stored in its own column.\nThe same observation may appear in multiple rows, where each instance of the row is associated with a different variable.\n\nTo address (a), we can use the pivot_longer function in tidyr. It takes an implicitly stored variable and explicitly stores it in a column defined by the names_to argument. In\nThe example below shows pivot_longer being used to tidy one of the non-tidy tuberculosis datasets. Note that the data has doubled in length, because there are now two rows per country (one per year).\n\nFor reference, these are the original data.\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\nThis step lengthens the data,\n\ntable4a_longer &lt;- table4a %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntable4a_longer\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\ndim(table4a)\n\n[1] 3 3\n\ndim(table4a_longer)\n\n[1] 6 3\n\n\n\nWe can pivot both the population and the cases table, then combine them using a join operation. A join operation matches rows across two tables according to their shared columns.\n\n\n# helper function, to avoid copying and pasting code\npivot_fun &lt;- function(x, value_column = \"cases\") {\n  x %&gt;%\n    pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = value_column)\n}\n\ntable4 &lt;- left_join(\n  pivot_fun(table4a), # look for all country x year combinations in left table\n  pivot_fun(table4b, \"population\") # and find matching rows in right table\n)\ntable4\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\nThis lets us make the year vs. rate plot that we had tried to put together in the last lecture. It’s much easier to recognize trends when comparing the rates, than when looking at the raw case counts.\n\nggplot(table4, aes(x = year, y = cases / population, col = country)) +\n  geom_point() +\n  geom_line(aes(group = country))\n\n\n\n\n\n\n\n\n\nTo address (b), we can use the pivot_wider function. It spreads the column in the values_from argument across new columns specified by the names_from argument.\nThe example below shows pivot_wider being used to tidy one of the other non-tidy datasets. Note when there are more than two levels in the names_from column, this will always be wider than the starting data frame, which is why this operation is called pivot_wider.\n\nFor reference, here is table2 before pivoting.\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\nNow, we spread the cases and population variables into their own columns.\n\ntable2 %&gt;%\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "content/6-5.html",
    "href": "content/6-5.html",
    "title": "Collections of Time Series",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(feasts)\nlibrary(fpp2)\nlibrary(ggrepel)\nlibrary(tsibble)\nlibrary(tsibbledata)\ntheme_set(theme_minimal())\n\n\nWe have seen ways of visualizing a single time series (seasonal plots, ACF) and small numbers of time series (Cross Correlation). In practice, it’s also common to encounter large collections of time series. These datasets tend to require more sophisticated analysis techniques, but we will review one useful approach, based on extracted features.\nThe high-level idea is to represent each time series by a vector of summary statistics, like the maximum value, the slope, and so on. These vector summaries can then be used to create an overview of variation seen across all time series. For example, just looking at the first few regions in the Australian tourism dataset, we can see that there might be useful features related to the overall level (Coral Coast is larger than Barkly), recent trends (increased business in North West), and seasonality (South West is especially seasonal).\n\n\ntourism &lt;- as_tsibble(tourism, index = Quarter) %&gt;%\n  mutate(key = str_c(Region, Purpose, sep=\"-\")) %&gt;%\n  update_tsibble(key = c(\"Region\", \"State\", \"Purpose\", \"key\"))\n\nregions &lt;- tourism %&gt;%\n  distinct(Region) %&gt;%\n  pull(Region)\n\nggplot(tourism %&gt;% filter(Region %in% regions[1:9])) +\n  geom_line(aes(x = date(Quarter), y = Trips, col = Purpose)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~Region, scale = \"free\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nComputing these kinds of summary statistics by hand would be tedious. Fortunately, the feasts package makes it easy to extract a variety of statistics for tsibble objects.\n\n\ntourism_features &lt;- tourism %&gt;%\n  features(Trips, feature_set(pkgs = \"feasts\"))\n\ntourism_features\n\n# A tibble: 304 × 52\n   Region         State      Purpose key   trend_strength seasonal_strength_year\n   &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Adelaide       South Aus… Busine… Adel…          0.464                  0.407\n 2 Adelaide       South Aus… Holiday Adel…          0.554                  0.619\n 3 Adelaide       South Aus… Other   Adel…          0.746                  0.202\n 4 Adelaide       South Aus… Visiti… Adel…          0.435                  0.452\n 5 Adelaide Hills South Aus… Busine… Adel…          0.464                  0.179\n 6 Adelaide Hills South Aus… Holiday Adel…          0.528                  0.296\n 7 Adelaide Hills South Aus… Other   Adel…          0.593                  0.404\n 8 Adelaide Hills South Aus… Visiti… Adel…          0.488                  0.254\n 9 Alice Springs  Northern … Busine… Alic…          0.534                  0.251\n10 Alice Springs  Northern … Holiday Alic…          0.381                  0.832\n# ℹ 294 more rows\n# ℹ 46 more variables: seasonal_peak_year &lt;dbl&gt;, seasonal_trough_year &lt;dbl&gt;,\n#   spikiness &lt;dbl&gt;, linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;,\n#   stl_e_acf10 &lt;dbl&gt;, acf1 &lt;dbl&gt;, acf10 &lt;dbl&gt;, diff1_acf1 &lt;dbl&gt;,\n#   diff1_acf10 &lt;dbl&gt;, diff2_acf1 &lt;dbl&gt;, diff2_acf10 &lt;dbl&gt;, season_acf1 &lt;dbl&gt;,\n#   pacf5 &lt;dbl&gt;, diff1_pacf5 &lt;dbl&gt;, diff2_pacf5 &lt;dbl&gt;, season_pacf &lt;dbl&gt;,\n#   zero_run_mean &lt;dbl&gt;, nonzero_squared_cv &lt;dbl&gt;, zero_start_prop &lt;dbl&gt;, …\n\n\n\nOnce you have a data.frame summarizing these time series, you can run any clustering or dimensionality reduction procedure on the summary. For example, this is 2D representation from PCA. We will get into much more depth about dimensionality reduction later in this course — for now, just think of this as an abstract map relating all the time series.\n\n\npcs &lt;- tourism_features %&gt;%\n  select(-State, -Region, -Purpose, -key) %&gt;%\n  prcomp(scale = TRUE) %&gt;%\n  augment(tourism_features)\n\noutliers &lt;- pcs %&gt;% \n  filter(.fittedPC1 ^ 2 + .fittedPC2 ^ 2 &gt; 120)\n\nThis PCA makes it very clear that the different travel purposes have different time series, likely due to the heavy seasonality of holiday travel (Melbourne seems to be an interesting exception).\n\nggplot(pcs, aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point(aes(col = Purpose)) +\n  geom_text_repel(\n    data = outliers,\n    aes(label = Region),\n    size = 2.5 \n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"PC1\", y = \"PC2\") +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nWe can look at the series that are outlying in the PCA. The reading has some stories for why these should be considered outliers. They seem to be series with substantial increasing trends or which have exceptionally high overall counts.\n\n\noutlier_series &lt;- tourism %&gt;%\n  filter(key %in% outliers$key)\n\nggplot(outlier_series) +\n  geom_line(aes(x = date(Quarter), y = Trips, col = Purpose)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~Region, scale = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nThis featurization approach is especially powerful when combined with coordinated views. It is possible to link the points in the PCA plot with the time series display, so that selecting points in the PCA shows the corresponding time series."
  },
  {
    "objectID": "content/6-4.html",
    "href": "content/6-4.html",
    "title": "Cross and Auto-Correlation",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(tsibbledata)\nlibrary(feasts)\ntheme_set(theme_minimal())\n\n\nThere is often interest in seeing how two time series relate to one another. A scatterplot can be useful in gauging this relationship. For example, the plot below suggests that there is a relationship between electricity demand and temperature, but it’s hard to make out exactly the nature of the relationship.\n\n\nvic_2014 &lt;- vic_elec %&gt;%\n  filter(year(Time) == 2014)\n\nvic_2014 %&gt;%\n  pivot_longer(Demand:Temperature) %&gt;%\n  ggplot(aes(x = Time, y = value)) +\n  geom_line() +\n  facet_wrap(~ name, scales = \"free_y\")\n\n\n\n\n\n\n\n\nA scatterplot clarifies that, while electricity demand generally goes up in the cooler months, the very highest demand happens during high heat days.\n\nggplot(vic_2014, aes(x = Temperature, y = Demand)) +\n  geom_point(alpha = 0.6, size = 0.7)\n\n\n\n\n\n\n\n\n\nNote that the timepoints are far from independent. Points tend to drift gradually across the scatterplot, rather than jumping to completely different regions in short time intervals. This is just the 2D consequence of the time series varying smoothly.\n\n\nlagged &lt;- vic_2014[c(2:nrow(vic_2014), 2), ] %&gt;%\n  setNames(str_c(\"lagged_\", colnames(vic_2014)))\n\nggplot(bind_cols(vic_2014, lagged), aes(x = Temperature, y = Demand)) +\n  geom_point(alpha = 0.6, size = 0.7) +\n  geom_segment(\n    aes(xend = lagged_Temperature, yend = lagged_Demand),\n    size = .4, alpha = 0.5\n  )\n\n\n\n\n\n\n\n\n\nTo formally measure the linear relationship between two time series, we can use the cross-correlation, \\[\\begin{align}\n\\frac{\\sum_{t}\\left(x_{t} - \\hat{\\mu}_{X}\\right)\\left(y_{t} - \\hat{\\mu}_{Y}\\right)}{\\hat{\\sigma}_{X}\\hat{\\sigma}_{Y}}\n\\end{align}\\] which for the data above is,\n\n\ncor(vic_2014$Temperature, vic_2014$Demand)\n\n[1] 0.2797854\n\n\n\nCross-correlation can be extended to autocorrelation — the correlation between a time series and a lagged version of itself. This measure is useful for quantifying the strength of seasonality within a time series. A daily time series with strong weekly seasonality will have high autocorrelation at lag 7, for example. The example below shows the lag-plots for Australian beer production after 2000. The plot makes clear that there is high autocorrelation at lags 4 and 8, suggesting high quarterly seasonality.\n\n\nrecent_production &lt;- aus_production %&gt;%\n  filter(year(Quarter) &gt; 2000)\n\ngg_lag(recent_production, Beer, geom = \"point\")\n\n\n\n\n\n\n\n\nIndeed, we can confirm this by looking at the original data.\n\nggplot(recent_production) +\n  geom_line(aes(x = time(Quarter), y = Beer))\n\n\n\n\n\n\n\n\n\nThese lag plots take up a bit of space. A more compact summary is to compute the autocorrelation function (ACF). Peaks and valleys in an ACF suggest seasonality at the frequency indicated by the lag value.\n\n\nacf_data &lt;- ACF(recent_production, Beer)\nautoplot(acf_data)\n\n\n\n\n\n\n\n\n\nGradually decreasing slopes in the ACF suggest trends. This is because if there is a trend, the current value tends to be very correlated with the recent past. It’s possible to have both seasonality within a trend, in which case the ACF function has bumps where the seasonal peaks align.\n\n\na10 &lt;- PBS |&gt;\n  filter(ATC2 == \"A10\") |&gt;\n  select(Month, Concession, Type, Cost) |&gt;\n  summarise(total_cost = sum(Cost)) |&gt;\n  mutate(cost = total_cost / 1e6) |&gt;\n  as_tsibble()\n\nggplot(a10, aes(x = time(Month), y = cost)) +\n  geom_line()\n\n\n\n\n\n\n\nacf_data &lt;- ACF(a10, lag_max = 100)\nautoplot(acf_data)"
  },
  {
    "objectID": "content/2-4.html",
    "href": "content/2-4.html",
    "title": "Tidy Data Example",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(skimr)\n\n\nLet’s work through the example of tidying a WHO dataset. This was discussed in the reading and is good practice in pivoting and deriving new variables.\nThe raw data, along with a summary from the skimr package, is shown below (notice the small multiples!).\n\n\nwho\n\n# A tibble: 7,240 × 60\n   country  iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghani… AF    AFG    1980          NA           NA           NA           NA\n 2 Afghani… AF    AFG    1981          NA           NA           NA           NA\n 3 Afghani… AF    AFG    1982          NA           NA           NA           NA\n 4 Afghani… AF    AFG    1983          NA           NA           NA           NA\n 5 Afghani… AF    AFG    1984          NA           NA           NA           NA\n 6 Afghani… AF    AFG    1985          NA           NA           NA           NA\n 7 Afghani… AF    AFG    1986          NA           NA           NA           NA\n 8 Afghani… AF    AFG    1987          NA           NA           NA           NA\n 9 Afghani… AF    AFG    1988          NA           NA           NA           NA\n10 Afghani… AF    AFG    1989          NA           NA           NA           NA\n# ℹ 7,230 more rows\n# ℹ 52 more variables: new_sp_m4554 &lt;dbl&gt;, new_sp_m5564 &lt;dbl&gt;,\n#   new_sp_m65 &lt;dbl&gt;, new_sp_f014 &lt;dbl&gt;, new_sp_f1524 &lt;dbl&gt;,\n#   new_sp_f2534 &lt;dbl&gt;, new_sp_f3544 &lt;dbl&gt;, new_sp_f4554 &lt;dbl&gt;,\n#   new_sp_f5564 &lt;dbl&gt;, new_sp_f65 &lt;dbl&gt;, new_sn_m014 &lt;dbl&gt;,\n#   new_sn_m1524 &lt;dbl&gt;, new_sn_m2534 &lt;dbl&gt;, new_sn_m3544 &lt;dbl&gt;,\n#   new_sn_m4554 &lt;dbl&gt;, new_sn_m5564 &lt;dbl&gt;, new_sn_m65 &lt;dbl&gt;, …\n\n\n\nskim(who)\n\n\nData summary\n\n\nName\nwho\n\n\nNumber of rows\n7240\n\n\nNumber of columns\n60\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n57\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncountry\n0\n1\n4\n52\n0\n219\n0\n\n\niso2\n34\n1\n2\n2\n0\n218\n0\n\n\niso3\n0\n1\n3\n3\n0\n219\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n1996.56\n9.83\n1980\n1988.00\n1997.0\n2005.00\n2013\n▇▇▇▇▇\n\n\nnew_sp_m014\n4067\n0.44\n83.71\n316.14\n0\n0.00\n5.0\n37.00\n5001\n▇▁▁▁▁\n\n\nnew_sp_m1524\n4031\n0.44\n1015.66\n4885.38\n0\n9.00\n90.0\n502.00\n78278\n▇▁▁▁▁\n\n\nnew_sp_m2534\n4034\n0.44\n1403.80\n5718.39\n0\n14.00\n150.0\n715.50\n84003\n▇▁▁▁▁\n\n\nnew_sp_m3544\n4021\n0.44\n1315.88\n6003.26\n0\n13.00\n130.0\n583.50\n90830\n▇▁▁▁▁\n\n\nnew_sp_m4554\n4017\n0.45\n1103.86\n5441.06\n0\n12.00\n102.0\n440.00\n82921\n▇▁▁▁▁\n\n\nnew_sp_m5564\n4022\n0.44\n800.70\n4418.31\n0\n8.00\n63.0\n279.00\n63814\n▇▁▁▁▁\n\n\nnew_sp_m65\n4031\n0.44\n682.82\n4089.14\n0\n8.00\n53.0\n232.00\n70376\n▇▁▁▁▁\n\n\nnew_sp_f014\n4066\n0.44\n114.33\n504.63\n0\n1.00\n7.0\n50.75\n8576\n▇▁▁▁▁\n\n\nnew_sp_f1524\n4046\n0.44\n826.11\n3552.02\n0\n7.00\n66.0\n421.00\n53975\n▇▁▁▁▁\n\n\nnew_sp_f2534\n4040\n0.44\n917.30\n3580.15\n0\n9.00\n84.0\n476.25\n49887\n▇▁▁▁▁\n\n\nnew_sp_f3544\n4041\n0.44\n640.43\n2542.51\n0\n6.00\n57.0\n308.00\n34698\n▇▁▁▁▁\n\n\nnew_sp_f4554\n4036\n0.44\n445.78\n1799.23\n0\n4.00\n38.0\n211.00\n23977\n▇▁▁▁▁\n\n\nnew_sp_f5564\n4045\n0.44\n313.87\n1381.25\n0\n3.00\n25.0\n146.50\n18203\n▇▁▁▁▁\n\n\nnew_sp_f65\n4043\n0.44\n283.93\n1267.94\n0\n4.00\n30.0\n129.00\n21339\n▇▁▁▁▁\n\n\nnew_sn_m014\n6195\n0.14\n308.75\n1727.25\n0\n1.00\n9.0\n61.00\n22355\n▇▁▁▁▁\n\n\nnew_sn_m1524\n6210\n0.14\n513.02\n3643.27\n0\n2.00\n15.5\n102.00\n60246\n▇▁▁▁▁\n\n\nnew_sn_m2534\n6218\n0.14\n653.69\n3430.03\n0\n2.00\n23.0\n135.50\n50282\n▇▁▁▁▁\n\n\nnew_sn_m3544\n6215\n0.14\n837.87\n8524.53\n0\n2.00\n19.0\n132.00\n250051\n▇▁▁▁▁\n\n\nnew_sn_m4554\n6213\n0.14\n520.79\n3301.70\n0\n2.00\n19.0\n127.50\n57181\n▇▁▁▁▁\n\n\nnew_sn_m5564\n6219\n0.14\n448.62\n3488.68\n0\n2.00\n16.0\n101.00\n64972\n▇▁▁▁▁\n\n\nnew_sn_m65\n6220\n0.14\n460.36\n3991.90\n0\n2.00\n20.5\n111.75\n74282\n▇▁▁▁▁\n\n\nnew_sn_f014\n6200\n0.14\n291.95\n1647.30\n0\n1.00\n8.0\n58.00\n21406\n▇▁▁▁▁\n\n\nnew_sn_f1524\n6218\n0.14\n407.90\n2379.13\n0\n1.00\n12.0\n89.00\n35518\n▇▁▁▁▁\n\n\nnew_sn_f2534\n6224\n0.14\n466.26\n2272.86\n0\n2.00\n18.0\n103.25\n28753\n▇▁▁▁▁\n\n\nnew_sn_f3544\n6220\n0.14\n506.59\n5013.53\n0\n1.00\n11.0\n82.25\n148811\n▇▁▁▁▁\n\n\nnew_sn_f4554\n6222\n0.14\n271.16\n1511.72\n0\n1.00\n10.0\n76.75\n23869\n▇▁▁▁▁\n\n\nnew_sn_f5564\n6223\n0.14\n213.39\n1468.62\n0\n1.00\n8.0\n56.00\n26085\n▇▁▁▁▁\n\n\nnew_sn_f65\n6221\n0.14\n230.75\n1597.70\n0\n1.00\n13.0\n74.00\n29630\n▇▁▁▁▁\n\n\nnew_ep_m014\n6202\n0.14\n128.61\n460.14\n0\n0.00\n6.0\n55.00\n7869\n▇▁▁▁▁\n\n\nnew_ep_m1524\n6214\n0.14\n158.30\n537.74\n0\n1.00\n11.0\n88.00\n8558\n▇▁▁▁▁\n\n\nnew_ep_m2534\n6220\n0.14\n201.23\n764.05\n0\n1.00\n13.0\n124.00\n11843\n▇▁▁▁▁\n\n\nnew_ep_m3544\n6216\n0.14\n272.72\n3381.41\n0\n1.00\n10.5\n91.25\n105825\n▇▁▁▁▁\n\n\nnew_ep_m4554\n6220\n0.14\n108.11\n380.61\n0\n1.00\n8.5\n63.25\n5875\n▇▁▁▁▁\n\n\nnew_ep_m5564\n6225\n0.14\n72.17\n234.55\n0\n1.00\n7.0\n46.00\n3957\n▇▁▁▁▁\n\n\nnew_ep_m65\n6222\n0.14\n78.94\n227.34\n0\n1.00\n10.0\n55.00\n3061\n▇▁▁▁▁\n\n\nnew_ep_f014\n6208\n0.14\n112.89\n446.55\n0\n0.00\n5.0\n50.00\n6960\n▇▁▁▁▁\n\n\nnew_ep_f1524\n6219\n0.14\n149.17\n543.89\n0\n1.00\n9.0\n78.00\n7866\n▇▁▁▁▁\n\n\nnew_ep_f2534\n6219\n0.14\n189.52\n761.79\n0\n1.00\n12.0\n95.00\n10759\n▇▁▁▁▁\n\n\nnew_ep_f3544\n6219\n0.14\n241.70\n3218.50\n0\n1.00\n9.0\n77.00\n101015\n▇▁▁▁▁\n\n\nnew_ep_f4554\n6223\n0.14\n93.77\n339.33\n0\n1.00\n8.0\n56.00\n6759\n▇▁▁▁▁\n\n\nnew_ep_f5564\n6223\n0.14\n63.04\n212.95\n0\n1.00\n6.0\n42.00\n4684\n▇▁▁▁▁\n\n\nnew_ep_f65\n6226\n0.14\n72.31\n202.72\n0\n0.00\n10.0\n51.00\n2548\n▇▁▁▁▁\n\n\nnewrel_m014\n7050\n0.03\n538.18\n2082.18\n0\n5.00\n32.5\n210.00\n18617\n▇▁▁▁▁\n\n\nnewrel_m1524\n7058\n0.03\n1489.51\n6848.18\n0\n17.50\n171.0\n684.25\n84785\n▇▁▁▁▁\n\n\nnewrel_m2534\n7057\n0.03\n2139.72\n7539.87\n0\n25.00\n217.0\n1091.00\n76917\n▇▁▁▁▁\n\n\nnewrel_m3544\n7056\n0.03\n2036.40\n7847.94\n0\n24.75\n208.0\n851.25\n84565\n▇▁▁▁▁\n\n\nnewrel_m4554\n7056\n0.03\n1835.07\n8324.28\n0\n19.00\n175.0\n688.50\n100297\n▇▁▁▁▁\n\n\nnewrel_m5564\n7055\n0.03\n1525.30\n8760.27\n0\n13.00\n136.0\n536.00\n112558\n▇▁▁▁▁\n\n\nnewrel_m65\n7058\n0.03\n1426.00\n9431.99\n0\n17.00\n117.0\n453.50\n124476\n▇▁▁▁▁\n\n\nnewrel_f014\n7050\n0.03\n532.84\n2117.78\n0\n5.00\n32.5\n226.00\n18054\n▇▁▁▁▁\n\n\nnewrel_f1524\n7056\n0.03\n1161.85\n4606.76\n0\n10.75\n123.0\n587.75\n49491\n▇▁▁▁▁\n\n\nnewrel_f2534\n7058\n0.03\n1472.80\n5259.59\n0\n18.00\n161.0\n762.50\n44985\n▇▁▁▁▁\n\n\nnewrel_f3544\n7057\n0.03\n1125.01\n4210.58\n0\n12.50\n125.0\n544.50\n38804\n▇▁▁▁▁\n\n\nnewrel_f4554\n7057\n0.03\n877.27\n3556.18\n0\n10.00\n92.0\n400.50\n37138\n▇▁▁▁▁\n\n\nnewrel_f5564\n7057\n0.03\n686.41\n3379.33\n0\n8.00\n69.0\n269.00\n40892\n▇▁▁▁▁\n\n\nnewrel_f65\n7055\n0.03\n683.76\n3618.47\n0\n9.00\n69.0\n339.00\n47438\n▇▁▁▁▁\n\n\n\n\n\n\nAccording to the data dictionary, the columns have the following meanings,\n\n\nThe first three letters -&gt; are we counting new or old cases of TB?\nNext two letters -&gt; Type of tab.\nSixth letter -&gt; Sex of patients\nRemaining numbers -&gt; Age group. E.g., 3544 should be interpreted as 35 - 44 years old.\n\n\nOur first step is to pivot_longer. There is quite a bit of information implicitly stored in the column names, and we want to make those variables explicitly available for visual encoding.\n\n\nwho_longer &lt;- who %&gt;% \n  pivot_longer(\n    cols = new_sp_m014:newrel_f65,  # notice we can refer to groups of columns without naming each one\n    names_to = \"key\", \n    values_to = \"cases\", \n    values_drop_na = TRUE # if a cell is empty, we do not keep it in the tidy version\n  )\n\nwho_longer\n\n# A tibble: 76,046 × 6\n   country     iso2  iso3   year key          cases\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1 Afghanistan AF    AFG    1997 new_sp_m014      0\n 2 Afghanistan AF    AFG    1997 new_sp_m1524    10\n 3 Afghanistan AF    AFG    1997 new_sp_m2534     6\n 4 Afghanistan AF    AFG    1997 new_sp_m3544     3\n 5 Afghanistan AF    AFG    1997 new_sp_m4554     5\n 6 Afghanistan AF    AFG    1997 new_sp_m5564     2\n 7 Afghanistan AF    AFG    1997 new_sp_m65       0\n 8 Afghanistan AF    AFG    1997 new_sp_f014      5\n 9 Afghanistan AF    AFG    1997 new_sp_f1524    38\n10 Afghanistan AF    AFG    1997 new_sp_f2534    36\n# ℹ 76,036 more rows\n\n\n\nThe new column key contains several variables at once. We can separate it into gender and age group.\n\n\nwho_separate &lt;- who_longer %&gt;% \n  mutate(key = str_replace(key, \"newrel\", \"new_rel\")) %&gt;%\n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %&gt;%\n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\nwho_separate\n\n# A tibble: 76,046 × 9\n   country     iso2  iso3   year new   type  sex   age   cases\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan AF    AFG    1997 new   sp    m     014       0\n 2 Afghanistan AF    AFG    1997 new   sp    m     1524     10\n 3 Afghanistan AF    AFG    1997 new   sp    m     2534      6\n 4 Afghanistan AF    AFG    1997 new   sp    m     3544      3\n 5 Afghanistan AF    AFG    1997 new   sp    m     4554      5\n 6 Afghanistan AF    AFG    1997 new   sp    m     5564      2\n 7 Afghanistan AF    AFG    1997 new   sp    m     65        0\n 8 Afghanistan AF    AFG    1997 new   sp    f     014       5\n 9 Afghanistan AF    AFG    1997 new   sp    f     1524     38\n10 Afghanistan AF    AFG    1997 new   sp    f     2534     36\n# ℹ 76,036 more rows\n\n\n\nWhile we have performed each step one at a time, it’s possible to chain them into a single block of code. This is good practice, because it avoids having to define intermediate variables that are only ever used once. This is also typically more concise.\n\n\nwho %&gt;%\n  pivot_longer(\n    cols = new_sp_m014:newrel_f65, \n    names_to = \"key\", \n    values_to = \"cases\", \n    values_drop_na = TRUE # if a cell is empty, we do not keep it in the tidy version\n  ) %&gt;%\n  mutate(key = str_replace(key, \"newrel\", \"new_rel\")) %&gt;%\n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %&gt;%\n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\n# A tibble: 76,046 × 9\n   country     iso2  iso3   year new   type  sex   age   cases\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan AF    AFG    1997 new   sp    m     014       0\n 2 Afghanistan AF    AFG    1997 new   sp    m     1524     10\n 3 Afghanistan AF    AFG    1997 new   sp    m     2534      6\n 4 Afghanistan AF    AFG    1997 new   sp    m     3544      3\n 5 Afghanistan AF    AFG    1997 new   sp    m     4554      5\n 6 Afghanistan AF    AFG    1997 new   sp    m     5564      2\n 7 Afghanistan AF    AFG    1997 new   sp    m     65        0\n 8 Afghanistan AF    AFG    1997 new   sp    f     014       5\n 9 Afghanistan AF    AFG    1997 new   sp    f     1524     38\n10 Afghanistan AF    AFG    1997 new   sp    f     2534     36\n# ℹ 76,036 more rows\n\n\n\nA recommendation for visualization in javascript. We have only discussed tidying in R. While there is work to implement tidy-style transformations in javascript, the R tidyverse provides a more mature suite of tools. If you are making an interactive visualization in javascript, I recommend first tidying data in R so that each row corresponds to a visual mark and each column to a visual property. You can always save the result as either a json or csv, which can serve as the source data for your javascript visualization."
  },
  {
    "objectID": "content/6-1.html",
    "href": "content/6-1.html",
    "title": "tsibble Objects",
    "section": "",
    "text": "Reading, Recording, Rmarkdown\n\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(tsibbledata)\n\n\nTsibbles are data structures that are designed specifically for storing time series data. They are useful because they create a unified interface to various time series visualization and modeling tasks. This removes the friction of having to transform back and forth between data.frames, lists, and matrices, depending on the particular task of interest.\nThe key difference between a tsibble and an ordinary data.frame is that it requires a temporal key variable, specifying the frequency with which observations are collected. For example, the code below generates a tsibble with yearly observations.\n\n\ntsibble(\n  Year = 2015:2019,\n  Observation = c(123, 39, 78, 52, 110),\n  index = Year\n)\n\n# A tsibble: 5 x 2 [1Y]\n   Year Observation\n  &lt;int&gt;       &lt;dbl&gt;\n1  2015         123\n2  2016          39\n3  2017          78\n4  2018          52\n5  2019         110\n\n\n\nWe can also create a tsibble from an ordinary data.frame by calling the as_tsibble function. The only subtlety is that we have to specify an index.\n\n\nx &lt;- data.frame(\n  Year = 2015:2019,\n  Observation = c(123, 39, 78, 52, 110)\n)\n\nas_tsibble(x, index = Year)\n\n# A tsibble: 5 x 2 [1Y]\n   Year Observation\n  &lt;int&gt;       &lt;dbl&gt;\n1  2015         123\n2  2016          39\n3  2017          78\n4  2018          52\n5  2019         110\n\n\n\nThe index is useful because it creates a data consistency check. If a few days are missing from a daily dataset, the index makes it easy to detect and fill in these gaps. Notice that when we print a tsibble object, it prints the index and guessed sampling frequency on the top right corner.\n\n\ndays &lt;- seq(as_date(\"2021-01-01\"), as_date(\"2021-01-31\"), by = \"day\")\ndays &lt;- days[-5] # Skip January 5\n\nx &lt;- tsibble(day = days, value = rnorm(30), index = day)\nfill_gaps(x)\n\n# A tsibble: 31 x 2 [1D]\n   day         value\n   &lt;date&gt;      &lt;dbl&gt;\n 1 2021-01-01  0.378\n 2 2021-01-02  0.376\n 3 2021-01-03  0.267\n 4 2021-01-04  0.220\n 5 2021-01-05 NA    \n 6 2021-01-06 -0.197\n 7 2021-01-07 -0.508\n 8 2021-01-08 -1.77 \n 9 2021-01-09 -1.19 \n10 2021-01-10  0.911\n# ℹ 21 more rows\n\n\n\nTsibbles can store more than one time series at a time. In this case, we have to specify key columns that distinguish between the separate time series. For example, in the olympics running times dataset,\n\n\nolympic_running\n\n# A tsibble: 312 x 4 [4Y]\n# Key:       Length, Sex [14]\n    Year Length Sex    Time\n   &lt;int&gt;  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1  1896    100 men    12  \n 2  1900    100 men    11  \n 3  1904    100 men    11  \n 4  1908    100 men    10.8\n 5  1912    100 men    10.8\n 6  1916    100 men    NA  \n 7  1920    100 men    10.8\n 8  1924    100 men    10.6\n 9  1928    100 men    10.8\n10  1932    100 men    10.3\n# ℹ 302 more rows\n\n\nthe keys are running distance and sex. If we were creating a tsibble from a data.frame containing these multiple time series, we would need to specify the keys. This protects against accidentally having duplicate observations at given times.\n\nolympic_df &lt;- as.data.frame(olympic_running)\nas_tsibble(olympic_df, index = Year, key = c(\"Sex\", \"Length\")) # what happens if we remove key?\n\n# A tsibble: 312 x 4 [4Y]\n# Key:       Sex, Length [14]\n    Year Length Sex    Time\n   &lt;int&gt;  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1  1896    100 men    12  \n 2  1900    100 men    11  \n 3  1904    100 men    11  \n 4  1908    100 men    10.8\n 5  1912    100 men    10.8\n 6  1916    100 men    NA  \n 7  1920    100 men    10.8\n 8  1924    100 men    10.6\n 9  1928    100 men    10.8\n10  1932    100 men    10.3\n# ℹ 302 more rows\n\n\n\nThe usual data tidying functions from dplyr are implemented for tsibbles. Filtering rows, selecting columns, deriving variables using mutate, and summarizing groups using group_by and summarise all work as expected. One distinction to be careful about is that the results will be grouped by their index.\nFor example, this computes the total cost of Australian pharmaceuticals per month for a particular type of script. We simply filter to the script type and take the sum of costs.\n\n\nPBS %&gt;%\n  filter(ATC2 == \"A10\") %&gt;%\n  summarise(TotalC = sum(Cost))\n\n# A tsibble: 204 x 2 [1M]\n      Month  TotalC\n      &lt;mth&gt;   &lt;dbl&gt;\n 1 1991 Jul 3526591\n 2 1991 Aug 3180891\n 3 1991 Sep 3252221\n 4 1991 Oct 3611003\n 5 1991 Nov 3565869\n 6 1991 Dec 4306371\n 7 1992 Jan 5088335\n 8 1992 Feb 2814520\n 9 1992 Mar 2985811\n10 1992 Apr 3204780\n# ℹ 194 more rows\n\n\nIf we had wanted the total cost by year, we would have to convert to an ordinary data.frame with a year variable. We cannot use a tsibble here because we would have multiple measurements per year, and this would violate tsibble’s policy of having no duplicates.\n\nPBS %&gt;%\n  filter(ATC2 == \"A10\") %&gt;%\n  mutate(Year = year(Month)) %&gt;%\n  as_tibble() %&gt;%\n  group_by(Year) %&gt;%\n  summarise(TotalC = sum(Cost))\n\n# A tibble: 18 × 2\n    Year     TotalC\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1  1991  21442946 \n 2  1992  45686946.\n 3  1993  55532688.\n 4  1994  60816080.\n 5  1995  67326599.\n 6  1996  77397927.\n 7  1997  85131672.\n 8  1998  93310626.\n 9  1999 105959043.\n10  2000 122496586.\n11  2001 136467442.\n12  2002 149066136.\n13  2003 156464261.\n14  2004 183798935.\n15  2005 199655595 \n16  2006 220354676 \n17  2007 265718966.\n18  2008 135036513"
  },
  {
    "objectID": "content/4-1.html",
    "href": "content/4-1.html",
    "title": "Elements of a Shiny App",
    "section": "",
    "text": "Recording, Code"
  },
  {
    "objectID": "content/4-1.html#footnotes",
    "href": "content/4-1.html#footnotes",
    "title": "Elements of a Shiny App",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI like to use these names to keep everything organized, but they are not standard in the community.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 436",
    "section": "",
    "text": "Placeholder"
  }
]